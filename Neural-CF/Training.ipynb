{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","collapsed_sections":["NiZIFY1rjptM"],"machine_shape":"hm","mount_file_id":"1C8DKzNq4VefaVUhbwmCjLk0CJ8ap3FgB","authorship_tag":"ABX9TyPIHXI9/fkEJvF7VaLkR9s4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import numpy as np\n","import json\n","import random\n","import os\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder\n","from collections import Counter\n","from typing import Tuple, List, Any\n","from scipy.sparse.linalg import svds\n","from scipy.sparse import csr_matrix"],"metadata":{"id":"tIPJcTnWxpYU","executionInfo":{"status":"ok","timestamp":1701339891138,"user_tz":300,"elapsed":1298,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Load Data\n","Using the subset data. 1000 user for now"],"metadata":{"id":"GohCVvVQ1XzK"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MLmuxtdWrI0t","executionInfo":{"status":"ok","timestamp":1701339893248,"user_tz":300,"elapsed":234,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}},"outputId":"2f8817bc-287c-4085-83b5-2b7a952bd52f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["%cd ./Neural-CF"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1TORizbKuCmf","executionInfo":{"status":"ok","timestamp":1701339894559,"user_tz":300,"elapsed":217,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}},"outputId":"1c28afbc-6bcf-4741-82ec-6325e2332bf4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Neural-CF\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"uFzz9aDfw_h0","executionInfo":{"status":"ok","timestamp":1701339899984,"user_tz":300,"elapsed":3791,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}}},"outputs":[],"source":["def load_data(\n","    data_folder: str,\n","    filenames=(\"yelp_academic_dataset_user.json\", \"yelp_academic_dataset_business.json\", \"yelp_academic_dataset_review.json\")\n","    ) -> Tuple[List[Any], ...]:\n","    return tuple(map(\n","                  lambda f: list(map(json.loads, open(os.path.join(data_folder, f), \"r\", encoding=\"utf-8\").readlines())),\n","                  filenames\n","                ))\n","\n","(subset_user_data, subset_business_data, subset_review_data) = load_data(\"./Yelp-Dataset/subset\",\n","                                                                        (\"subset_user.json\",\n","                                                                          \"subset_business.json\",\n","                                                                          \"subset_review.json\"))"]},{"cell_type":"code","source":["print(subset_user_data[0].keys())\n","print(subset_business_data[0].keys())\n","print(subset_review_data[0].keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nd0nGvFUyPzs","executionInfo":{"status":"ok","timestamp":1701339902088,"user_tz":300,"elapsed":226,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}},"outputId":"9ac848e5-f8b0-45fe-dc9f-e24319f70bb8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny', 'cool', 'elite', 'friends', 'fans', 'average_stars', 'compliment_hot', 'compliment_more', 'compliment_profile', 'compliment_cute', 'compliment_list', 'compliment_note', 'compliment_plain', 'compliment_cool', 'compliment_funny', 'compliment_writer', 'compliment_photos'])\n","dict_keys(['business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours'])\n","dict_keys(['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny', 'cool', 'text', 'date'])\n"]}]},{"cell_type":"code","source":["# Display City names in the subset data\n","city_list = []\n","for i, business in enumerate(subset_business_data):\n","  if business['city'] not in city_list:\n","    city_list.append(business['city'])\n","print(city_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LULPTicDfDJd","executionInfo":{"status":"ok","timestamp":1701339931635,"user_tz":300,"elapsed":219,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}},"outputId":"df1e866e-c3a2-4207-b578-b619c98d1305"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['Philadelphia', 'Tampa', 'St Louis', 'St. Louis', 'New Orleans', 'Tucson', 'Saint Louis', 'Indianapolis', 'Ardmore', 'Collingswood', 'Dunedin', 'Tarpon Springs', 'Saint Petersburg', 'Cherry Hill', 'Nashville', 'Reno', 'St. Petersburg', 'Clayton', 'Brandon', 'East Norriton', 'Pottstown', 'Tierra Verde', 'Lutz', 'Brentwood', 'Oldsmar', 'Kennett Square', 'Media', 'King Of Prussia', 'Florissant', 'King of Prussia', 'Carmel', 'ST LOUIS', 'Conshohocken', 'Clearwater Beach', 'Wynnewood', 'Sparks', 'Cinnaminson', 'Bensalem', 'North Redington Bch', 'Clearwater', 'Alton', 'Camden', 'Plymouth Meeting', 'Westmont', 'Creve Coeur', 'Horsham', 'Palm Harbor', 'Harahan', 'Manayunk', 'Maplewood', 'Kenner', 'South Tampa', 'St Petersburg', 'Maple Shade', 'Webster Groves', 'Upper Darby', 'Fishers', 'Collegeville', 'Haddon Township', 'Downingtown', 'Chesterfield', 'Wayne', 'Plant City', 'Richmond Heights']\n"]}]},{"cell_type":"markdown","source":["# Data Preprocess Test (For Debugging)\n","For debugging only, no need to run\n","\n","Ref: https://github.com/zhrlove/NCF/tree/master \\\n","Ref2: https://github.com/hexiangnan/sigir16-eals\n","\n","NCF: https://github.com/hexiangnan/neural_collaborative_filtering \\\n","NCF (torch): https://github.com/yihong-chen/neural-collaborative-filtering/tree/master\n","\n","Training Input: `userID::itemID::rating::timestamp (if have)`"],"metadata":{"id":"NiZIFY1rjptM"}},{"cell_type":"code","source":["for i, business in enumerate(subset_business_data):\n","  if business['business_id'] == 'gGyqnAlpFrka_qzpO7j4lQ':\n","    print(business['name'])\n","for i, user in enumerate(subset_user_data):\n","  if user['user_id'] == 'GcdYgbaF75vj7RO6EZhPOQ':\n","    print(user['name'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pZcme74BLoi","executionInfo":{"status":"ok","timestamp":1700734111697,"user_tz":300,"elapsed":166,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}},"outputId":"df55fadb-5105-4ca3-b5d1-3182ee2572b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Citizens Bank Park\n","Kathleen\n"]}]},{"cell_type":"code","source":["# Reindex\n","user_item_interactions = subset_review_data\n","df = pd.DataFrame(user_item_interactions)\n","print(\"Number of Unique Users:\", df['user_id'].nunique())\n","print(\"Number of Unique Businesses:\", df['business_id'].nunique())\n","\n","df = df.groupby(['user_id', 'business_id']).agg({'stars': 'mean'}).reset_index()\n","print(df.head(10))\n","\n","user_id = df[['user_id']].drop_duplicates().reindex()\n","user_id['userId'] = np.arange(len(user_id))\n","ml1m_rating = pd.merge(df, user_id, on=['user_id'], how='left')\n","\n","item_id = df[['business_id']].drop_duplicates()\n","item_id['itemId'] = np.arange(len(item_id))\n","yelp_rating = pd.merge(ml1m_rating, item_id, on=['business_id'], how='left')\n","yelp_rating = yelp_rating[['userId', 'itemId', 'stars']]\n","print(yelp_rating.head(10))\n","print('Range of userId is [{}, {}]'.format(yelp_rating.userId.min(), yelp_rating.userId.max()))\n","print('Range of itemId is [{}, {}]'.format(yelp_rating.itemId.min(), yelp_rating.itemId.max()))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7eTqCtjKLLX","executionInfo":{"status":"ok","timestamp":1700734114706,"user_tz":300,"elapsed":722,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}},"outputId":"8163cec2-0a0c-43cd-ce23-79e9e590285e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Unique Users: 961\n","Number of Unique Businesses: 1000\n","                  user_id             business_id  stars\n","0  -3s52C4zL_DHRK0ULG6qtg  -kqjc8DxxRac4cz2qTKCLw    4.0\n","1  -3s52C4zL_DHRK0ULG6qtg  0QYWhij_YZ7Lyk9F6213Sg    5.0\n","2  -3s52C4zL_DHRK0ULG6qtg  1YflE3DkiCZGgLnf3paLnA    5.0\n","3  -3s52C4zL_DHRK0ULG6qtg  2BMk_drsikKWslJCXmQtjQ    2.0\n","4  -3s52C4zL_DHRK0ULG6qtg  2IahpaBR4U2Kdy9HF28EQA    2.5\n","5  -3s52C4zL_DHRK0ULG6qtg  33JlrWf0kmHd2VzW58Wp0g    3.0\n","6  -3s52C4zL_DHRK0ULG6qtg  6t0sNev22mcbvOB4gYVVOw    3.0\n","7  -3s52C4zL_DHRK0ULG6qtg  89SD5fNDDnJj-ITB40hLsQ    1.0\n","8  -3s52C4zL_DHRK0ULG6qtg  8O35ji_yOMVJmZ6bl96yhQ    3.0\n","9  -3s52C4zL_DHRK0ULG6qtg  8QZJvkx29OQNZgrM53aVbw    4.0\n","   userId  itemId  stars\n","0       0       0    4.0\n","1       0       1    5.0\n","2       0       2    5.0\n","3       0       3    2.0\n","4       0       4    2.5\n","5       0       5    3.0\n","6       0       6    3.0\n","7       0       7    1.0\n","8       0       8    3.0\n","9       0       9    4.0\n","Range of userId is [0, 960]\n","Range of itemId is [0, 999]\n"]}]},{"cell_type":"markdown","source":["# Training Setup\n","\n","We can train NeuralMF without training the GMF and MLP. But, the author suggest that training GMF and MLP first can lead to better performance for large scale data.\n","\n","Edit the config parameter in `train.py` to adjust any hyperparameters"],"metadata":{"id":"4fr-BQSgr5Zu"}},{"cell_type":"code","source":["%cd ./Torch-NCF"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2g29lUur4nn","executionInfo":{"status":"ok","timestamp":1701339937381,"user_tz":300,"elapsed":228,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}},"outputId":"cceed8ce-f861-40d1-f382-23ebd4200977"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Neural-CF/Torch-NCF\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"5GI4WkrFuhY0"}},{"cell_type":"code","source":["# Make directory to save the models\n","import os\n","if not os.path.exists('checkpoints'):\n","    os.makedirs('checkpoints')\n","\n","# For running in Google Colab\n","!pip install tensorboardX==1.8.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xupJbW6Fr9XI","executionInfo":{"status":"ok","timestamp":1701339944393,"user_tz":300,"elapsed":5747,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}},"outputId":"7aaceb4e-3385-47ab-9d8b-eac562aae910"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX==1.8.0\n","  Downloading tensorboardX-1.8-py2.py3-none-any.whl (216 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/216.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m194.6/216.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.3/216.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.8.0) (1.23.5)\n","Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.8.0) (3.20.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.8.0) (1.16.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-1.8\n"]}]},{"cell_type":"markdown","source":["# Training GMF\n","Before Training, adjust the config for num_users and num_items correctly"],"metadata":{"id":"BE-d40Eb7JdJ"}},{"cell_type":"code","source":["!python train.py --data_dir '/content/drive/MyDrive/Neural-CF/Yelp-Dataset/subset/subset_review.json' --model 'gmf'"],"metadata":{"id":"4LD1gtym8dKG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training MLP\n","Set the pretrained MF path in the mlp_config in `train.py` if you set pretrain = true.\n"],"metadata":{"id":"0I_AVfe5jmbs"}},{"cell_type":"code","source":["!python train.py --data_dir '/content/drive/MyDrive/Neural-CF/Yelp-Dataset/subset/subset_review.json' --model 'mlp'"],"metadata":{"id":"EpiYaPOekXXq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train NeuralMF\n","Edit the pretrain setting of neumf_config in `train.py` to determine whether you want to use the pretrained MLP and GMF to train the NeuralMF or not.\n","\n","For small predictive factors, running NeuMF without pre-training can achieve better performance than GMF and MLP. For large predictive factors, pre-training NeuMF can yield better performance."],"metadata":{"id":"uD9PQGP_hgcJ"}},{"cell_type":"code","source":["!python train.py --data_dir '/content/drive/MyDrive/Neural-CF/Yelp-Dataset/subset/subset_review.json' --model 'neumf'"],"metadata":{"id":"yZdIPJd-h3Ye","colab":{"base_uri":"https://localhost:8080/"},"outputId":"75346d9f-3276-436b-a8e3-6176c576a4a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Data....\n","/content/drive/MyDrive/Neural-CF/Torch-NCF/data.py:64: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  ratings['rating'][ratings['rating'] > 0] = 1.0\n","     userId  ...                                   negative_samples\n","0         0  ...  [922, 452, 834, 969, 488, 99, 323, 581, 555, 4...\n","1         1  ...  [840, 950, 559, 214, 992, 824, 623, 566, 607, ...\n","2         2  ...  [46, 871, 622, 684, 966, 993, 582, 651, 731, 1...\n","3         3  ...  [617, 750, 12, 511, 802, 80, 386, 799, 46, 600...\n","4         4  ...  [14, 784, 822, 2, 892, 741, 591, 676, 118, 245...\n","..      ...  ...                                                ...\n","956     956  ...  [999, 789, 465, 11, 443, 631, 249, 753, 152, 5...\n","957     957  ...  [262, 181, 633, 427, 504, 329, 284, 694, 877, ...\n","958     958  ...  [22, 499, 26, 303, 957, 518, 630, 185, 893, 30...\n","959     959  ...  [501, 358, 430, 238, 49, 298, 381, 634, 578, 1...\n","960     960  ...  [301, 454, 343, 101, 798, 830, 595, 778, 235, ...\n","\n","[961 rows x 3 columns]\n","[tensor([ 10,  10,  10,  ..., 772, 885, 156]), tensor([461, 503, 431,  ..., 959, 988, 708]), tensor([ 10,  10,  10,  ..., 156, 156, 156]), tensor([601, 379, 672,  ..., 145, 151, 998])]\n","NeuMF(\n","  (embedding_user_mlp): Embedding(961, 8)\n","  (embedding_item_mlp): Embedding(1000, 8)\n","  (embedding_user_mf): Embedding(961, 8)\n","  (embedding_item_mf): Embedding(1000, 8)\n","  (fc_layers): ModuleList(\n","    (0): Linear(in_features=16, out_features=64, bias=True)\n","    (1): Linear(in_features=64, out_features=32, bias=True)\n","    (2): Linear(in_features=32, out_features=16, bias=True)\n","    (3): Linear(in_features=16, out_features=8, bias=True)\n","  )\n","  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n","  (logistic): Sigmoid()\n",")\n","Epoch 0 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 0] Batch 0, Loss 0.7377346754074097\n","[Training Epoch 0] Batch 1, Loss 0.7412512302398682\n","[Training Epoch 0] Batch 2, Loss 0.7486304044723511\n","[Training Epoch 0] Batch 3, Loss 0.7378124594688416\n","[Training Epoch 0] Batch 4, Loss 0.7368788719177246\n","[Training Epoch 0] Batch 5, Loss 0.7426809668540955\n","[Training Epoch 0] Batch 6, Loss 0.7482247352600098\n","[Training Epoch 0] Batch 7, Loss 0.7293042540550232\n","[Training Epoch 0] Batch 8, Loss 0.7174053192138672\n","[Training Epoch 0] Batch 9, Loss 0.7336699366569519\n","[Training Epoch 0] Batch 10, Loss 0.7305642366409302\n","[Training Epoch 0] Batch 11, Loss 0.7360841035842896\n","[Training Epoch 0] Batch 12, Loss 0.7265300750732422\n","[Training Epoch 0] Batch 13, Loss 0.7344611883163452\n","[Training Epoch 0] Batch 14, Loss 0.726666271686554\n","[Training Epoch 0] Batch 15, Loss 0.7269474267959595\n","[Training Epoch 0] Batch 16, Loss 0.7267552018165588\n","[Training Epoch 0] Batch 17, Loss 0.7311701774597168\n","[Training Epoch 0] Batch 18, Loss 0.7256613969802856\n","[Training Epoch 0] Batch 19, Loss 0.7075117826461792\n","[Training Epoch 0] Batch 20, Loss 0.7187658548355103\n","[Training Epoch 0] Batch 21, Loss 0.7167744040489197\n","[Training Epoch 0] Batch 22, Loss 0.7142899036407471\n","[Training Epoch 0] Batch 23, Loss 0.7016081213951111\n","[Training Epoch 0] Batch 24, Loss 0.7042738199234009\n","[Training Epoch 0] Batch 25, Loss 0.6974759697914124\n","[Training Epoch 0] Batch 26, Loss 0.7063912749290466\n","[Training Epoch 0] Batch 27, Loss 0.7050886154174805\n","[Training Epoch 0] Batch 28, Loss 0.697887659072876\n","[Training Epoch 0] Batch 29, Loss 0.7026913166046143\n","[Training Epoch 0] Batch 30, Loss 0.6969192028045654\n","[Training Epoch 0] Batch 31, Loss 0.679161787033081\n","[Training Epoch 0] Batch 32, Loss 0.6968345046043396\n","[Training Epoch 0] Batch 33, Loss 0.6835695505142212\n","[Training Epoch 0] Batch 34, Loss 0.6925017833709717\n","[Training Epoch 0] Batch 35, Loss 0.6772974729537964\n","[Training Epoch 0] Batch 36, Loss 0.6830201148986816\n","[Training Epoch 0] Batch 37, Loss 0.6781109571456909\n","[Training Epoch 0] Batch 38, Loss 0.6650285720825195\n","[Training Epoch 0] Batch 39, Loss 0.6664828658103943\n","[Training Epoch 0] Batch 40, Loss 0.6670376062393188\n","[Training Epoch 0] Batch 41, Loss 0.6540333032608032\n","[Training Epoch 0] Batch 42, Loss 0.6630792021751404\n","[Training Epoch 0] Batch 43, Loss 0.6534785628318787\n","[Training Epoch 0] Batch 44, Loss 0.6463428735733032\n","[Training Epoch 0] Batch 45, Loss 0.6478419303894043\n","[Training Epoch 0] Batch 46, Loss 0.6324178576469421\n","[Training Epoch 0] Batch 47, Loss 0.6308854818344116\n","[Training Epoch 0] Batch 48, Loss 0.6335710287094116\n","[Training Epoch 0] Batch 49, Loss 0.6262272596359253\n","[Training Epoch 0] Batch 50, Loss 0.6262476444244385\n","[Training Epoch 0] Batch 51, Loss 0.6203497648239136\n","[Training Epoch 0] Batch 52, Loss 0.6024982333183289\n","[Training Epoch 0] Batch 53, Loss 0.6022639870643616\n","[Training Epoch 0] Batch 54, Loss 0.5964618921279907\n","[Training Epoch 0] Batch 55, Loss 0.6054683923721313\n","[Training Epoch 0] Batch 56, Loss 0.586767315864563\n","[Training Epoch 0] Batch 57, Loss 0.5844550132751465\n","[Training Epoch 0] Batch 58, Loss 0.5765489339828491\n","[Training Epoch 0] Batch 59, Loss 0.5874199867248535\n","[Training Epoch 0] Batch 60, Loss 0.5597771406173706\n","[Training Epoch 0] Batch 61, Loss 0.5832787156105042\n","[Training Epoch 0] Batch 62, Loss 0.5625508427619934\n","[Training Epoch 0] Batch 63, Loss 0.551047682762146\n","[Training Epoch 0] Batch 64, Loss 0.5332560539245605\n","[Training Epoch 0] Batch 65, Loss 0.5460048317909241\n","[Training Epoch 0] Batch 66, Loss 0.5601842403411865\n","[Training Epoch 0] Batch 67, Loss 0.5424771308898926\n","[Training Epoch 0] Batch 68, Loss 0.5387363433837891\n","[Training Epoch 0] Batch 69, Loss 0.5272728204727173\n","[Training Epoch 0] Batch 70, Loss 0.5174252986907959\n","[Training Epoch 0] Batch 71, Loss 0.5238929986953735\n","[Training Epoch 0] Batch 72, Loss 0.5297905206680298\n","[Training Epoch 0] Batch 73, Loss 0.509692370891571\n","[Training Epoch 0] Batch 74, Loss 0.5328047275543213\n","[Training Epoch 0] Batch 75, Loss 0.5213064551353455\n","[Training Epoch 0] Batch 76, Loss 0.4882389008998871\n","[Training Epoch 0] Batch 77, Loss 0.5279382467269897\n","[Training Epoch 0] Batch 78, Loss 0.5210760831832886\n","[Training Epoch 0] Batch 79, Loss 0.5270211696624756\n","[Training Epoch 0] Batch 80, Loss 0.5093101263046265\n","[Training Epoch 0] Batch 81, Loss 0.5138725638389587\n","[Training Epoch 0] Batch 82, Loss 0.522138237953186\n","[Training Epoch 0] Batch 83, Loss 0.48830893635749817\n","[Training Epoch 0] Batch 84, Loss 0.5029194355010986\n","[Training Epoch 0] Batch 85, Loss 0.5176182985305786\n","[Training Epoch 0] Batch 86, Loss 0.5196192264556885\n","[Training Epoch 0] Batch 87, Loss 0.49624812602996826\n","[Training Epoch 0] Batch 88, Loss 0.5496214628219604\n","[Training Epoch 0] Batch 89, Loss 0.5048242211341858\n","[Training Epoch 0] Batch 90, Loss 0.5069422721862793\n","[Training Epoch 0] Batch 91, Loss 0.514560341835022\n","[Training Epoch 0] Batch 92, Loss 0.5018535852432251\n","[Training Epoch 0] Batch 93, Loss 0.5109208822250366\n","[Training Epoch 0] Batch 94, Loss 0.48167121410369873\n","[Training Epoch 0] Batch 95, Loss 0.5429763793945312\n","[Training Epoch 0] Batch 96, Loss 0.5215461254119873\n","[Training Epoch 0] Batch 97, Loss 0.5121161937713623\n","[Training Epoch 0] Batch 98, Loss 0.5416415929794312\n","[Training Epoch 0] Batch 99, Loss 0.5048837661743164\n","[Training Epoch 0] Batch 100, Loss 0.5023457407951355\n","[Training Epoch 0] Batch 101, Loss 0.5012850761413574\n","[Training Epoch 0] Batch 102, Loss 0.5021902918815613\n","[Training Epoch 0] Batch 103, Loss 0.5237061977386475\n","[Training Epoch 0] Batch 104, Loss 0.503803014755249\n","[Training Epoch 0] Batch 105, Loss 0.5149012804031372\n","[Training Epoch 0] Batch 106, Loss 0.5132625102996826\n","[Training Epoch 0] Batch 107, Loss 0.5191678404808044\n","[Training Epoch 0] Batch 108, Loss 0.4846028685569763\n","[Training Epoch 0] Batch 109, Loss 0.5002362728118896\n","[Training Epoch 0] Batch 110, Loss 0.4918503165245056\n","[Training Epoch 0] Batch 111, Loss 0.5004832744598389\n","[Training Epoch 0] Batch 112, Loss 0.5129396319389343\n","[Training Epoch 0] Batch 113, Loss 0.5136469602584839\n","[Training Epoch 0] Batch 114, Loss 0.5239046812057495\n","[Training Epoch 0] Batch 115, Loss 0.5225983262062073\n","[Training Epoch 0] Batch 116, Loss 0.488378643989563\n","[Training Epoch 0] Batch 117, Loss 0.4878133535385132\n","[Training Epoch 0] Batch 118, Loss 0.5018792152404785\n","[Training Epoch 0] Batch 119, Loss 0.48188087344169617\n","[Training Epoch 0] Batch 120, Loss 0.49763911962509155\n","[Training Epoch 0] Batch 121, Loss 0.5082049369812012\n","[Training Epoch 0] Batch 122, Loss 0.5135423541069031\n","[Training Epoch 0] Batch 123, Loss 0.4946118891239166\n","[Training Epoch 0] Batch 124, Loss 0.49314355850219727\n","[Training Epoch 0] Batch 125, Loss 0.5070840120315552\n","[Training Epoch 0] Batch 126, Loss 0.5080965757369995\n","[Training Epoch 0] Batch 127, Loss 0.5017671585083008\n","[Training Epoch 0] Batch 128, Loss 0.49634578824043274\n","[Training Epoch 0] Batch 129, Loss 0.48658061027526855\n","[Training Epoch 0] Batch 130, Loss 0.5055967569351196\n","[Training Epoch 0] Batch 131, Loss 0.5361887812614441\n","[Training Epoch 0] Batch 132, Loss 0.5016982555389404\n","[Training Epoch 0] Batch 133, Loss 0.5434705018997192\n","[Training Epoch 0] Batch 134, Loss 0.5227977633476257\n","[Training Epoch 0] Batch 135, Loss 0.5262174010276794\n","[Training Epoch 0] Batch 136, Loss 0.4742431044578552\n","[Training Epoch 0] Batch 137, Loss 0.4933292865753174\n","[Training Epoch 0] Batch 138, Loss 0.516216516494751\n","[Training Epoch 0] Batch 139, Loss 0.5157403945922852\n","[Training Epoch 0] Batch 140, Loss 0.5151572227478027\n","[Training Epoch 0] Batch 141, Loss 0.531062662601471\n","[Training Epoch 0] Batch 142, Loss 0.5096323490142822\n","[Training Epoch 0] Batch 143, Loss 0.5047126412391663\n","[Training Epoch 0] Batch 144, Loss 0.48735129833221436\n","[Training Epoch 0] Batch 145, Loss 0.5281002521514893\n","[Training Epoch 0] Batch 146, Loss 0.5001974105834961\n","[Training Epoch 0] Batch 147, Loss 0.4998652935028076\n","[Training Epoch 0] Batch 148, Loss 0.5023785829544067\n","[Training Epoch 0] Batch 149, Loss 0.506199836730957\n","[Training Epoch 0] Batch 150, Loss 0.5148249864578247\n","[Training Epoch 0] Batch 151, Loss 0.5183044075965881\n","[Training Epoch 0] Batch 152, Loss 0.5137554407119751\n","[Training Epoch 0] Batch 153, Loss 0.5060716867446899\n","[Training Epoch 0] Batch 154, Loss 0.5137297511100769\n","[Training Epoch 0] Batch 155, Loss 0.5216201543807983\n","[Training Epoch 0] Batch 156, Loss 0.5235136151313782\n","[Training Epoch 0] Batch 157, Loss 0.5015549659729004\n","[Training Epoch 0] Batch 158, Loss 0.5082666873931885\n","[Training Epoch 0] Batch 159, Loss 0.5185438394546509\n","[Training Epoch 0] Batch 160, Loss 0.49516332149505615\n","[Training Epoch 0] Batch 161, Loss 0.4765423536300659\n","[Training Epoch 0] Batch 162, Loss 0.5028966665267944\n","[Training Epoch 0] Batch 163, Loss 0.4642396867275238\n","[Training Epoch 0] Batch 164, Loss 0.4854501485824585\n","[Training Epoch 0] Batch 165, Loss 0.506894588470459\n","[Training Epoch 0] Batch 166, Loss 0.5015261173248291\n","[Training Epoch 0] Batch 167, Loss 0.4833294451236725\n","/content/drive/MyDrive/Neural-CF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 0] HR = 0.0599, NDCG = 0.0303\n","Model saved at epoch 0\n","Epoch 1 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 1] Batch 0, Loss 0.5159734487533569\n","[Training Epoch 1] Batch 1, Loss 0.4715604782104492\n","[Training Epoch 1] Batch 2, Loss 0.48514777421951294\n","[Training Epoch 1] Batch 3, Loss 0.4947309195995331\n","[Training Epoch 1] Batch 4, Loss 0.4990655183792114\n","[Training Epoch 1] Batch 5, Loss 0.49421194195747375\n","[Training Epoch 1] Batch 6, Loss 0.5067777633666992\n","[Training Epoch 1] Batch 7, Loss 0.5390323996543884\n","[Training Epoch 1] Batch 8, Loss 0.5108667612075806\n","[Training Epoch 1] Batch 9, Loss 0.5088784694671631\n","[Training Epoch 1] Batch 10, Loss 0.505928635597229\n","[Training Epoch 1] Batch 11, Loss 0.4875302314758301\n","[Training Epoch 1] Batch 12, Loss 0.5049627423286438\n","[Training Epoch 1] Batch 13, Loss 0.5071070194244385\n","[Training Epoch 1] Batch 14, Loss 0.4803529381752014\n","[Training Epoch 1] Batch 15, Loss 0.5371198654174805\n","[Training Epoch 1] Batch 16, Loss 0.5166340470314026\n","[Training Epoch 1] Batch 17, Loss 0.5014413595199585\n","[Training Epoch 1] Batch 18, Loss 0.49797070026397705\n","[Training Epoch 1] Batch 19, Loss 0.4807857573032379\n","[Training Epoch 1] Batch 20, Loss 0.5039287209510803\n","[Training Epoch 1] Batch 21, Loss 0.48995140194892883\n","[Training Epoch 1] Batch 22, Loss 0.4768174886703491\n","[Training Epoch 1] Batch 23, Loss 0.505424976348877\n","[Training Epoch 1] Batch 24, Loss 0.499828964471817\n","[Training Epoch 1] Batch 25, Loss 0.5120863914489746\n","[Training Epoch 1] Batch 26, Loss 0.4945051074028015\n","[Training Epoch 1] Batch 27, Loss 0.4999963343143463\n","[Training Epoch 1] Batch 28, Loss 0.49411994218826294\n","[Training Epoch 1] Batch 29, Loss 0.540814220905304\n","[Training Epoch 1] Batch 30, Loss 0.490001916885376\n","[Training Epoch 1] Batch 31, Loss 0.519023060798645\n","[Training Epoch 1] Batch 32, Loss 0.5238041281700134\n","[Training Epoch 1] Batch 33, Loss 0.49988797307014465\n","[Training Epoch 1] Batch 34, Loss 0.49420925974845886\n","[Training Epoch 1] Batch 35, Loss 0.5130840539932251\n","[Training Epoch 1] Batch 36, Loss 0.5193554162979126\n","[Training Epoch 1] Batch 37, Loss 0.47260767221450806\n","[Training Epoch 1] Batch 38, Loss 0.49143102765083313\n","[Training Epoch 1] Batch 39, Loss 0.4849061965942383\n","[Training Epoch 1] Batch 40, Loss 0.492885023355484\n","[Training Epoch 1] Batch 41, Loss 0.4996916651725769\n","[Training Epoch 1] Batch 42, Loss 0.5052855014801025\n","[Training Epoch 1] Batch 43, Loss 0.4834039509296417\n","[Training Epoch 1] Batch 44, Loss 0.5046483874320984\n","[Training Epoch 1] Batch 45, Loss 0.4942227602005005\n","[Training Epoch 1] Batch 46, Loss 0.49959319829940796\n","[Training Epoch 1] Batch 47, Loss 0.5143696069717407\n","[Training Epoch 1] Batch 48, Loss 0.506842315196991\n","[Training Epoch 1] Batch 49, Loss 0.48810404539108276\n","[Training Epoch 1] Batch 50, Loss 0.5099116563796997\n","[Training Epoch 1] Batch 51, Loss 0.5085389614105225\n","[Training Epoch 1] Batch 52, Loss 0.483516663312912\n","[Training Epoch 1] Batch 53, Loss 0.49953651428222656\n","[Training Epoch 1] Batch 54, Loss 0.5007516741752625\n","[Training Epoch 1] Batch 55, Loss 0.4681900441646576\n","[Training Epoch 1] Batch 56, Loss 0.5178670883178711\n","[Training Epoch 1] Batch 57, Loss 0.5214946269989014\n","[Training Epoch 1] Batch 58, Loss 0.501896858215332\n","[Training Epoch 1] Batch 59, Loss 0.5090377926826477\n","[Training Epoch 1] Batch 60, Loss 0.49795353412628174\n","[Training Epoch 1] Batch 61, Loss 0.47405868768692017\n","[Training Epoch 1] Batch 62, Loss 0.4917510151863098\n","[Training Epoch 1] Batch 63, Loss 0.5160022974014282\n","[Training Epoch 1] Batch 64, Loss 0.5264317393302917\n","[Training Epoch 1] Batch 65, Loss 0.48621028661727905\n","[Training Epoch 1] Batch 66, Loss 0.48271262645721436\n","[Training Epoch 1] Batch 67, Loss 0.500352680683136\n","[Training Epoch 1] Batch 68, Loss 0.507839560508728\n","[Training Epoch 1] Batch 69, Loss 0.5150082111358643\n","[Training Epoch 1] Batch 70, Loss 0.519896388053894\n","[Training Epoch 1] Batch 71, Loss 0.4916841685771942\n","[Training Epoch 1] Batch 72, Loss 0.4844718277454376\n","[Training Epoch 1] Batch 73, Loss 0.49619609117507935\n","[Training Epoch 1] Batch 74, Loss 0.5126127004623413\n","[Training Epoch 1] Batch 75, Loss 0.5248254537582397\n","[Training Epoch 1] Batch 76, Loss 0.5234048366546631\n","[Training Epoch 1] Batch 77, Loss 0.4839426875114441\n","[Training Epoch 1] Batch 78, Loss 0.5115622282028198\n","[Training Epoch 1] Batch 79, Loss 0.4735426902770996\n","[Training Epoch 1] Batch 80, Loss 0.4989137649536133\n","[Training Epoch 1] Batch 81, Loss 0.5052521228790283\n","[Training Epoch 1] Batch 82, Loss 0.4873697757720947\n","[Training Epoch 1] Batch 83, Loss 0.5227366089820862\n","[Training Epoch 1] Batch 84, Loss 0.5061593055725098\n","[Training Epoch 1] Batch 85, Loss 0.5435492992401123\n","[Training Epoch 1] Batch 86, Loss 0.4684586822986603\n","[Training Epoch 1] Batch 87, Loss 0.5049326419830322\n","[Training Epoch 1] Batch 88, Loss 0.45695382356643677\n","[Training Epoch 1] Batch 89, Loss 0.48124176263809204\n","[Training Epoch 1] Batch 90, Loss 0.49965590238571167\n","[Training Epoch 1] Batch 91, Loss 0.5023261308670044\n","[Training Epoch 1] Batch 92, Loss 0.5069795846939087\n","[Training Epoch 1] Batch 93, Loss 0.49860215187072754\n","[Training Epoch 1] Batch 94, Loss 0.4650605022907257\n","[Training Epoch 1] Batch 95, Loss 0.5016007423400879\n","[Training Epoch 1] Batch 96, Loss 0.5034295320510864\n","[Training Epoch 1] Batch 97, Loss 0.5108120441436768\n","[Training Epoch 1] Batch 98, Loss 0.5222524404525757\n","[Training Epoch 1] Batch 99, Loss 0.503851056098938\n","[Training Epoch 1] Batch 100, Loss 0.48764416575431824\n","[Training Epoch 1] Batch 101, Loss 0.5019866228103638\n","[Training Epoch 1] Batch 102, Loss 0.49046918749809265\n","[Training Epoch 1] Batch 103, Loss 0.5464655160903931\n","[Training Epoch 1] Batch 104, Loss 0.5124020576477051\n","[Training Epoch 1] Batch 105, Loss 0.469134658575058\n","[Training Epoch 1] Batch 106, Loss 0.5001977682113647\n","[Training Epoch 1] Batch 107, Loss 0.5087606310844421\n","[Training Epoch 1] Batch 108, Loss 0.4875006079673767\n","[Training Epoch 1] Batch 109, Loss 0.5065010786056519\n","[Training Epoch 1] Batch 110, Loss 0.5107412338256836\n","[Training Epoch 1] Batch 111, Loss 0.4850364923477173\n","[Training Epoch 1] Batch 112, Loss 0.5120043754577637\n","[Training Epoch 1] Batch 113, Loss 0.475252628326416\n","[Training Epoch 1] Batch 114, Loss 0.4952031970024109\n","[Training Epoch 1] Batch 115, Loss 0.5287587642669678\n","[Training Epoch 1] Batch 116, Loss 0.5111432075500488\n","[Training Epoch 1] Batch 117, Loss 0.488513708114624\n","[Training Epoch 1] Batch 118, Loss 0.5071281790733337\n","[Training Epoch 1] Batch 119, Loss 0.49198776483535767\n","[Training Epoch 1] Batch 120, Loss 0.49317115545272827\n","[Training Epoch 1] Batch 121, Loss 0.5001797080039978\n","[Training Epoch 1] Batch 122, Loss 0.5028121471405029\n","[Training Epoch 1] Batch 123, Loss 0.5037273168563843\n","[Training Epoch 1] Batch 124, Loss 0.5223822593688965\n","[Training Epoch 1] Batch 125, Loss 0.4990366995334625\n","[Training Epoch 1] Batch 126, Loss 0.5443599224090576\n","[Training Epoch 1] Batch 127, Loss 0.5143119096755981\n","[Training Epoch 1] Batch 128, Loss 0.4847501516342163\n","[Training Epoch 1] Batch 129, Loss 0.5022576451301575\n","[Training Epoch 1] Batch 130, Loss 0.5051305294036865\n","[Training Epoch 1] Batch 131, Loss 0.48129236698150635\n","[Training Epoch 1] Batch 132, Loss 0.5076634883880615\n","[Training Epoch 1] Batch 133, Loss 0.5086550712585449\n","[Training Epoch 1] Batch 134, Loss 0.5028141140937805\n","[Training Epoch 1] Batch 135, Loss 0.4826294183731079\n","[Training Epoch 1] Batch 136, Loss 0.5052639245986938\n","[Training Epoch 1] Batch 137, Loss 0.526874840259552\n","[Training Epoch 1] Batch 138, Loss 0.48614364862442017\n","[Training Epoch 1] Batch 139, Loss 0.4959649443626404\n","[Training Epoch 1] Batch 140, Loss 0.4877396523952484\n","[Training Epoch 1] Batch 141, Loss 0.49729055166244507\n","[Training Epoch 1] Batch 142, Loss 0.5180333256721497\n","[Training Epoch 1] Batch 143, Loss 0.4948652982711792\n","[Training Epoch 1] Batch 144, Loss 0.48476162552833557\n","[Training Epoch 1] Batch 145, Loss 0.48390495777130127\n","[Training Epoch 1] Batch 146, Loss 0.49959278106689453\n","[Training Epoch 1] Batch 147, Loss 0.5684504508972168\n","[Training Epoch 1] Batch 148, Loss 0.5135694146156311\n","[Training Epoch 1] Batch 149, Loss 0.5268990993499756\n","[Training Epoch 1] Batch 150, Loss 0.4811783730983734\n","[Training Epoch 1] Batch 151, Loss 0.5227236747741699\n","[Training Epoch 1] Batch 152, Loss 0.5045258402824402\n","[Training Epoch 1] Batch 153, Loss 0.49696236848831177\n","[Training Epoch 1] Batch 154, Loss 0.5301321744918823\n","[Training Epoch 1] Batch 155, Loss 0.48570698499679565\n","[Training Epoch 1] Batch 156, Loss 0.5093350410461426\n","[Training Epoch 1] Batch 157, Loss 0.5074080228805542\n","[Training Epoch 1] Batch 158, Loss 0.4922446310520172\n","[Training Epoch 1] Batch 159, Loss 0.49347051978111267\n","[Training Epoch 1] Batch 160, Loss 0.5371279120445251\n","[Training Epoch 1] Batch 161, Loss 0.48880815505981445\n","[Training Epoch 1] Batch 162, Loss 0.4711950421333313\n","[Training Epoch 1] Batch 163, Loss 0.5169570446014404\n","[Training Epoch 1] Batch 164, Loss 0.5045100450515747\n","[Training Epoch 1] Batch 165, Loss 0.5277755260467529\n","[Training Epoch 1] Batch 166, Loss 0.49879127740859985\n","[Training Epoch 1] Batch 167, Loss 0.5644046068191528\n","/content/drive/MyDrive/Neural-CF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 1] HR = 0.0855, NDCG = 0.0426\n","Epoch 2 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 2] Batch 0, Loss 0.5081174969673157\n","[Training Epoch 2] Batch 1, Loss 0.5288538932800293\n","[Training Epoch 2] Batch 2, Loss 0.5105109810829163\n","[Training Epoch 2] Batch 3, Loss 0.5165449976921082\n","[Training Epoch 2] Batch 4, Loss 0.514704167842865\n","[Training Epoch 2] Batch 5, Loss 0.5041013360023499\n","[Training Epoch 2] Batch 6, Loss 0.4857921898365021\n","[Training Epoch 2] Batch 7, Loss 0.5224806070327759\n","[Training Epoch 2] Batch 8, Loss 0.5056524872779846\n","[Training Epoch 2] Batch 9, Loss 0.49297499656677246\n","[Training Epoch 2] Batch 10, Loss 0.50621497631073\n","[Training Epoch 2] Batch 11, Loss 0.4992833733558655\n","[Training Epoch 2] Batch 12, Loss 0.49045106768608093\n","[Training Epoch 2] Batch 13, Loss 0.49578607082366943\n","[Training Epoch 2] Batch 14, Loss 0.5017585158348083\n","[Training Epoch 2] Batch 15, Loss 0.4832618534564972\n","[Training Epoch 2] Batch 16, Loss 0.5037205815315247\n","[Training Epoch 2] Batch 17, Loss 0.4785310626029968\n","[Training Epoch 2] Batch 18, Loss 0.5052758455276489\n","[Training Epoch 2] Batch 19, Loss 0.510092556476593\n","[Training Epoch 2] Batch 20, Loss 0.5053958296775818\n","[Training Epoch 2] Batch 21, Loss 0.5129005908966064\n","[Training Epoch 2] Batch 22, Loss 0.4988994002342224\n","[Training Epoch 2] Batch 23, Loss 0.5052496790885925\n","[Training Epoch 2] Batch 24, Loss 0.5027620196342468\n","[Training Epoch 2] Batch 25, Loss 0.524114727973938\n","[Training Epoch 2] Batch 26, Loss 0.4716974198818207\n","[Training Epoch 2] Batch 27, Loss 0.5049870014190674\n","[Training Epoch 2] Batch 28, Loss 0.48229917883872986\n","[Training Epoch 2] Batch 29, Loss 0.5105352401733398\n","[Training Epoch 2] Batch 30, Loss 0.5125381946563721\n","[Training Epoch 2] Batch 31, Loss 0.5101245641708374\n","[Training Epoch 2] Batch 32, Loss 0.4999598562717438\n","[Training Epoch 2] Batch 33, Loss 0.48255497217178345\n","[Training Epoch 2] Batch 34, Loss 0.5237548351287842\n","[Training Epoch 2] Batch 35, Loss 0.45445793867111206\n","[Training Epoch 2] Batch 36, Loss 0.5108222365379333\n","[Training Epoch 2] Batch 37, Loss 0.4982351064682007\n","[Training Epoch 2] Batch 38, Loss 0.5036026239395142\n","[Training Epoch 2] Batch 39, Loss 0.4991486668586731\n","[Training Epoch 2] Batch 40, Loss 0.4922542870044708\n","[Training Epoch 2] Batch 41, Loss 0.4886167049407959\n","[Training Epoch 2] Batch 42, Loss 0.481056272983551\n","[Training Epoch 2] Batch 43, Loss 0.5000981092453003\n","[Training Epoch 2] Batch 44, Loss 0.5109419226646423\n","[Training Epoch 2] Batch 45, Loss 0.5104269981384277\n","[Training Epoch 2] Batch 46, Loss 0.50141441822052\n","[Training Epoch 2] Batch 47, Loss 0.4840746819972992\n","[Training Epoch 2] Batch 48, Loss 0.5075923204421997\n","[Training Epoch 2] Batch 49, Loss 0.5132063031196594\n","[Training Epoch 2] Batch 50, Loss 0.47528451681137085\n","[Training Epoch 2] Batch 51, Loss 0.5113078951835632\n","[Training Epoch 2] Batch 52, Loss 0.4871406555175781\n","[Training Epoch 2] Batch 53, Loss 0.5004681348800659\n","[Training Epoch 2] Batch 54, Loss 0.5188102722167969\n","[Training Epoch 2] Batch 55, Loss 0.4790503978729248\n","[Training Epoch 2] Batch 56, Loss 0.49333328008651733\n","[Training Epoch 2] Batch 57, Loss 0.5083827972412109\n","[Training Epoch 2] Batch 58, Loss 0.4716423749923706\n","[Training Epoch 2] Batch 59, Loss 0.4959888160228729\n","[Training Epoch 2] Batch 60, Loss 0.49279749393463135\n","[Training Epoch 2] Batch 61, Loss 0.4970611333847046\n","[Training Epoch 2] Batch 62, Loss 0.4907083809375763\n","[Training Epoch 2] Batch 63, Loss 0.4884452819824219\n","[Training Epoch 2] Batch 64, Loss 0.5002232789993286\n","[Training Epoch 2] Batch 65, Loss 0.5133621096611023\n","[Training Epoch 2] Batch 66, Loss 0.47563689947128296\n","[Training Epoch 2] Batch 67, Loss 0.527499794960022\n","[Training Epoch 2] Batch 68, Loss 0.4979943037033081\n","[Training Epoch 2] Batch 69, Loss 0.47747278213500977\n","[Training Epoch 2] Batch 70, Loss 0.5269593000411987\n","[Training Epoch 2] Batch 71, Loss 0.5089786052703857\n","[Training Epoch 2] Batch 72, Loss 0.4978388547897339\n","[Training Epoch 2] Batch 73, Loss 0.5031672120094299\n","[Training Epoch 2] Batch 74, Loss 0.49454858899116516\n","[Training Epoch 2] Batch 75, Loss 0.5217156410217285\n","[Training Epoch 2] Batch 76, Loss 0.5119335651397705\n","[Training Epoch 2] Batch 77, Loss 0.48615336418151855\n","[Training Epoch 2] Batch 78, Loss 0.5170958042144775\n","[Training Epoch 2] Batch 79, Loss 0.49647003412246704\n","[Training Epoch 2] Batch 80, Loss 0.5155788660049438\n","[Training Epoch 2] Batch 81, Loss 0.48068612813949585\n","[Training Epoch 2] Batch 82, Loss 0.5018232464790344\n","[Training Epoch 2] Batch 83, Loss 0.5065025091171265\n","[Training Epoch 2] Batch 84, Loss 0.48156750202178955\n","[Training Epoch 2] Batch 85, Loss 0.5119108557701111\n","[Training Epoch 2] Batch 86, Loss 0.5212371349334717\n","[Training Epoch 2] Batch 87, Loss 0.5472484827041626\n","[Training Epoch 2] Batch 88, Loss 0.5120029449462891\n","[Training Epoch 2] Batch 89, Loss 0.47605907917022705\n","[Training Epoch 2] Batch 90, Loss 0.5195724368095398\n","[Training Epoch 2] Batch 91, Loss 0.5196502208709717\n","[Training Epoch 2] Batch 92, Loss 0.483049213886261\n","[Training Epoch 2] Batch 93, Loss 0.5191580057144165\n","[Training Epoch 2] Batch 94, Loss 0.5109300017356873\n","[Training Epoch 2] Batch 95, Loss 0.5042874813079834\n","[Training Epoch 2] Batch 96, Loss 0.486179381608963\n","[Training Epoch 2] Batch 97, Loss 0.5060423612594604\n","[Training Epoch 2] Batch 98, Loss 0.5031982660293579\n","[Training Epoch 2] Batch 99, Loss 0.5209769010543823\n","[Training Epoch 2] Batch 100, Loss 0.4783173203468323\n","[Training Epoch 2] Batch 101, Loss 0.48184043169021606\n","[Training Epoch 2] Batch 102, Loss 0.5140266418457031\n","[Training Epoch 2] Batch 103, Loss 0.4942588806152344\n","[Training Epoch 2] Batch 104, Loss 0.4921947121620178\n","[Training Epoch 2] Batch 105, Loss 0.5217953324317932\n","[Training Epoch 2] Batch 106, Loss 0.5017023682594299\n","[Training Epoch 2] Batch 107, Loss 0.48082560300827026\n","[Training Epoch 2] Batch 108, Loss 0.53755784034729\n","[Training Epoch 2] Batch 109, Loss 0.485380083322525\n","[Training Epoch 2] Batch 110, Loss 0.5074920654296875\n","[Training Epoch 2] Batch 111, Loss 0.5044665336608887\n","[Training Epoch 2] Batch 112, Loss 0.5120794177055359\n","[Training Epoch 2] Batch 113, Loss 0.4988369941711426\n","[Training Epoch 2] Batch 114, Loss 0.5020177960395813\n","[Training Epoch 2] Batch 115, Loss 0.48206350207328796\n","[Training Epoch 2] Batch 116, Loss 0.4811880588531494\n","[Training Epoch 2] Batch 117, Loss 0.4964628517627716\n","[Training Epoch 2] Batch 118, Loss 0.5007526874542236\n","[Training Epoch 2] Batch 119, Loss 0.4674580693244934\n","[Training Epoch 2] Batch 120, Loss 0.4912119507789612\n","[Training Epoch 2] Batch 121, Loss 0.4923819899559021\n","[Training Epoch 2] Batch 122, Loss 0.48330917954444885\n","[Training Epoch 2] Batch 123, Loss 0.5185446739196777\n","[Training Epoch 2] Batch 124, Loss 0.4802384674549103\n","[Training Epoch 2] Batch 125, Loss 0.5399459004402161\n","[Training Epoch 2] Batch 126, Loss 0.5464333891868591\n","[Training Epoch 2] Batch 127, Loss 0.5144415497779846\n","[Training Epoch 2] Batch 128, Loss 0.49414849281311035\n","[Training Epoch 2] Batch 129, Loss 0.49701130390167236\n","[Training Epoch 2] Batch 130, Loss 0.5318160057067871\n","[Training Epoch 2] Batch 131, Loss 0.5037930011749268\n","[Training Epoch 2] Batch 132, Loss 0.49571141600608826\n","[Training Epoch 2] Batch 133, Loss 0.5184788107872009\n","[Training Epoch 2] Batch 134, Loss 0.4937383234500885\n","[Training Epoch 2] Batch 135, Loss 0.4863954484462738\n","[Training Epoch 2] Batch 136, Loss 0.49066004157066345\n","[Training Epoch 2] Batch 137, Loss 0.5238529443740845\n","[Training Epoch 2] Batch 138, Loss 0.48391854763031006\n","[Training Epoch 2] Batch 139, Loss 0.5047045946121216\n","[Training Epoch 2] Batch 140, Loss 0.492121160030365\n","[Training Epoch 2] Batch 141, Loss 0.5012322664260864\n","[Training Epoch 2] Batch 142, Loss 0.4985513687133789\n","[Training Epoch 2] Batch 143, Loss 0.4949186146259308\n","[Training Epoch 2] Batch 144, Loss 0.50722736120224\n","[Training Epoch 2] Batch 145, Loss 0.49870777130126953\n","[Training Epoch 2] Batch 146, Loss 0.5057783126831055\n","[Training Epoch 2] Batch 147, Loss 0.4922686815261841\n","[Training Epoch 2] Batch 148, Loss 0.5177101492881775\n","[Training Epoch 2] Batch 149, Loss 0.5081133246421814\n","[Training Epoch 2] Batch 150, Loss 0.49693265557289124\n","[Training Epoch 2] Batch 151, Loss 0.5077624320983887\n","[Training Epoch 2] Batch 152, Loss 0.49070924520492554\n","[Training Epoch 2] Batch 153, Loss 0.4916497468948364\n","[Training Epoch 2] Batch 154, Loss 0.48796647787094116\n","[Training Epoch 2] Batch 155, Loss 0.464786171913147\n","[Training Epoch 2] Batch 156, Loss 0.5202230215072632\n","[Training Epoch 2] Batch 157, Loss 0.5196656584739685\n","[Training Epoch 2] Batch 158, Loss 0.4876965284347534\n","[Training Epoch 2] Batch 159, Loss 0.49256494641304016\n","[Training Epoch 2] Batch 160, Loss 0.513813316822052\n","[Training Epoch 2] Batch 161, Loss 0.5263447761535645\n","[Training Epoch 2] Batch 162, Loss 0.49507203698158264\n","[Training Epoch 2] Batch 163, Loss 0.4884227514266968\n","[Training Epoch 2] Batch 164, Loss 0.5125143527984619\n","[Training Epoch 2] Batch 165, Loss 0.4867887496948242\n","[Training Epoch 2] Batch 166, Loss 0.49507540464401245\n","[Training Epoch 2] Batch 167, Loss 0.5184822678565979\n","/content/drive/MyDrive/Neural-CF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 2] HR = 0.0832, NDCG = 0.0394\n","Epoch 3 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 3] Batch 0, Loss 0.49190956354141235\n","[Training Epoch 3] Batch 1, Loss 0.49309417605400085\n","[Training Epoch 3] Batch 2, Loss 0.49210312962532043\n","[Training Epoch 3] Batch 3, Loss 0.48846298456192017\n","[Training Epoch 3] Batch 4, Loss 0.48811075091362\n","[Training Epoch 3] Batch 5, Loss 0.4847017824649811\n","[Training Epoch 3] Batch 6, Loss 0.5186163187026978\n","[Training Epoch 3] Batch 7, Loss 0.5162910223007202\n","[Training Epoch 3] Batch 8, Loss 0.4866756796836853\n","[Training Epoch 3] Batch 9, Loss 0.49488183856010437\n","[Training Epoch 3] Batch 10, Loss 0.5140889883041382\n","[Training Epoch 3] Batch 11, Loss 0.5137435793876648\n","[Training Epoch 3] Batch 12, Loss 0.4803119897842407\n","[Training Epoch 3] Batch 13, Loss 0.5036801695823669\n","[Training Epoch 3] Batch 14, Loss 0.5127463340759277\n","[Training Epoch 3] Batch 15, Loss 0.5140252113342285\n","[Training Epoch 3] Batch 16, Loss 0.5201597213745117\n","[Training Epoch 3] Batch 17, Loss 0.5189030170440674\n","[Training Epoch 3] Batch 18, Loss 0.4996945261955261\n","[Training Epoch 3] Batch 19, Loss 0.4980751872062683\n","[Training Epoch 3] Batch 20, Loss 0.5225546956062317\n","[Training Epoch 3] Batch 21, Loss 0.4900085926055908\n","[Training Epoch 3] Batch 22, Loss 0.49793004989624023\n","[Training Epoch 3] Batch 23, Loss 0.4958265721797943\n","[Training Epoch 3] Batch 24, Loss 0.48407626152038574\n","[Training Epoch 3] Batch 25, Loss 0.4994131326675415\n","[Training Epoch 3] Batch 26, Loss 0.5062084197998047\n","[Training Epoch 3] Batch 27, Loss 0.5041409134864807\n","[Training Epoch 3] Batch 28, Loss 0.5186911225318909\n","[Training Epoch 3] Batch 29, Loss 0.49647602438926697\n","[Training Epoch 3] Batch 30, Loss 0.472464919090271\n","[Training Epoch 3] Batch 31, Loss 0.49195173382759094\n","[Training Epoch 3] Batch 32, Loss 0.519119381904602\n","[Training Epoch 3] Batch 33, Loss 0.47986724972724915\n","[Training Epoch 3] Batch 34, Loss 0.5052904486656189\n","[Training Epoch 3] Batch 35, Loss 0.5058426856994629\n","[Training Epoch 3] Batch 36, Loss 0.515437662601471\n","[Training Epoch 3] Batch 37, Loss 0.46987253427505493\n","[Training Epoch 3] Batch 38, Loss 0.4812646508216858\n","[Training Epoch 3] Batch 39, Loss 0.5173036456108093\n","[Training Epoch 3] Batch 40, Loss 0.5323905944824219\n","[Training Epoch 3] Batch 41, Loss 0.4800795018672943\n","[Training Epoch 3] Batch 42, Loss 0.5172538757324219\n","[Training Epoch 3] Batch 43, Loss 0.5098986029624939\n","[Training Epoch 3] Batch 44, Loss 0.5003558397293091\n","[Training Epoch 3] Batch 45, Loss 0.522000789642334\n","[Training Epoch 3] Batch 46, Loss 0.4946356415748596\n","[Training Epoch 3] Batch 47, Loss 0.5108944177627563\n","[Training Epoch 3] Batch 48, Loss 0.4933040142059326\n","[Training Epoch 3] Batch 49, Loss 0.49106380343437195\n","[Training Epoch 3] Batch 50, Loss 0.5140644311904907\n","[Training Epoch 3] Batch 51, Loss 0.5153297781944275\n","[Training Epoch 3] Batch 52, Loss 0.4888778328895569\n","[Training Epoch 3] Batch 53, Loss 0.4961719214916229\n","[Training Epoch 3] Batch 54, Loss 0.4980979561805725\n","[Training Epoch 3] Batch 55, Loss 0.47398585081100464\n","[Training Epoch 3] Batch 56, Loss 0.5190012454986572\n","[Training Epoch 3] Batch 57, Loss 0.517533540725708\n","[Training Epoch 3] Batch 58, Loss 0.5216182470321655\n","[Training Epoch 3] Batch 59, Loss 0.5061522722244263\n","[Training Epoch 3] Batch 60, Loss 0.4969354569911957\n","[Training Epoch 3] Batch 61, Loss 0.5342245101928711\n","[Training Epoch 3] Batch 62, Loss 0.4815784692764282\n","[Training Epoch 3] Batch 63, Loss 0.4865496754646301\n","[Training Epoch 3] Batch 64, Loss 0.5002418756484985\n","[Training Epoch 3] Batch 65, Loss 0.5018896460533142\n","[Training Epoch 3] Batch 66, Loss 0.4973144829273224\n","[Training Epoch 3] Batch 67, Loss 0.5063637495040894\n","[Training Epoch 3] Batch 68, Loss 0.49438297748565674\n","[Training Epoch 3] Batch 69, Loss 0.4840018153190613\n","[Training Epoch 3] Batch 70, Loss 0.5131236910820007\n","[Training Epoch 3] Batch 71, Loss 0.5074920058250427\n","[Training Epoch 3] Batch 72, Loss 0.4830111265182495\n","[Training Epoch 3] Batch 73, Loss 0.4947124123573303\n","[Training Epoch 3] Batch 74, Loss 0.4906631112098694\n","[Training Epoch 3] Batch 75, Loss 0.5041630268096924\n","[Training Epoch 3] Batch 76, Loss 0.49855485558509827\n","[Training Epoch 3] Batch 77, Loss 0.4791312515735626\n","[Training Epoch 3] Batch 78, Loss 0.5227742195129395\n","[Training Epoch 3] Batch 79, Loss 0.5050226449966431\n","[Training Epoch 3] Batch 80, Loss 0.5006937980651855\n","[Training Epoch 3] Batch 81, Loss 0.47988361120224\n","[Training Epoch 3] Batch 82, Loss 0.4982427954673767\n","[Training Epoch 3] Batch 83, Loss 0.5111055970191956\n","[Training Epoch 3] Batch 84, Loss 0.506359338760376\n","[Training Epoch 3] Batch 85, Loss 0.5171038508415222\n","[Training Epoch 3] Batch 86, Loss 0.5192915797233582\n","[Training Epoch 3] Batch 87, Loss 0.4938444495201111\n","[Training Epoch 3] Batch 88, Loss 0.4886614680290222\n","[Training Epoch 3] Batch 89, Loss 0.49738746881484985\n","[Training Epoch 3] Batch 90, Loss 0.50441575050354\n","[Training Epoch 3] Batch 91, Loss 0.49969154596328735\n","[Training Epoch 3] Batch 92, Loss 0.5120340585708618\n","[Training Epoch 3] Batch 93, Loss 0.5227987766265869\n","[Training Epoch 3] Batch 94, Loss 0.4833633005619049\n","[Training Epoch 3] Batch 95, Loss 0.48775506019592285\n","[Training Epoch 3] Batch 96, Loss 0.4881567358970642\n","[Training Epoch 3] Batch 97, Loss 0.48048096895217896\n","[Training Epoch 3] Batch 98, Loss 0.4885879158973694\n","[Training Epoch 3] Batch 99, Loss 0.5572433471679688\n","[Training Epoch 3] Batch 100, Loss 0.48952651023864746\n","[Training Epoch 3] Batch 101, Loss 0.5037133693695068\n","[Training Epoch 3] Batch 102, Loss 0.5035126209259033\n","[Training Epoch 3] Batch 103, Loss 0.47867119312286377\n","[Training Epoch 3] Batch 104, Loss 0.47287076711654663\n","[Training Epoch 3] Batch 105, Loss 0.48273617029190063\n","[Training Epoch 3] Batch 106, Loss 0.5141540169715881\n","[Training Epoch 3] Batch 107, Loss 0.5122267603874207\n","[Training Epoch 3] Batch 108, Loss 0.5252116322517395\n","[Training Epoch 3] Batch 109, Loss 0.526977002620697\n","[Training Epoch 3] Batch 110, Loss 0.5048490762710571\n","[Training Epoch 3] Batch 111, Loss 0.48944053053855896\n","[Training Epoch 3] Batch 112, Loss 0.5064259171485901\n","[Training Epoch 3] Batch 113, Loss 0.46485647559165955\n","[Training Epoch 3] Batch 114, Loss 0.49867433309555054\n","[Training Epoch 3] Batch 115, Loss 0.5102073550224304\n","[Training Epoch 3] Batch 116, Loss 0.49274158477783203\n","[Training Epoch 3] Batch 117, Loss 0.5255463123321533\n","[Training Epoch 3] Batch 118, Loss 0.4931753873825073\n","[Training Epoch 3] Batch 119, Loss 0.5087211728096008\n","[Training Epoch 3] Batch 120, Loss 0.5031867027282715\n","[Training Epoch 3] Batch 121, Loss 0.5150477886199951\n","[Training Epoch 3] Batch 122, Loss 0.5139790177345276\n","[Training Epoch 3] Batch 123, Loss 0.5204145908355713\n","[Training Epoch 3] Batch 124, Loss 0.48992007970809937\n","[Training Epoch 3] Batch 125, Loss 0.4876418709754944\n","[Training Epoch 3] Batch 126, Loss 0.5008957982063293\n","[Training Epoch 3] Batch 127, Loss 0.496498167514801\n","[Training Epoch 3] Batch 128, Loss 0.48329031467437744\n","[Training Epoch 3] Batch 129, Loss 0.4889126121997833\n","[Training Epoch 3] Batch 130, Loss 0.5074735879898071\n","[Training Epoch 3] Batch 131, Loss 0.4856431186199188\n","[Training Epoch 3] Batch 132, Loss 0.47749024629592896\n","[Training Epoch 3] Batch 133, Loss 0.4987089931964874\n","[Training Epoch 3] Batch 134, Loss 0.5192172527313232\n","[Training Epoch 3] Batch 135, Loss 0.46817129850387573\n","[Training Epoch 3] Batch 136, Loss 0.5072553157806396\n","[Training Epoch 3] Batch 137, Loss 0.49145418405532837\n","[Training Epoch 3] Batch 138, Loss 0.4964395761489868\n","[Training Epoch 3] Batch 139, Loss 0.4834960699081421\n","[Training Epoch 3] Batch 140, Loss 0.5194061994552612\n","[Training Epoch 3] Batch 141, Loss 0.5046864748001099\n","[Training Epoch 3] Batch 142, Loss 0.4964272081851959\n","[Training Epoch 3] Batch 143, Loss 0.5192184448242188\n","[Training Epoch 3] Batch 144, Loss 0.5058719515800476\n","[Training Epoch 3] Batch 145, Loss 0.536049485206604\n","[Training Epoch 3] Batch 146, Loss 0.504543662071228\n","[Training Epoch 3] Batch 147, Loss 0.5071744918823242\n","[Training Epoch 3] Batch 148, Loss 0.48298120498657227\n","[Training Epoch 3] Batch 149, Loss 0.5059870481491089\n","[Training Epoch 3] Batch 150, Loss 0.4961441159248352\n","[Training Epoch 3] Batch 151, Loss 0.5316128730773926\n","[Training Epoch 3] Batch 152, Loss 0.5011647939682007\n","[Training Epoch 3] Batch 153, Loss 0.5201172828674316\n","[Training Epoch 3] Batch 154, Loss 0.47496312856674194\n","[Training Epoch 3] Batch 155, Loss 0.5164321660995483\n","[Training Epoch 3] Batch 156, Loss 0.5119211673736572\n","[Training Epoch 3] Batch 157, Loss 0.49224498867988586\n","[Training Epoch 3] Batch 158, Loss 0.49089211225509644\n","[Training Epoch 3] Batch 159, Loss 0.49045708775520325\n","[Training Epoch 3] Batch 160, Loss 0.48446041345596313\n","[Training Epoch 3] Batch 161, Loss 0.5217787623405457\n","[Training Epoch 3] Batch 162, Loss 0.5075849890708923\n","[Training Epoch 3] Batch 163, Loss 0.4648294746875763\n","[Training Epoch 3] Batch 164, Loss 0.4950614273548126\n","[Training Epoch 3] Batch 165, Loss 0.5063838958740234\n","[Training Epoch 3] Batch 166, Loss 0.5074240565299988\n","[Training Epoch 3] Batch 167, Loss 0.4575951099395752\n"]}]},{"cell_type":"markdown","source":["# Evaluated with Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG)"],"metadata":{"id":"feJEitTGhtPS"}},{"cell_type":"code","source":["!lsof -i :6006"],"metadata":{"id":"UfqFfGIX2llM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701339114917,"user_tz":300,"elapsed":887,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}},"outputId":"b3a6e45b-396c-485b-f726-0dd086cc6ca3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n","tensorboa 4311 root   14u  IPv4 117764      0t0  TCP localhost:6006 (LISTEN)\n"]}]},{"cell_type":"code","source":["!kill -9 4311"],"metadata":{"id":"Oxv9531l2u5i","executionInfo":{"status":"ok","timestamp":1701339122511,"user_tz":300,"elapsed":394,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Load Tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir '/content/drive/MyDrive/Neural-CF/Torch-NCF/runs/pretrain_neumf_factor8neg4' --port 6006"],"metadata":{"id":"QLUBSQOHhwQ3"},"execution_count":null,"outputs":[]}]}