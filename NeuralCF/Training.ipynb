{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1117,"status":"ok","timestamp":1701660529778,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"},"user_tz":300},"id":"tIPJcTnWxpYU","outputId":"20f609c3-d97b-4260-979d-e896ba842f6a"},"outputs":[],"source":["import numpy as np\n","import json\n","import os\n","import pandas as pd\n","from typing import Tuple, List, Any"]},{"cell_type":"markdown","metadata":{"id":"GohCVvVQ1XzK"},"source":["# Load Data Test"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"uFzz9aDfw_h0"},"outputs":[],"source":["# def load_data(\n","#     data_folder: str,\n","#     filenames=(\"yelp_academic_dataset_user.json\", \"yelp_academic_dataset_business.json\", \"yelp_academic_dataset_review.json\")\n","#     ) -> Tuple[List[Any], ...]:\n","#     return tuple(map(\n","#                   lambda f: list(map(json.loads, open(os.path.join(data_folder, f), \"r\", encoding=\"utf-8\").readlines())),\n","#                   filenames\n","#                 ))\n","\n","# (subset_user_data, subset_business_data, subset_review_data) = load_data(os.path.abspath(\"../data\"),\n","#                                                                         (\"sample_users.json\",\n","#                                                                          \"sample_business.json\",\n","#                                                                          \"sample_reviews_train.json\"))\n","subset_user_data = json.load(open(\"../data/sample_users.json\", \"r\"))\n","subset_business_data = json.load(open(\"../data/sample_business.json\", \"r\"))\n","subset_review_data = json.load(open(\"../data/sample_reviews_train.json\", \"r\"))\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":162,"status":"ok","timestamp":1701366500360,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"},"user_tz":300},"id":"nd0nGvFUyPzs","outputId":"104ba6ae-193f-4f0a-fe16-dbb4532b4702"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny', 'cool', 'elite', 'friends', 'fans', 'average_stars', 'compliment_hot', 'compliment_more', 'compliment_profile', 'compliment_cute', 'compliment_list', 'compliment_note', 'compliment_plain', 'compliment_cool', 'compliment_funny', 'compliment_writer', 'compliment_photos'])\n","dict_keys(['business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours'])\n","dict_keys(['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny', 'cool', 'text', 'date'])\n"]}],"source":["print(subset_user_data[0].keys())\n","print(subset_business_data[0].keys())\n","print(subset_review_data[0].keys())"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1701366503353,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"},"user_tz":300},"id":"LULPTicDfDJd","outputId":"07ba05f5-51c2-461d-9ce5-53e7583c73d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Philadelphia', 'Ashland City', 'Tampa Bay', 'Indianapolis', 'Reno', 'Ardmore', 'Alton', 'Bala Cynwyd', 'Williamstown', 'Glenolden', 'Wesley Chapel', 'Santa Barbara', 'New Orleans', 'Camden', 'Nashville', 'Tampa', 'Fairview Heights', 'Treasure Island', 'Saint Louis', 'Tucson', 'Largo', 'Madison', 'Warrington', 'St. Louis', 'Lutz', 'King of Prussia', 'Clearwater', 'Franklin', 'Meridian', 'St Albert', 'Downingtown', 'Virginia City', 'Saint Petersburg', 'Brandon', 'Exton', 'Odessa', 'Brownsburg', 'Edmonton', 'Lansdale', 'Goodlettsville', 'Narberth', 'Oldsmar', 'Langhorne', 'Haddon Heights', 'Goleta', 'Brookhaven', 'Noblesville', 'Metairie', 'Cherry Hill', 'Isla Vista', 'Boise', 'Mount Juliet', 'Carmel', 'Fishers', 'Conshohocken', 'Hernando Beach', 'Florissant', 'Newtown', 'Chadds Ford', 'Plant City', 'Spring Hill', 'Ruskin', 'Town and Country', 'Lebanon', 'Bryn Mawr', 'Sparks', 'Jennings', 'Sherwood Park', 'Carpinteria', 'Voorhees', 'Spring City', 'Southampton', 'Riverside', 'St Petersburg', 'Godfrey', 'Norristown', 'Marlton', 'Huntingdon Valley', 'Media', 'Creve Coeur', 'Kenner', 'Ballwin', 'Turnersville', 'Doylestown', 'St Louis', 'Trenton', 'Bensalem', 'Bridgeport', 'Wayne', 'Moorestown', 'Mount Laurel Township', 'Glen Carbon', 'Washington Crossing', 'Zionsville', 'Drexel Hill', 'Folsom', 'Fenton', 'Eagle', 'Kimberton', 'East Norriton', 'Jenkintown', 'West Chester', 'Dublin', 'Harahan', 'Old Hickory', 'Palm Harbor', 'University City', 'Seminole', 'Pitman', 'Mullica Hill', 'Malvern', 'Ferguson', 'Harvey', 'Port Richey', 'Springfield', 'Newtown Square', 'Des Peres', 'Smyrna', 'St. Petersburg', 'Mount Laurel', 'Pinellas Park', 'Woolwich Twp', 'Marana', 'Belle Chasse', 'Brentwood', 'Glenside', 'North Wales', 'Dunedin', 'Gulfport', 'Montgomeryville', 'Darby', 'Avon', 'Woodstown', 'Belleville', 'Gallatin', 'Willow Grove', 'Maple Shade', 'Barrington', 'Cahokia', 'Chalfont', 'Whiteland', 'Sewell', 'Antioch', 'Swedesboro', 'Levittown', 'St Pete Beach', 'Maplewood', 'Freeburg', 'Eddystone', \"Land O' Lakes\", 'Audubon ', 'Holiday', 'Aston', 'Chesterfield', 'Prospect Park', 'Willingboro Township', 'Morton', 'King Of Prussia', 'New Port Richey', 'St. Pete Beach', 'Clearwater Beach', 'Swansea', 'New Hope', 'Wilmington', 'Blue Bell', 'Sicklerville', 'Plainfield', 'Lansdowne', 'Seffner', 'Brooklawn', 'Phoenixville', 'Woodbury', 'Collinsville', 'Hudson', 'Oaks', 'Broomall', 'Abington', 'Atco', 'Warminster', 'Blackwood', 'Ridley Park', 'Newark', 'Penns Grove', 'Saint Charles', 'Gilbertsville', 'Mt. Juliet', 'Essington', 'Morrisville', 'Maryland Heights', 'Schwenksville', 'Saint Pete Beach', 'Claymont', 'Valrico', 'Chester', 'Gretna', 'Riverview', 'Secane', 'Hendersonville', 'Bristol', 'St.Ann', 'Elverson', 'St Charles', 'Greenville', 'Bordentown', 'Hatboro', 'New Castle', 'Fairless Hills', 'Dover', 'Upper Darby', 'Spring House', 'Greenwood', 'Voorhees Township', 'High Ridge', 'Merchantville', 'Manchester', 'O Fallon', 'Spanish Springs', 'Wimauma', 'Mooresville', 'Willingboro', 'Mt Juliet', 'Haddonfield', 'Woodbury Heights', 'Trappe', 'Green Valley', 'Shamong', 'Jefferson', 'Mt. Laurel', 'Medford', 'Richboro', 'Hockessin', 'Pottstown', 'Kennett Square', 'Beech Grove', 'Hamilton Township', 'Plymouth Meeting', 'Dresher', 'Berlin', 'Paulsboro', 'Pennsville', 'Millstadt', 'Holland', 'Collingswood', 'Sahuarita', 'Clementon', 'Trinity', 'Collegeville', 'Valley Park', 'Royersford', 'Yeadon', 'Saint Ann', 'Feasterville', 'Dupo', 'Garden City', 'Plumsteadville', 'Temple Terrace', 'Bridgeton', 'Westwego', 'Deptford', 'Westville', 'Nolensville', 'Wood River', 'NEW PORT RICHEY', 'Safety Harbor', 'Vail', 'Mansfield', 'South Cinnaminson', 'Mt Laurel', 'Elkins Park', 'Clayton', 'Webster Groves', 'Apollo Beach', 'Oro Valley', 'Edwardsville', 'Feasterville-Trevose', 'Tarpon Springs', 'Havertown', 'Oaklyn', 'Southport', 'Runnemede', 'Hermitage', 'Paoli', 'Columbia', 'Madeira Beach', 'Glassboro', 'Yardley', 'Hazelwood', 'Arnold', 'South Pasadena', 'Folcroft', 'West Conshohocken', 'Mendenhall', 'Delran', 'Berwyn', 'TAMPA', 'Berlin Township', 'Monroe Township', 'Pennsauken', 'Ewing', 'Hatfield', 'Mercerville', 'St. Albert', \"O'Fallon\", 'Rockledge', 'Perkiomenville', 'Linwood', 'Mount Ephraim', 'Land O Lakes', 'Burlington', 'Indian Rocks Beach', 'New Britain', 'Marrero', 'Sun City', \"O'fallon\", 'Trevose', 'Croydon', 'Lindenwold', 'Speedway', 'Oreland', 'Thorndale', 'St. Charles', 'Pittsgrove Township', 'Talleyville', 'La Vergne', 'Tabernacle', 'Lawrence', 'Westtown', 'Clifton Heights', 'West Deptford', 'West Berlin', 'St Louis Downtown', 'Chester Springs', 'PHILA', 'Lafayette Hill', 'Boothwyn', 'Flourtown', 'Montecito', 'Telford', 'Palmyra', 'Buckingham', 'Edgewater Park', 'Pike Creek', 'Affton', 'Pontoon Beach', 'Delanco', 'Pennsburg', 'Feasterville Trevose', 'Horsham', 'Chalmette', 'Audubon', 'Thorofare', 'New Palestine', 'Granite City', 'Haddon Township', 'Carrollwood', 'Trainer', 'Zephyrhills', 'Belleair Bluffs', 'Twn N Cntry', 'Hamilton', 'Town And Country', 'NEW ORLEANS', 'Concordville', 'Imperial', 'RENO', 'Westampton', 'Lithia', 'Perkasie', 'Unionville', 'Quakertown', 'TUCSON', 'Sun City Center', 'Clarksboro', 'Tinicum', 'Oakville', 'St Ann', 'Villanova', 'Ambler', 'Lumberton', 'Gibsonton', 'Glen Mills', 'Bellmawr', 'Devon', 'Kenneth City', 'Westmont', 'Abington Township', 'Marcus Hook', 'Trooper', 'W.Chester', 'Pinecrest West Park', 'Deptford Township', 'Lawnside', 'Pipersville', 'Bellevue', 'Tierra Verde', 'Franklinville', 'lawrence', 'Laurel Springs', 'Indianapolis ', 'Souderton', 'Pennsauken Township', 'wilmington', 'Mount Holly', 'Eagleville', 'Birchrunville', 'Ewing Township', 'Salem', 'Mount Lemmon', 'Breckenridge Hills', 'Rosemont', 'Waterloo', 'Kirkwood', 'Boise City', 'Skippack', 'Lambertville', 'Pittsgrove', 'Joelton', 'McCordsville', 'Troy', 'Whitestown', 'Ladue', 'Evesham Township', 'East Nashville', 'Normandy', 'Verdi', 'Berry Hill', 'Primos', 'Glendora', 'Ashland', 'Bethalto', 'Lester', 'River Ridge', 'Valencia West', 'Maryville', 'Wyncote', 'Mantua', 'Holicong', 'Shiloh', 'Newport', 'Pine HIll', 'Coatesville', 'Cinnaminson', 'Fairview Hts.', 'Gwynedd Valley', 'Terrytown', 'EdMonton', 'Palmetto', 'Maple Glen', 'Violet', 'Kulpsville', 'Crum Lynne', 'Inglewood', 'Lahaska', 'Lawrence Township', 'Haverford', 'Southwest Tampa', 'Columbus', 'Hainesport', 'Saint Albert', 'Frazer', 'WEST CHESTER', 'Titusville', 'Ivyland', 'Christiana', 'Sun Valley', 'Harleysville', 'Hulmeville', 'Swarthmore', 'Beverly', 'Sappington', 'Town n Country', \"Land O'Lakes\", 'Merion Station', 'Overland', 'Arabi', 'Jamison', 'Newfield', 'Mantua Township', 'LOWER PROVIDENCE', 'Frontenac', 'Pleasant View', 'Gibbsboro', 'Catalina', 'White House', 'Aldan', 'Woodlyn', 'NORRISTOWN', 'Gloucester City', 'North Redington Beach', 'Boyertown', 'Wynnewood', 'St Pete', 'Bargersville', 'Colmar', 'Mt Laurel Twp, NJ', 'Skippack Village', 'West Mount Holly', 'Radnor', 'Mascoutah', 'Indian Shores', 'Penndel', 'Pennsville Township', 'Eastampton', 'Gloucester Township', 'Florence', 'Stratford', 'Thonosassa', 'Summerland', 'Cheltenham', 'Danville', 'Richmond Heights', 'Balm', 'Upper Chichester', 'Bethel', 'East St. Louis', 'Glenoldan', 'Fort Washington', 'Meraux', 'Somerdale', 'Haddon Twp', 'Mont Clare', 'reno', 'Concord Township', 'Blvd', 'Sharon Hill', 'East Lansdowne', 'Gibbstown', 'Scott Air Force Base', 'Magnolia', 'San Antonio', 'Limerick', 'Cheltenham Township', 'Lansdale ', 'Smithton', 'Jeffersonville', 'Cedar Brook', 'CLEARWATER', 'Olivette', 'indianopolis', 'Norwood', 'Sunset Hills', 'Thonotosassa', 'Camby', 'St.Petersburg', 'St.Louis', 'ST LOUIS', 'Earth City', 'Avondale', 'Bristol Twp', 'Kenneth', 'Crestwood', 'Manayunk', 'Glendale', 'Caseyville', 'Tarpon springs', 'Pineville', 'Bucks', 'Southwest Philadelphia', 'Greenfield', 'Bethel Township', 'Evesham', 'Saint Bernard', 'Vincentown', 'Wrightstown', 'West Point', 'Shrewsbury', 'Garnet Valley', 'Washington Twp', 'Maple Shade Township', 'MADISON', 'Riveridge', 'Collingdale', 'Beaumont', 'Tuscon', 'Carneys Point', 'Bridge City', 'Pine Hill', 'Rural Hill', 'Luling', 'Saint Rose', 'Fairview Hts', 'St. Ann', 'Elmer', 'Wenonah', 'Eastampton Township', 'Woolwich Twp.', 'Wanamaker', 'Saint Peters', 'Greater Northdale', 'Freehold', 'Masaryktown', \"Town 'n' Country\", 'Lutz fl', 'Holmes', 'Hammonton', 'Pleasant Township', 'Gulph Mills', 'SPRINGHILL', 'ST. PETE BEACH', 'Mehlville', 'Redingtn Shor', 'Chesilhurst', 'Twin Oaks', 'North Redington Bch', 'Elsmere', 'Logan Township', 'Pilesgrove', 'Mt.Laurel', 'Mt.Juliet', 'sewell', 'Wallingford', 'Fox Street', 'Fairview Village', 'Medford Lakes', 'Castleton', 'South Tucson', 'Glenmoore', 'Linfield', 'Wyndmoor', 'Kingston Springs', 'Erial', 'Eddington', 'Fairmont City', 'Dade City', 'Mount Royal', 'Zieglerville', 'Waterford Works', 'Center Square', \"O' Fallon\", 'Bucktown', 'Cumberland', 'West Chester PA', 'Redington Shores', 'Wilmington ', 'Lima', 'Greenbrier', 'Royersford ', 'Roslyn', 'Sellersville', 'Chesterbrook', 'Largo (Walsingham)', 'Phila', 'Land o Lakes', 'Woolwich Township', 'Enoch', 'Citrus Park', 'LITHIA', 'POTTSTOWN', 'Toughkenamon', 'LOWER MERION', 'Town & Country', 'BRANDON', 'Riverton', 'Warrington Township', 'East Alton', 'Belleair Blf', 'Cottage Hills', 'Green Lane', 'East St Louis', 'Metarie', 'Florence Township', 'Stowe', 'Carversville', 'MEDIA', 'Mccordsville', 'Lawrenceville', 'Stead', 'Whites Creek', 'South Tampa', 'Berkeley', 'Saint John', 'Goodletsville', 'Nampa', 'Sherwood', 'Corona De Tucson', 'Bensalem Township', 'LANSDALE', 'Newportville', 'NORTH WALES', 'Lower Southampton Township', 'Corona de Tucson', 'East Falls', 'Riverview Fl', 'National Park', 'Bellefontaine', 'Palm harbor', 'Upper Pottsgrove', 'W. Berlin', 'Ozona', 'Erdenheim', 'Town and Country ', 'Burlington Township', 'Lederach', 'Montchanin', 'Worcester', 'Saint Louis,', 'West Trenton', 'Elmwood', 'Upland', 'Gwynedd', 'Blackwood ', 'Monroeville', 'Line Lexington', 'Plainfiled', 'Telford ', 'LANGHORNE', 'St.Charles', 'TRINITY', 'Philly', 'Malaga', 'Feasterville-trevose', 'Alloway', 'Tierre Verde', 'ST. PETERSBURG', 'Sanatoga', 'Foster Pond', 'Kuna', 'Santa  Barbara', 'West Collingswood Heights', 'Tampa,Fl', 'Chichester', 'Nashville ', 'Lionville', 'Tampa Palms', 'Silverdale', 'St. Peters', 'Kimmswick', 'Washington Township', 'Blooming Glen', 'St Louis County', 'goodlettsville', 'Bel Ridge', 'Harrison Township', 'Hilltown', 'LARGO', 'INpolis', 'Delran Township', \"Land O'lakes\", 'Cane Ridge', 'St. Leo', 'Kirklyn', 'Philadelphia ', 'Clearwater/ Countryside', 'Hi-Nella', 'RIVERVIEW', 'TEMPLE TERR', 'New Whiteland', 'RADNOR', 'St. Louis County', 'Pass-a-Grille Beach', 'Cedars', 'WILLOW GROVE', 'Belle Meade', 'Green Park', 'Middletown', 'Hillsborough County', \"Town 'N' Country\", 'S.Pasadena', 'Downtown', 'Churchville', 'Winchester', 'St.  Charles', 'Mount Holly,', 'Tampa Florida', 'UPPER MORELAND', 'philadelphia']\n"]}],"source":["# Display City names in the subset data\n","city_list = []\n","for i, business in enumerate(subset_business_data):\n","  if business['city'] not in city_list:\n","    city_list.append(business['city'])\n","print(city_list)"]},{"cell_type":"markdown","metadata":{"id":"NiZIFY1rjptM"},"source":["# Data Preprocess Test (For Debugging)\n","For debugging only, no need to run\n","\n","Ref: https://github.com/zhrlove/NCF/tree/master \\\n","Ref2: https://github.com/hexiangnan/sigir16-eals\n","\n","NCF: https://github.com/hexiangnan/neural_collaborative_filtering \\\n","NCF (torch): https://github.com/yihong-chen/neural-collaborative-filtering/tree/master\n","\n","Training Input: `userID::itemID::rating::timestamp (if have)`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166,"status":"ok","timestamp":1700734111697,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"},"user_tz":300},"id":"_pZcme74BLoi","outputId":"df55fadb-5105-4ca3-b5d1-3182ee2572b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Citizens Bank Park\n","Kathleen\n"]}],"source":["for i, business in enumerate(subset_business_data):\n","  if business['business_id'] == 'gGyqnAlpFrka_qzpO7j4lQ':\n","    print(business['name'])\n","for i, user in enumerate(subset_user_data):\n","  if user['user_id'] == 'GcdYgbaF75vj7RO6EZhPOQ':\n","    print(user['name'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":722,"status":"ok","timestamp":1700734114706,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"},"user_tz":300},"id":"N7eTqCtjKLLX","outputId":"8163cec2-0a0c-43cd-ce23-79e9e590285e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Unique Users: 961\n","Number of Unique Businesses: 1000\n","                  user_id             business_id  stars\n","0  -3s52C4zL_DHRK0ULG6qtg  -kqjc8DxxRac4cz2qTKCLw    4.0\n","1  -3s52C4zL_DHRK0ULG6qtg  0QYWhij_YZ7Lyk9F6213Sg    5.0\n","2  -3s52C4zL_DHRK0ULG6qtg  1YflE3DkiCZGgLnf3paLnA    5.0\n","3  -3s52C4zL_DHRK0ULG6qtg  2BMk_drsikKWslJCXmQtjQ    2.0\n","4  -3s52C4zL_DHRK0ULG6qtg  2IahpaBR4U2Kdy9HF28EQA    2.5\n","5  -3s52C4zL_DHRK0ULG6qtg  33JlrWf0kmHd2VzW58Wp0g    3.0\n","6  -3s52C4zL_DHRK0ULG6qtg  6t0sNev22mcbvOB4gYVVOw    3.0\n","7  -3s52C4zL_DHRK0ULG6qtg  89SD5fNDDnJj-ITB40hLsQ    1.0\n","8  -3s52C4zL_DHRK0ULG6qtg  8O35ji_yOMVJmZ6bl96yhQ    3.0\n","9  -3s52C4zL_DHRK0ULG6qtg  8QZJvkx29OQNZgrM53aVbw    4.0\n","   userId  itemId  stars\n","0       0       0    4.0\n","1       0       1    5.0\n","2       0       2    5.0\n","3       0       3    2.0\n","4       0       4    2.5\n","5       0       5    3.0\n","6       0       6    3.0\n","7       0       7    1.0\n","8       0       8    3.0\n","9       0       9    4.0\n","Range of userId is [0, 960]\n","Range of itemId is [0, 999]\n"]}],"source":["# Reindex\n","user_item_interactions = subset_review_data\n","df = pd.DataFrame(user_item_interactions)\n","print(\"Number of Unique Users:\", df['user_id'].nunique())\n","print(\"Number of Unique Businesses:\", df['business_id'].nunique())\n","\n","df = df.groupby(['user_id', 'business_id']).agg({'stars': 'mean'}).reset_index()\n","print(df.head(10))\n","\n","user_id = df[['user_id']].drop_duplicates().reindex()\n","user_id['userId'] = np.arange(len(user_id))\n","ml1m_rating = pd.merge(df, user_id, on=['user_id'], how='left')\n","\n","item_id = df[['business_id']].drop_duplicates()\n","item_id['itemId'] = np.arange(len(item_id))\n","yelp_rating = pd.merge(ml1m_rating, item_id, on=['business_id'], how='left')\n","yelp_rating = yelp_rating[['userId', 'itemId', 'stars']]\n","print(yelp_rating.head(10))\n","print('Range of userId is [{}, {}]'.format(yelp_rating.userId.min(), yelp_rating.userId.max()))\n","print('Range of itemId is [{}, {}]'.format(yelp_rating.itemId.min(), yelp_rating.itemId.max()))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4fr-BQSgr5Zu"},"source":["# Training Setup\n","\n","We can train NeuralMF without training the GMF and MLP. But, the author suggest that training GMF and MLP first can lead to better performance for large scale data.\n","\n","Edit the config parameter in `train.py` to adjust any hyperparameters"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":162,"status":"ok","timestamp":1701660538543,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"},"user_tz":300},"id":"K2g29lUur4nn","outputId":"1960e2c1-cf8c-4aef-aec4-14f5f0191af4"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Neural-CF/Torch-NCF\n"]}],"source":["%cd ./Torch-NCF"]},{"cell_type":"markdown","metadata":{"id":"5GI4WkrFuhY0"},"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6245,"status":"ok","timestamp":1701660546174,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"},"user_tz":300},"id":"xupJbW6Fr9XI","outputId":"07db069b-7e8f-4421-a57e-d6334b2dc450"},"outputs":[],"source":["# Make directory to save the models\n","import os\n","if not os.path.exists('checkpoints'):\n","    os.makedirs('checkpoints')\n","\n","# # For running in Google Colab\n","# !pip install tensorboardX==1.8.0"]},{"cell_type":"markdown","metadata":{"id":"BE-d40Eb7JdJ"},"source":["# Training GMF (Optional)\n","Before Training, adjust the config for num_users and num_items correctly"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"4LD1gtym8dKG"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading Data....\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/data.py:96: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  ratings['rating'][ratings['rating'] > 0] = 1.0\n","Generating Negative Items...\n","      userId                                   interacted_items\n","0          0  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","1          1  {26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 3...\n","2          2  {69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 8...\n","3          3  {84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 9...\n","4          4  {128, 129, 130, 2, 131, 132, 6, 133, 134, 135,...\n","...      ...                                                ...\n","5908    5908  {22016, 10629, 8073, 10634, 17168, 20245, 2639...\n","5909    5909  {16387, 4733, 3433, 4061, 1727, 16498, 5236, 2...\n","5910    5910  {12806, 5263, 5522, 14355, 5527, 5529, 10143, ...\n","5911    5911  {2946, 3075, 4866, 3973, 3334, 8711, 3975, 179...\n","5912    5912  {12290, 12419, 11907, 1542, 10891, 5773, 2189,...\n","\n","[5913 rows x 2 columns]\n","--------------------------------------------------\n","      userId  ...                                     negative_items\n","0          0  ...  {26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 3...\n","1          1  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","2          2  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","3          3  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","4          4  ...  {0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 1...\n","...      ...  ...                                                ...\n","5908    5908  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","5909    5909  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","5910    5910  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","5911    5911  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","5912    5912  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","\n","[5913 rows x 3 columns]\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/data.py:124: DeprecationWarning: Sampling from a set deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, min(99, len(x)))) # min(99, len(x))\n","--------------------------------------------------\n","      userId  ...                                   negative_samples\n","0          0  ...  [12649, 24862, 13807, 1352, 8510, 16779, 15948...\n","1          1  ...  [12655, 10431, 18905, 7976, 9556, 6068, 6248, ...\n","2          2  ...  [22275, 761, 17848, 13961, 20350, 3340, 8532, ...\n","3          3  ...  [20930, 14728, 12451, 23320, 22122, 18661, 136...\n","4          4  ...  [19931, 21496, 16225, 23364, 21129, 15051, 209...\n","...      ...  ...                                                ...\n","5908    5908  ...  [18499, 23201, 10265, 11938, 5271, 7370, 19911...\n","5909    5909  ...  [14211, 19206, 15809, 17299, 25447, 4754, 488,...\n","5910    5910  ...  [17217, 26324, 17176, 15930, 13053, 18959, 479...\n","5911    5911  ...  [17661, 13161, 12156, 22112, 1512, 22952, 2233...\n","5912    5912  ...  [9769, 17273, 1308, 23218, 10673, 4761, 16943,...\n","\n","[5913 rows x 4 columns]\n","[tensor([5357, 5357, 5357,  ...,  146, 3136, 3797]), tensor([ 5834, 14615, 16502,  ...,  4068,  6292,  7705]), tensor([5357, 5357, 5357,  ..., 3797, 3797, 3797]), tensor([ 5610, 15291,  4500,  ...,  1508, 13695,  8520])]\n","Begin Training....\n","Epoch 0 starts !\n","--------------------------------------------------------------------------------\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/data.py:150: DeprecationWarning: Sampling from a set deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n","[Training Epoch 0] Batch 0, Loss 0.7124742269515991\n","[Training Epoch 0] Batch 1, Loss 0.7174797654151917\n","[Training Epoch 0] Batch 2, Loss 0.7282690405845642\n","[Training Epoch 0] Batch 3, Loss 0.7159295678138733\n","[Training Epoch 0] Batch 4, Loss 0.7283971905708313\n","[Training Epoch 0] Batch 5, Loss 0.713442862033844\n","[Training Epoch 0] Batch 6, Loss 0.7105965614318848\n","[Training Epoch 0] Batch 7, Loss 0.701513409614563\n","[Training Epoch 0] Batch 8, Loss 0.7085901498794556\n","[Training Epoch 0] Batch 9, Loss 0.7078051567077637\n","[Training Epoch 0] Batch 10, Loss 0.7158394455909729\n","[Training Epoch 0] Batch 11, Loss 0.7159619331359863\n","[Training Epoch 0] Batch 12, Loss 0.7145999670028687\n","[Training Epoch 0] Batch 13, Loss 0.7126017212867737\n","[Training Epoch 0] Batch 14, Loss 0.6955493688583374\n","[Training Epoch 0] Batch 15, Loss 0.6935281753540039\n","[Training Epoch 0] Batch 16, Loss 0.694557785987854\n","[Training Epoch 0] Batch 17, Loss 0.6987357139587402\n","[Training Epoch 0] Batch 18, Loss 0.711338222026825\n","[Training Epoch 0] Batch 19, Loss 0.7142612338066101\n","[Training Epoch 0] Batch 20, Loss 0.696890115737915\n","[Training Epoch 0] Batch 21, Loss 0.7068692445755005\n","[Training Epoch 0] Batch 22, Loss 0.7020422220230103\n","[Training Epoch 0] Batch 23, Loss 0.7011255025863647\n","[Training Epoch 0] Batch 24, Loss 0.6953728795051575\n","[Training Epoch 0] Batch 25, Loss 0.7005065679550171\n","[Training Epoch 0] Batch 26, Loss 0.7008922100067139\n","[Training Epoch 0] Batch 27, Loss 0.6975620985031128\n","[Training Epoch 0] Batch 28, Loss 0.7013205885887146\n","[Training Epoch 0] Batch 29, Loss 0.6957312226295471\n","[Training Epoch 0] Batch 30, Loss 0.6999877691268921\n","[Training Epoch 0] Batch 31, Loss 0.7037926912307739\n","[Training Epoch 0] Batch 32, Loss 0.6991415023803711\n","[Training Epoch 0] Batch 33, Loss 0.7041306495666504\n","[Training Epoch 0] Batch 34, Loss 0.6984282732009888\n","[Training Epoch 0] Batch 35, Loss 0.6811918020248413\n","[Training Epoch 0] Batch 36, Loss 0.7068154811859131\n","[Training Epoch 0] Batch 37, Loss 0.6970588564872742\n","[Training Epoch 0] Batch 38, Loss 0.7030296325683594\n","[Training Epoch 0] Batch 39, Loss 0.6982256174087524\n","[Training Epoch 0] Batch 40, Loss 0.6871654987335205\n","[Training Epoch 0] Batch 41, Loss 0.6874792575836182\n","[Training Epoch 0] Batch 42, Loss 0.6965267658233643\n","[Training Epoch 0] Batch 43, Loss 0.6815662384033203\n","[Training Epoch 0] Batch 44, Loss 0.6958240270614624\n","[Training Epoch 0] Batch 45, Loss 0.685794472694397\n","[Training Epoch 0] Batch 46, Loss 0.6872076988220215\n","[Training Epoch 0] Batch 47, Loss 0.6982943415641785\n","[Training Epoch 0] Batch 48, Loss 0.6894913911819458\n","[Training Epoch 0] Batch 49, Loss 0.6799160242080688\n","[Training Epoch 0] Batch 50, Loss 0.6809219121932983\n","[Training Epoch 0] Batch 51, Loss 0.6786483526229858\n","[Training Epoch 0] Batch 52, Loss 0.6935285329818726\n","[Training Epoch 0] Batch 53, Loss 0.6786605715751648\n","[Training Epoch 0] Batch 54, Loss 0.689185619354248\n","[Training Epoch 0] Batch 55, Loss 0.6917116641998291\n","[Training Epoch 0] Batch 56, Loss 0.6856676340103149\n","[Training Epoch 0] Batch 57, Loss 0.6821101903915405\n","[Training Epoch 0] Batch 58, Loss 0.6799595355987549\n","[Training Epoch 0] Batch 59, Loss 0.7008842825889587\n","[Training Epoch 0] Batch 60, Loss 0.6880612373352051\n","[Training Epoch 0] Batch 61, Loss 0.6767758131027222\n","[Training Epoch 0] Batch 62, Loss 0.6806008219718933\n","[Training Epoch 0] Batch 63, Loss 0.6841592788696289\n","[Training Epoch 0] Batch 64, Loss 0.6671972274780273\n","[Training Epoch 0] Batch 65, Loss 0.6876407861709595\n","[Training Epoch 0] Batch 66, Loss 0.6787323355674744\n","[Training Epoch 0] Batch 67, Loss 0.676031231880188\n","[Training Epoch 0] Batch 68, Loss 0.6926854252815247\n","[Training Epoch 0] Batch 69, Loss 0.6814897656440735\n","[Training Epoch 0] Batch 70, Loss 0.6709297895431519\n","[Training Epoch 0] Batch 71, Loss 0.6868153810501099\n","[Training Epoch 0] Batch 72, Loss 0.6828961372375488\n","[Training Epoch 0] Batch 73, Loss 0.6800117492675781\n","[Training Epoch 0] Batch 74, Loss 0.6794217228889465\n","[Training Epoch 0] Batch 75, Loss 0.6791293621063232\n","[Training Epoch 0] Batch 76, Loss 0.6703664660453796\n","[Training Epoch 0] Batch 77, Loss 0.6677423715591431\n","[Training Epoch 0] Batch 78, Loss 0.6757398843765259\n","[Training Epoch 0] Batch 79, Loss 0.6714780330657959\n","[Training Epoch 0] Batch 80, Loss 0.6822917461395264\n","[Training Epoch 0] Batch 81, Loss 0.6783379316329956\n","[Training Epoch 0] Batch 82, Loss 0.6743754148483276\n","[Training Epoch 0] Batch 83, Loss 0.6729757785797119\n","[Training Epoch 0] Batch 84, Loss 0.6735342741012573\n","[Training Epoch 0] Batch 85, Loss 0.6648062467575073\n","[Training Epoch 0] Batch 86, Loss 0.6648359298706055\n","[Training Epoch 0] Batch 87, Loss 0.657371461391449\n","[Training Epoch 0] Batch 88, Loss 0.6722780466079712\n","[Training Epoch 0] Batch 89, Loss 0.6624866724014282\n","[Training Epoch 0] Batch 90, Loss 0.6749207377433777\n","[Training Epoch 0] Batch 91, Loss 0.672431230545044\n","[Training Epoch 0] Batch 92, Loss 0.6724777221679688\n","[Training Epoch 0] Batch 93, Loss 0.6697185635566711\n","[Training Epoch 0] Batch 94, Loss 0.6626468896865845\n","[Training Epoch 0] Batch 95, Loss 0.6604151725769043\n","[Training Epoch 0] Batch 96, Loss 0.6620020866394043\n","[Training Epoch 0] Batch 97, Loss 0.6693800687789917\n","[Training Epoch 0] Batch 98, Loss 0.6694711446762085\n","[Training Epoch 0] Batch 99, Loss 0.6667664051055908\n","[Training Epoch 0] Batch 100, Loss 0.6704503893852234\n","[Training Epoch 0] Batch 101, Loss 0.6676138639450073\n","[Training Epoch 0] Batch 102, Loss 0.6560802459716797\n","[Training Epoch 0] Batch 103, Loss 0.6560025215148926\n","[Training Epoch 0] Batch 104, Loss 0.6575989723205566\n","[Training Epoch 0] Batch 105, Loss 0.6603518128395081\n","[Training Epoch 0] Batch 106, Loss 0.6642313003540039\n","[Training Epoch 0] Batch 107, Loss 0.6586613059043884\n","[Training Epoch 0] Batch 108, Loss 0.6594089269638062\n","[Training Epoch 0] Batch 109, Loss 0.652608335018158\n","[Training Epoch 0] Batch 110, Loss 0.6592572331428528\n","[Training Epoch 0] Batch 111, Loss 0.6661429405212402\n","[Training Epoch 0] Batch 112, Loss 0.6467430591583252\n","[Training Epoch 0] Batch 113, Loss 0.6647047400474548\n","[Training Epoch 0] Batch 114, Loss 0.6493840217590332\n","[Training Epoch 0] Batch 115, Loss 0.6578738689422607\n","[Training Epoch 0] Batch 116, Loss 0.6659439206123352\n","[Training Epoch 0] Batch 117, Loss 0.6496955156326294\n","[Training Epoch 0] Batch 118, Loss 0.6610602140426636\n","[Training Epoch 0] Batch 119, Loss 0.6576840281486511\n","[Training Epoch 0] Batch 120, Loss 0.6653703451156616\n","[Training Epoch 0] Batch 121, Loss 0.6632936000823975\n","[Training Epoch 0] Batch 122, Loss 0.6469277739524841\n","[Training Epoch 0] Batch 123, Loss 0.6582180857658386\n","[Training Epoch 0] Batch 124, Loss 0.6496825218200684\n","[Training Epoch 0] Batch 125, Loss 0.6523876190185547\n","[Training Epoch 0] Batch 126, Loss 0.6655179262161255\n","[Training Epoch 0] Batch 127, Loss 0.6462011337280273\n","[Training Epoch 0] Batch 128, Loss 0.6544874906539917\n","[Training Epoch 0] Batch 129, Loss 0.6580119132995605\n","[Training Epoch 0] Batch 130, Loss 0.6561253070831299\n","[Training Epoch 0] Batch 131, Loss 0.6597808599472046\n","[Training Epoch 0] Batch 132, Loss 0.6538802981376648\n","[Training Epoch 0] Batch 133, Loss 0.6623660326004028\n","[Training Epoch 0] Batch 134, Loss 0.6504340767860413\n","[Training Epoch 0] Batch 135, Loss 0.6501787900924683\n","[Training Epoch 0] Batch 136, Loss 0.65474933385849\n","[Training Epoch 0] Batch 137, Loss 0.656671404838562\n","[Training Epoch 0] Batch 138, Loss 0.654863715171814\n","[Training Epoch 0] Batch 139, Loss 0.6556316614151001\n","[Training Epoch 0] Batch 140, Loss 0.6479883790016174\n","[Training Epoch 0] Batch 141, Loss 0.6531836986541748\n","[Training Epoch 0] Batch 142, Loss 0.6491711139678955\n","[Training Epoch 0] Batch 143, Loss 0.647188663482666\n","[Training Epoch 0] Batch 144, Loss 0.6445310115814209\n","[Training Epoch 0] Batch 145, Loss 0.6370700597763062\n","[Training Epoch 0] Batch 146, Loss 0.6502994894981384\n","[Training Epoch 0] Batch 147, Loss 0.6354004144668579\n","[Training Epoch 0] Batch 148, Loss 0.6477708220481873\n","[Training Epoch 0] Batch 149, Loss 0.6540497541427612\n","[Training Epoch 0] Batch 150, Loss 0.649705708026886\n","[Training Epoch 0] Batch 151, Loss 0.654539942741394\n","[Training Epoch 0] Batch 152, Loss 0.6472417116165161\n","[Training Epoch 0] Batch 153, Loss 0.644894003868103\n","[Training Epoch 0] Batch 154, Loss 0.6381902098655701\n","[Training Epoch 0] Batch 155, Loss 0.6468178033828735\n","[Training Epoch 0] Batch 156, Loss 0.651381254196167\n","[Training Epoch 0] Batch 157, Loss 0.6517289876937866\n","[Training Epoch 0] Batch 158, Loss 0.6501108407974243\n","[Training Epoch 0] Batch 159, Loss 0.6466192007064819\n","[Training Epoch 0] Batch 160, Loss 0.6440128087997437\n","[Training Epoch 0] Batch 161, Loss 0.6380643248558044\n","[Training Epoch 0] Batch 162, Loss 0.6369422674179077\n","[Training Epoch 0] Batch 163, Loss 0.6370365619659424\n","[Training Epoch 0] Batch 164, Loss 0.6428179740905762\n","[Training Epoch 0] Batch 165, Loss 0.6447341442108154\n","[Training Epoch 0] Batch 166, Loss 0.6433507204055786\n","[Training Epoch 0] Batch 167, Loss 0.6370217800140381\n","[Training Epoch 0] Batch 168, Loss 0.6315537691116333\n","[Training Epoch 0] Batch 169, Loss 0.6340793371200562\n","[Training Epoch 0] Batch 170, Loss 0.6359333992004395\n","[Training Epoch 0] Batch 171, Loss 0.6432392597198486\n","[Training Epoch 0] Batch 172, Loss 0.6428108811378479\n","[Training Epoch 0] Batch 173, Loss 0.6360881328582764\n","[Training Epoch 0] Batch 174, Loss 0.6409178972244263\n","[Training Epoch 0] Batch 175, Loss 0.64108806848526\n","[Training Epoch 0] Batch 176, Loss 0.6393700838088989\n","[Training Epoch 0] Batch 177, Loss 0.6387565732002258\n","[Training Epoch 0] Batch 178, Loss 0.6376988291740417\n","[Training Epoch 0] Batch 179, Loss 0.637302815914154\n","[Training Epoch 0] Batch 180, Loss 0.6404551267623901\n","[Training Epoch 0] Batch 181, Loss 0.6345053315162659\n","[Training Epoch 0] Batch 182, Loss 0.6336666941642761\n","[Training Epoch 0] Batch 183, Loss 0.6340185403823853\n","[Training Epoch 0] Batch 184, Loss 0.6405712366104126\n","[Training Epoch 0] Batch 185, Loss 0.6282894015312195\n","[Training Epoch 0] Batch 186, Loss 0.6300466060638428\n","[Training Epoch 0] Batch 187, Loss 0.6341326236724854\n","[Training Epoch 0] Batch 188, Loss 0.6362580060958862\n","[Training Epoch 0] Batch 189, Loss 0.6431292295455933\n","[Training Epoch 0] Batch 190, Loss 0.6309754252433777\n","[Training Epoch 0] Batch 191, Loss 0.6377531290054321\n","[Training Epoch 0] Batch 192, Loss 0.6343752145767212\n","[Training Epoch 0] Batch 193, Loss 0.6351599097251892\n","[Training Epoch 0] Batch 194, Loss 0.6358382701873779\n","[Training Epoch 0] Batch 195, Loss 0.6366006731987\n","[Training Epoch 0] Batch 196, Loss 0.630712628364563\n","[Training Epoch 0] Batch 197, Loss 0.6306227445602417\n","[Training Epoch 0] Batch 198, Loss 0.6325753927230835\n","[Training Epoch 0] Batch 199, Loss 0.6272745132446289\n","[Training Epoch 0] Batch 200, Loss 0.633271336555481\n","[Training Epoch 0] Batch 201, Loss 0.6222699880599976\n","[Training Epoch 0] Batch 202, Loss 0.6219450235366821\n","[Training Epoch 0] Batch 203, Loss 0.6418157815933228\n","[Training Epoch 0] Batch 204, Loss 0.6248642206192017\n","[Training Epoch 0] Batch 205, Loss 0.629530668258667\n","[Training Epoch 0] Batch 206, Loss 0.6269674301147461\n","[Training Epoch 0] Batch 207, Loss 0.626332700252533\n","[Training Epoch 0] Batch 208, Loss 0.6326473951339722\n","[Training Epoch 0] Batch 209, Loss 0.6257100105285645\n","[Training Epoch 0] Batch 210, Loss 0.617945671081543\n","[Training Epoch 0] Batch 211, Loss 0.62286776304245\n","[Training Epoch 0] Batch 212, Loss 0.6324396133422852\n","[Training Epoch 0] Batch 213, Loss 0.625110387802124\n","[Training Epoch 0] Batch 214, Loss 0.621943473815918\n","[Training Epoch 0] Batch 215, Loss 0.6246597170829773\n","[Training Epoch 0] Batch 216, Loss 0.6242582201957703\n","[Training Epoch 0] Batch 217, Loss 0.62507164478302\n","[Training Epoch 0] Batch 218, Loss 0.6292115449905396\n","[Training Epoch 0] Batch 219, Loss 0.6243865489959717\n","[Training Epoch 0] Batch 220, Loss 0.6215451955795288\n","[Training Epoch 0] Batch 221, Loss 0.6217206716537476\n","[Training Epoch 0] Batch 222, Loss 0.6182166337966919\n","[Training Epoch 0] Batch 223, Loss 0.6245981454849243\n","[Training Epoch 0] Batch 224, Loss 0.6198451519012451\n","[Training Epoch 0] Batch 225, Loss 0.6297427415847778\n","[Training Epoch 0] Batch 226, Loss 0.6277150511741638\n","[Training Epoch 0] Batch 227, Loss 0.620675265789032\n","[Training Epoch 0] Batch 228, Loss 0.6269243955612183\n","[Training Epoch 0] Batch 229, Loss 0.6234726905822754\n","[Training Epoch 0] Batch 230, Loss 0.6229387521743774\n","[Training Epoch 0] Batch 231, Loss 0.628329873085022\n","[Training Epoch 0] Batch 232, Loss 0.6216428279876709\n","[Training Epoch 0] Batch 233, Loss 0.6217218637466431\n","[Training Epoch 0] Batch 234, Loss 0.6252076625823975\n","[Training Epoch 0] Batch 235, Loss 0.6232715845108032\n","[Training Epoch 0] Batch 236, Loss 0.6156937479972839\n","[Training Epoch 0] Batch 237, Loss 0.6277972459793091\n","[Training Epoch 0] Batch 238, Loss 0.6235804557800293\n","[Training Epoch 0] Batch 239, Loss 0.6235959529876709\n","[Training Epoch 0] Batch 240, Loss 0.6225757598876953\n","[Training Epoch 0] Batch 241, Loss 0.6169872283935547\n","[Training Epoch 0] Batch 242, Loss 0.6263089179992676\n","[Training Epoch 0] Batch 243, Loss 0.6204407215118408\n","[Training Epoch 0] Batch 244, Loss 0.6226674914360046\n","[Training Epoch 0] Batch 245, Loss 0.6169928312301636\n","[Training Epoch 0] Batch 246, Loss 0.6197385787963867\n","[Training Epoch 0] Batch 247, Loss 0.6177181005477905\n","[Training Epoch 0] Batch 248, Loss 0.6158455610275269\n","[Training Epoch 0] Batch 249, Loss 0.6272760629653931\n","[Training Epoch 0] Batch 250, Loss 0.6141185164451599\n","[Training Epoch 0] Batch 251, Loss 0.6116987466812134\n","[Training Epoch 0] Batch 252, Loss 0.6176602244377136\n","[Training Epoch 0] Batch 253, Loss 0.6129668951034546\n","[Training Epoch 0] Batch 254, Loss 0.6224163770675659\n","[Training Epoch 0] Batch 255, Loss 0.6092286109924316\n","[Training Epoch 0] Batch 256, Loss 0.616519033908844\n","[Training Epoch 0] Batch 257, Loss 0.617333710193634\n","[Training Epoch 0] Batch 258, Loss 0.610065221786499\n","[Training Epoch 0] Batch 259, Loss 0.6162794232368469\n","[Training Epoch 0] Batch 260, Loss 0.6251899600028992\n","[Training Epoch 0] Batch 261, Loss 0.6117833852767944\n","[Training Epoch 0] Batch 262, Loss 0.6108953356742859\n","[Training Epoch 0] Batch 263, Loss 0.6206135749816895\n","[Training Epoch 0] Batch 264, Loss 0.6171131134033203\n","[Training Epoch 0] Batch 265, Loss 0.6164951920509338\n","[Training Epoch 0] Batch 266, Loss 0.6067683696746826\n","[Training Epoch 0] Batch 267, Loss 0.6114007830619812\n","[Training Epoch 0] Batch 268, Loss 0.6117308139801025\n","[Training Epoch 0] Batch 269, Loss 0.6077355146408081\n","[Training Epoch 0] Batch 270, Loss 0.6070016622543335\n","[Training Epoch 0] Batch 271, Loss 0.6089848875999451\n","[Training Epoch 0] Batch 272, Loss 0.6154866218566895\n","[Training Epoch 0] Batch 273, Loss 0.613730251789093\n","[Training Epoch 0] Batch 274, Loss 0.6170305013656616\n","[Training Epoch 0] Batch 275, Loss 0.6089367866516113\n","[Training Epoch 0] Batch 276, Loss 0.6050786972045898\n","[Training Epoch 0] Batch 277, Loss 0.612481415271759\n","[Training Epoch 0] Batch 278, Loss 0.6075279116630554\n","[Training Epoch 0] Batch 279, Loss 0.6081856489181519\n","[Training Epoch 0] Batch 280, Loss 0.6072137951850891\n","[Training Epoch 0] Batch 281, Loss 0.6153537034988403\n","[Training Epoch 0] Batch 282, Loss 0.5982011556625366\n","[Training Epoch 0] Batch 283, Loss 0.614636242389679\n","[Training Epoch 0] Batch 284, Loss 0.6082252264022827\n","[Training Epoch 0] Batch 285, Loss 0.6021392345428467\n","[Training Epoch 0] Batch 286, Loss 0.6181797981262207\n","[Training Epoch 0] Batch 287, Loss 0.6087320446968079\n","[Training Epoch 0] Batch 288, Loss 0.6049618721008301\n","[Training Epoch 0] Batch 289, Loss 0.607361912727356\n","[Training Epoch 0] Batch 290, Loss 0.6083612442016602\n","[Training Epoch 0] Batch 291, Loss 0.6097204685211182\n","[Training Epoch 0] Batch 292, Loss 0.6096644401550293\n","[Training Epoch 0] Batch 293, Loss 0.6101018786430359\n","[Training Epoch 0] Batch 294, Loss 0.6139742136001587\n","[Training Epoch 0] Batch 295, Loss 0.611790657043457\n","[Training Epoch 0] Batch 296, Loss 0.614984393119812\n","[Training Epoch 0] Batch 297, Loss 0.6071039438247681\n","[Training Epoch 0] Batch 298, Loss 0.6123942136764526\n","[Training Epoch 0] Batch 299, Loss 0.612865686416626\n","[Training Epoch 0] Batch 300, Loss 0.5995582938194275\n","[Training Epoch 0] Batch 301, Loss 0.6024850606918335\n","[Training Epoch 0] Batch 302, Loss 0.6088565587997437\n","[Training Epoch 0] Batch 303, Loss 0.6089601516723633\n","[Training Epoch 0] Batch 304, Loss 0.6018260717391968\n","[Training Epoch 0] Batch 305, Loss 0.6001690030097961\n","[Training Epoch 0] Batch 306, Loss 0.5994085073471069\n","[Training Epoch 0] Batch 307, Loss 0.6078082323074341\n","[Training Epoch 0] Batch 308, Loss 0.5995451211929321\n","[Training Epoch 0] Batch 309, Loss 0.5988568067550659\n","[Training Epoch 0] Batch 310, Loss 0.59864342212677\n","[Training Epoch 0] Batch 311, Loss 0.6042897701263428\n","[Training Epoch 0] Batch 312, Loss 0.602866530418396\n","[Training Epoch 0] Batch 313, Loss 0.6013399362564087\n","[Training Epoch 0] Batch 314, Loss 0.59670090675354\n","[Training Epoch 0] Batch 315, Loss 0.5996931791305542\n","[Training Epoch 0] Batch 316, Loss 0.5984331369400024\n","[Training Epoch 0] Batch 317, Loss 0.5937541723251343\n","[Training Epoch 0] Batch 318, Loss 0.6084564328193665\n","[Training Epoch 0] Batch 319, Loss 0.6067090034484863\n","[Training Epoch 0] Batch 320, Loss 0.6029189825057983\n","[Training Epoch 0] Batch 321, Loss 0.6018213033676147\n","[Training Epoch 0] Batch 322, Loss 0.6092110276222229\n","[Training Epoch 0] Batch 323, Loss 0.5968495011329651\n","[Training Epoch 0] Batch 324, Loss 0.6082650423049927\n","[Training Epoch 0] Batch 325, Loss 0.6113518476486206\n","[Training Epoch 0] Batch 326, Loss 0.6038926839828491\n","[Training Epoch 0] Batch 327, Loss 0.5929704904556274\n","[Training Epoch 0] Batch 328, Loss 0.6012247800827026\n","[Training Epoch 0] Batch 329, Loss 0.5986799001693726\n","[Training Epoch 0] Batch 330, Loss 0.5900867581367493\n","[Training Epoch 0] Batch 331, Loss 0.5967137813568115\n","[Training Epoch 0] Batch 332, Loss 0.5990234017372131\n","[Training Epoch 0] Batch 333, Loss 0.5901448726654053\n","[Training Epoch 0] Batch 334, Loss 0.6031818389892578\n","[Training Epoch 0] Batch 335, Loss 0.5956466197967529\n","[Training Epoch 0] Batch 336, Loss 0.5980567932128906\n","[Training Epoch 0] Batch 337, Loss 0.5985921621322632\n","[Training Epoch 0] Batch 338, Loss 0.6059460639953613\n","[Training Epoch 0] Batch 339, Loss 0.5950624346733093\n","[Training Epoch 0] Batch 340, Loss 0.6000220775604248\n","[Training Epoch 0] Batch 341, Loss 0.598618745803833\n","[Training Epoch 0] Batch 342, Loss 0.597745418548584\n","[Training Epoch 0] Batch 343, Loss 0.5987834930419922\n","[Training Epoch 0] Batch 344, Loss 0.5957362651824951\n","[Training Epoch 0] Batch 345, Loss 0.6004021167755127\n","[Training Epoch 0] Batch 346, Loss 0.5911619663238525\n","[Training Epoch 0] Batch 347, Loss 0.5873755216598511\n","[Training Epoch 0] Batch 348, Loss 0.5968666076660156\n","[Training Epoch 0] Batch 349, Loss 0.5948730111122131\n","[Training Epoch 0] Batch 350, Loss 0.5971029996871948\n","[Training Epoch 0] Batch 351, Loss 0.5966629385948181\n","[Training Epoch 0] Batch 352, Loss 0.5950451493263245\n","[Training Epoch 0] Batch 353, Loss 0.5877070426940918\n","[Training Epoch 0] Batch 354, Loss 0.5923517942428589\n","[Training Epoch 0] Batch 355, Loss 0.5902326107025146\n","[Training Epoch 0] Batch 356, Loss 0.5923059582710266\n","[Training Epoch 0] Batch 357, Loss 0.5935941338539124\n","[Training Epoch 0] Batch 358, Loss 0.589982271194458\n","[Training Epoch 0] Batch 359, Loss 0.5955929756164551\n","[Training Epoch 0] Batch 360, Loss 0.6050059795379639\n","[Training Epoch 0] Batch 361, Loss 0.5968407392501831\n","[Training Epoch 0] Batch 362, Loss 0.5979686975479126\n","[Training Epoch 0] Batch 363, Loss 0.5896445512771606\n","[Training Epoch 0] Batch 364, Loss 0.5919665098190308\n","[Training Epoch 0] Batch 365, Loss 0.6036589741706848\n","[Training Epoch 0] Batch 366, Loss 0.5876076221466064\n","[Training Epoch 0] Batch 367, Loss 0.6013890504837036\n","[Training Epoch 0] Batch 368, Loss 0.5958876013755798\n","[Training Epoch 0] Batch 369, Loss 0.5848047733306885\n","[Training Epoch 0] Batch 370, Loss 0.5846832990646362\n","[Training Epoch 0] Batch 371, Loss 0.5931290984153748\n","[Training Epoch 0] Batch 372, Loss 0.595843493938446\n","[Training Epoch 0] Batch 373, Loss 0.5880769491195679\n","[Training Epoch 0] Batch 374, Loss 0.5835331082344055\n","[Training Epoch 0] Batch 375, Loss 0.6041325330734253\n","[Training Epoch 0] Batch 376, Loss 0.5925393104553223\n","[Training Epoch 0] Batch 377, Loss 0.5893168449401855\n","[Training Epoch 0] Batch 378, Loss 0.5839390754699707\n","[Training Epoch 0] Batch 379, Loss 0.5883119106292725\n","[Training Epoch 0] Batch 380, Loss 0.5805884003639221\n","[Training Epoch 0] Batch 381, Loss 0.5934303402900696\n","[Training Epoch 0] Batch 382, Loss 0.5963971614837646\n","[Training Epoch 0] Batch 383, Loss 0.596924364566803\n","[Training Epoch 0] Batch 384, Loss 0.5944671630859375\n","[Training Epoch 0] Batch 385, Loss 0.5920292735099792\n","[Training Epoch 0] Batch 386, Loss 0.5914990901947021\n","[Training Epoch 0] Batch 387, Loss 0.5912213325500488\n","[Training Epoch 0] Batch 388, Loss 0.5916014909744263\n","[Training Epoch 0] Batch 389, Loss 0.5864804983139038\n","[Training Epoch 0] Batch 390, Loss 0.5993569493293762\n","[Training Epoch 0] Batch 391, Loss 0.5802384614944458\n","[Training Epoch 0] Batch 392, Loss 0.5878725051879883\n","[Training Epoch 0] Batch 393, Loss 0.5837544798851013\n","[Training Epoch 0] Batch 394, Loss 0.5848410129547119\n","[Training Epoch 0] Batch 395, Loss 0.5815656185150146\n","[Training Epoch 0] Batch 396, Loss 0.589425802230835\n","[Training Epoch 0] Batch 397, Loss 0.5852301120758057\n","[Training Epoch 0] Batch 398, Loss 0.5841610431671143\n","[Training Epoch 0] Batch 399, Loss 0.5868708491325378\n","[Training Epoch 0] Batch 400, Loss 0.5872204303741455\n","[Training Epoch 0] Batch 401, Loss 0.5881291031837463\n","[Training Epoch 0] Batch 402, Loss 0.5762637257575989\n","[Training Epoch 0] Batch 403, Loss 0.5906347036361694\n","[Training Epoch 0] Batch 404, Loss 0.5864827036857605\n","[Training Epoch 0] Batch 405, Loss 0.5799870491027832\n","[Training Epoch 0] Batch 406, Loss 0.5825411677360535\n","[Training Epoch 0] Batch 407, Loss 0.5866103768348694\n","[Training Epoch 0] Batch 408, Loss 0.5792735815048218\n","[Training Epoch 0] Batch 409, Loss 0.5920612812042236\n","[Training Epoch 0] Batch 410, Loss 0.5830423831939697\n","[Training Epoch 0] Batch 411, Loss 0.588769793510437\n","[Training Epoch 0] Batch 412, Loss 0.5876867175102234\n","[Training Epoch 0] Batch 413, Loss 0.5778313279151917\n","[Training Epoch 0] Batch 414, Loss 0.5953251123428345\n","[Training Epoch 0] Batch 415, Loss 0.5884445905685425\n","[Training Epoch 0] Batch 416, Loss 0.5821916460990906\n","[Training Epoch 0] Batch 417, Loss 0.5880063772201538\n","[Training Epoch 0] Batch 418, Loss 0.5860758423805237\n","[Training Epoch 0] Batch 419, Loss 0.5887719392776489\n","[Training Epoch 0] Batch 420, Loss 0.5856650471687317\n","[Training Epoch 0] Batch 421, Loss 0.5786371231079102\n","[Training Epoch 0] Batch 422, Loss 0.5825361609458923\n","[Training Epoch 0] Batch 423, Loss 0.5781083106994629\n","[Training Epoch 0] Batch 424, Loss 0.5818489193916321\n","[Training Epoch 0] Batch 425, Loss 0.5908316373825073\n","[Training Epoch 0] Batch 426, Loss 0.5849907398223877\n","[Training Epoch 0] Batch 427, Loss 0.5810842514038086\n","[Training Epoch 0] Batch 428, Loss 0.5790782570838928\n","[Training Epoch 0] Batch 429, Loss 0.5839678049087524\n","[Training Epoch 0] Batch 430, Loss 0.5789531469345093\n","[Training Epoch 0] Batch 431, Loss 0.5890729427337646\n","[Training Epoch 0] Batch 432, Loss 0.5890549421310425\n","[Training Epoch 0] Batch 433, Loss 0.5777034163475037\n","[Training Epoch 0] Batch 434, Loss 0.5791327953338623\n","[Training Epoch 0] Batch 435, Loss 0.5911017656326294\n","[Training Epoch 0] Batch 436, Loss 0.5746493935585022\n","[Training Epoch 0] Batch 437, Loss 0.576103687286377\n","[Training Epoch 0] Batch 438, Loss 0.5794746279716492\n","[Training Epoch 0] Batch 439, Loss 0.5774843692779541\n","[Training Epoch 0] Batch 440, Loss 0.5853965282440186\n","[Training Epoch 0] Batch 441, Loss 0.5877746343612671\n","[Training Epoch 0] Batch 442, Loss 0.5799767971038818\n","[Training Epoch 0] Batch 443, Loss 0.5820944309234619\n","[Training Epoch 0] Batch 444, Loss 0.5847129225730896\n","[Training Epoch 0] Batch 445, Loss 0.583945631980896\n","[Training Epoch 0] Batch 446, Loss 0.5769233703613281\n","[Training Epoch 0] Batch 447, Loss 0.5784658193588257\n","[Training Epoch 0] Batch 448, Loss 0.591317892074585\n","[Training Epoch 0] Batch 449, Loss 0.5851792097091675\n","[Training Epoch 0] Batch 450, Loss 0.5887228846549988\n","[Training Epoch 0] Batch 451, Loss 0.5873519778251648\n","[Training Epoch 0] Batch 452, Loss 0.5784459114074707\n","[Training Epoch 0] Batch 453, Loss 0.5737361907958984\n","[Training Epoch 0] Batch 454, Loss 0.584099292755127\n","[Training Epoch 0] Batch 455, Loss 0.5780601501464844\n","[Training Epoch 0] Batch 456, Loss 0.5846788883209229\n","[Training Epoch 0] Batch 457, Loss 0.5864918231964111\n","[Training Epoch 0] Batch 458, Loss 0.5773455500602722\n","[Training Epoch 0] Batch 459, Loss 0.5761175751686096\n","[Training Epoch 0] Batch 460, Loss 0.581134021282196\n","[Training Epoch 0] Batch 461, Loss 0.5842219591140747\n","[Training Epoch 0] Batch 462, Loss 0.5692240595817566\n","[Training Epoch 0] Batch 463, Loss 0.5755264759063721\n","[Training Epoch 0] Batch 464, Loss 0.5740520358085632\n","[Training Epoch 0] Batch 465, Loss 0.5669837594032288\n","[Training Epoch 0] Batch 466, Loss 0.568429708480835\n","[Training Epoch 0] Batch 467, Loss 0.5751464366912842\n","[Training Epoch 0] Batch 468, Loss 0.5776318311691284\n","[Training Epoch 0] Batch 469, Loss 0.5733331441879272\n","[Training Epoch 0] Batch 470, Loss 0.5842033624649048\n","[Training Epoch 0] Batch 471, Loss 0.5797628164291382\n","[Training Epoch 0] Batch 472, Loss 0.5619365572929382\n","[Training Epoch 0] Batch 473, Loss 0.5671513080596924\n","[Training Epoch 0] Batch 474, Loss 0.5713916420936584\n","[Training Epoch 0] Batch 475, Loss 0.583594560623169\n","[Training Epoch 0] Batch 476, Loss 0.5748840570449829\n","[Training Epoch 0] Batch 477, Loss 0.5701825618743896\n","[Training Epoch 0] Batch 478, Loss 0.5754733085632324\n","[Training Epoch 0] Batch 479, Loss 0.5814841985702515\n","[Training Epoch 0] Batch 480, Loss 0.5833035707473755\n","[Training Epoch 0] Batch 481, Loss 0.5684688091278076\n","[Training Epoch 0] Batch 482, Loss 0.574809193611145\n","[Training Epoch 0] Batch 483, Loss 0.5713858604431152\n","[Training Epoch 0] Batch 484, Loss 0.5778061747550964\n","[Training Epoch 0] Batch 485, Loss 0.5714647769927979\n","[Training Epoch 0] Batch 486, Loss 0.5690367221832275\n","[Training Epoch 0] Batch 487, Loss 0.5761300921440125\n","[Training Epoch 0] Batch 488, Loss 0.5673627257347107\n","[Training Epoch 0] Batch 489, Loss 0.575564980506897\n","[Training Epoch 0] Batch 490, Loss 0.5800637006759644\n","[Training Epoch 0] Batch 491, Loss 0.5703547596931458\n","[Training Epoch 0] Batch 492, Loss 0.5676097869873047\n","[Training Epoch 0] Batch 493, Loss 0.5705729126930237\n","[Training Epoch 0] Batch 494, Loss 0.5735999941825867\n","[Training Epoch 0] Batch 495, Loss 0.5690733194351196\n","[Training Epoch 0] Batch 496, Loss 0.5606052279472351\n","[Training Epoch 0] Batch 497, Loss 0.5796650052070618\n","[Training Epoch 0] Batch 498, Loss 0.5643618106842041\n","[Training Epoch 0] Batch 499, Loss 0.5637115240097046\n","[Training Epoch 0] Batch 500, Loss 0.570637583732605\n","[Training Epoch 0] Batch 501, Loss 0.570220947265625\n","[Training Epoch 0] Batch 502, Loss 0.5832043290138245\n","[Training Epoch 0] Batch 503, Loss 0.5703740119934082\n","[Training Epoch 0] Batch 504, Loss 0.5643942356109619\n","[Training Epoch 0] Batch 505, Loss 0.5628724098205566\n","[Training Epoch 0] Batch 506, Loss 0.5815633535385132\n","[Training Epoch 0] Batch 507, Loss 0.5591698288917542\n","[Training Epoch 0] Batch 508, Loss 0.5611981153488159\n","[Training Epoch 0] Batch 509, Loss 0.5693233013153076\n","[Training Epoch 0] Batch 510, Loss 0.5708368420600891\n","[Training Epoch 0] Batch 511, Loss 0.5669505000114441\n","[Training Epoch 0] Batch 512, Loss 0.5583515167236328\n","[Training Epoch 0] Batch 513, Loss 0.5733107328414917\n","[Training Epoch 0] Batch 514, Loss 0.5642484426498413\n","[Training Epoch 0] Batch 515, Loss 0.5825284719467163\n","[Training Epoch 0] Batch 516, Loss 0.5697874426841736\n","[Training Epoch 0] Batch 517, Loss 0.5773363709449768\n","[Training Epoch 0] Batch 518, Loss 0.5720553994178772\n","[Training Epoch 0] Batch 519, Loss 0.5691252946853638\n","[Training Epoch 0] Batch 520, Loss 0.5639257431030273\n","[Training Epoch 0] Batch 521, Loss 0.5677827596664429\n","[Training Epoch 0] Batch 522, Loss 0.5636664628982544\n","[Training Epoch 0] Batch 523, Loss 0.5633012056350708\n","[Training Epoch 0] Batch 524, Loss 0.5668524503707886\n","[Training Epoch 0] Batch 525, Loss 0.5669012665748596\n","[Training Epoch 0] Batch 526, Loss 0.5848511457443237\n","[Training Epoch 0] Batch 527, Loss 0.566438615322113\n","[Training Epoch 0] Batch 528, Loss 0.5786550045013428\n","[Training Epoch 0] Batch 529, Loss 0.5611085891723633\n","[Training Epoch 0] Batch 530, Loss 0.5701371431350708\n","[Training Epoch 0] Batch 531, Loss 0.5674925446510315\n","[Training Epoch 0] Batch 532, Loss 0.5604439973831177\n","[Training Epoch 0] Batch 533, Loss 0.5569227337837219\n","[Training Epoch 0] Batch 534, Loss 0.5709441900253296\n","[Training Epoch 0] Batch 535, Loss 0.5732277631759644\n","[Training Epoch 0] Batch 536, Loss 0.5613044500350952\n","[Training Epoch 0] Batch 537, Loss 0.5572605133056641\n","[Training Epoch 0] Batch 538, Loss 0.5650357604026794\n","[Training Epoch 0] Batch 539, Loss 0.5659230947494507\n","[Training Epoch 0] Batch 540, Loss 0.5723087787628174\n","[Training Epoch 0] Batch 541, Loss 0.5802018046379089\n","[Training Epoch 0] Batch 542, Loss 0.5583329200744629\n","[Training Epoch 0] Batch 543, Loss 0.5549423694610596\n","[Training Epoch 0] Batch 544, Loss 0.5748231410980225\n","[Training Epoch 0] Batch 545, Loss 0.5675642490386963\n","[Training Epoch 0] Batch 546, Loss 0.5659502744674683\n","[Training Epoch 0] Batch 547, Loss 0.5722640752792358\n","[Training Epoch 0] Batch 548, Loss 0.5627689361572266\n","[Training Epoch 0] Batch 549, Loss 0.5597041845321655\n","[Training Epoch 0] Batch 550, Loss 0.5793212652206421\n","[Training Epoch 0] Batch 551, Loss 0.5639458894729614\n","[Training Epoch 0] Batch 552, Loss 0.5821248292922974\n","[Training Epoch 0] Batch 553, Loss 0.5680704116821289\n","[Training Epoch 0] Batch 554, Loss 0.5637333989143372\n","[Training Epoch 0] Batch 555, Loss 0.5660847425460815\n","[Training Epoch 0] Batch 556, Loss 0.5602391958236694\n","[Training Epoch 0] Batch 557, Loss 0.5650068521499634\n","[Training Epoch 0] Batch 558, Loss 0.5808785557746887\n","[Training Epoch 0] Batch 559, Loss 0.5603483319282532\n","[Training Epoch 0] Batch 560, Loss 0.5673305988311768\n","[Training Epoch 0] Batch 561, Loss 0.5696315169334412\n","[Training Epoch 0] Batch 562, Loss 0.5533754825592041\n","[Training Epoch 0] Batch 563, Loss 0.565415620803833\n","[Training Epoch 0] Batch 564, Loss 0.5629774332046509\n","[Training Epoch 0] Batch 565, Loss 0.5583397150039673\n","[Training Epoch 0] Batch 566, Loss 0.5503612756729126\n","[Training Epoch 0] Batch 567, Loss 0.5632473230361938\n","[Training Epoch 0] Batch 568, Loss 0.5526816248893738\n","[Training Epoch 0] Batch 569, Loss 0.5589027404785156\n","[Training Epoch 0] Batch 570, Loss 0.5593152046203613\n","[Training Epoch 0] Batch 571, Loss 0.5694482326507568\n","[Training Epoch 0] Batch 572, Loss 0.5683242082595825\n","[Training Epoch 0] Batch 573, Loss 0.5693318247795105\n","[Training Epoch 0] Batch 574, Loss 0.5606967806816101\n","[Training Epoch 0] Batch 575, Loss 0.5683521032333374\n","[Training Epoch 0] Batch 576, Loss 0.5490779876708984\n","[Training Epoch 0] Batch 577, Loss 0.5569766759872437\n","[Training Epoch 0] Batch 578, Loss 0.5667521953582764\n","[Training Epoch 0] Batch 579, Loss 0.5683670043945312\n","[Training Epoch 0] Batch 580, Loss 0.5559113025665283\n","[Training Epoch 0] Batch 581, Loss 0.5591902732849121\n","[Training Epoch 0] Batch 582, Loss 0.5743586421012878\n","[Training Epoch 0] Batch 583, Loss 0.5620392560958862\n","[Training Epoch 0] Batch 584, Loss 0.5715912580490112\n","[Training Epoch 0] Batch 585, Loss 0.5600817799568176\n","[Training Epoch 0] Batch 586, Loss 0.5702204704284668\n","[Training Epoch 0] Batch 587, Loss 0.5659139156341553\n","[Training Epoch 0] Batch 588, Loss 0.5598547458648682\n","[Training Epoch 0] Batch 589, Loss 0.5565072298049927\n","[Training Epoch 0] Batch 590, Loss 0.5683781504631042\n","[Training Epoch 0] Batch 591, Loss 0.5725590586662292\n","[Training Epoch 0] Batch 592, Loss 0.5580754280090332\n","[Training Epoch 0] Batch 593, Loss 0.5669950842857361\n","[Training Epoch 0] Batch 594, Loss 0.5520129799842834\n","[Training Epoch 0] Batch 595, Loss 0.5462082028388977\n","[Training Epoch 0] Batch 596, Loss 0.5581068992614746\n","[Training Epoch 0] Batch 597, Loss 0.5624802708625793\n","[Training Epoch 0] Batch 598, Loss 0.5595589280128479\n","[Training Epoch 0] Batch 599, Loss 0.5706215500831604\n","[Training Epoch 0] Batch 600, Loss 0.5514553785324097\n","[Training Epoch 0] Batch 601, Loss 0.557445764541626\n","[Training Epoch 0] Batch 602, Loss 0.5653643012046814\n","[Training Epoch 0] Batch 603, Loss 0.5576288104057312\n","[Training Epoch 0] Batch 604, Loss 0.5475913286209106\n","[Training Epoch 0] Batch 605, Loss 0.5576717853546143\n","[Training Epoch 0] Batch 606, Loss 0.5638291835784912\n","[Training Epoch 0] Batch 607, Loss 0.5671783685684204\n","[Training Epoch 0] Batch 608, Loss 0.5597776174545288\n","[Training Epoch 0] Batch 609, Loss 0.5739468336105347\n","[Training Epoch 0] Batch 610, Loss 0.5441453456878662\n","[Training Epoch 0] Batch 611, Loss 0.553994357585907\n","[Training Epoch 0] Batch 612, Loss 0.5631641149520874\n","[Training Epoch 0] Batch 613, Loss 0.5458638072013855\n","[Training Epoch 0] Batch 614, Loss 0.5527087450027466\n","[Training Epoch 0] Batch 615, Loss 0.5611411333084106\n","[Training Epoch 0] Batch 616, Loss 0.5491518974304199\n","[Training Epoch 0] Batch 617, Loss 0.56285560131073\n","[Training Epoch 0] Batch 618, Loss 0.5482427477836609\n","[Training Epoch 0] Batch 619, Loss 0.5492672920227051\n","[Training Epoch 0] Batch 620, Loss 0.5576128363609314\n","[Training Epoch 0] Batch 621, Loss 0.5609266757965088\n","[Training Epoch 0] Batch 622, Loss 0.5508195161819458\n","[Training Epoch 0] Batch 623, Loss 0.5701885223388672\n","[Training Epoch 0] Batch 624, Loss 0.5589091181755066\n","[Training Epoch 0] Batch 625, Loss 0.5538448095321655\n","[Training Epoch 0] Batch 626, Loss 0.5575257539749146\n","[Training Epoch 0] Batch 627, Loss 0.5610610246658325\n","[Training Epoch 0] Batch 628, Loss 0.5569926500320435\n","[Training Epoch 0] Batch 629, Loss 0.567308247089386\n","[Training Epoch 0] Batch 630, Loss 0.5533785820007324\n","[Training Epoch 0] Batch 631, Loss 0.5556574463844299\n","[Training Epoch 0] Batch 632, Loss 0.5573830604553223\n","[Training Epoch 0] Batch 633, Loss 0.5548221468925476\n","[Training Epoch 0] Batch 634, Loss 0.5583544373512268\n","[Training Epoch 0] Batch 635, Loss 0.5601155161857605\n","[Training Epoch 0] Batch 636, Loss 0.549241304397583\n","[Training Epoch 0] Batch 637, Loss 0.5586910843849182\n","[Training Epoch 0] Batch 638, Loss 0.5645686388015747\n","[Training Epoch 0] Batch 639, Loss 0.5511475205421448\n","[Training Epoch 0] Batch 640, Loss 0.5572646856307983\n","[Training Epoch 0] Batch 641, Loss 0.5595492124557495\n","[Training Epoch 0] Batch 642, Loss 0.5445537567138672\n","[Training Epoch 0] Batch 643, Loss 0.5526167154312134\n","[Training Epoch 0] Batch 644, Loss 0.5488476753234863\n","[Training Epoch 0] Batch 645, Loss 0.5532156229019165\n","[Training Epoch 0] Batch 646, Loss 0.5465497374534607\n","[Training Epoch 0] Batch 647, Loss 0.5579984188079834\n","[Training Epoch 0] Batch 648, Loss 0.5361263751983643\n","[Training Epoch 0] Batch 649, Loss 0.5594147443771362\n","[Training Epoch 0] Batch 650, Loss 0.5419448614120483\n","[Training Epoch 0] Batch 651, Loss 0.5413879156112671\n","[Training Epoch 0] Batch 652, Loss 0.555923342704773\n","[Training Epoch 0] Batch 653, Loss 0.5715587139129639\n","[Training Epoch 0] Batch 654, Loss 0.5652551054954529\n","[Training Epoch 0] Batch 655, Loss 0.5631763339042664\n","[Training Epoch 0] Batch 656, Loss 0.5535127520561218\n","[Training Epoch 0] Batch 657, Loss 0.5578975677490234\n","[Training Epoch 0] Batch 658, Loss 0.5504516363143921\n","[Training Epoch 0] Batch 659, Loss 0.5635393857955933\n","[Training Epoch 0] Batch 660, Loss 0.5627994537353516\n","[Training Epoch 0] Batch 661, Loss 0.5621320009231567\n","[Training Epoch 0] Batch 662, Loss 0.5508260726928711\n","[Training Epoch 0] Batch 663, Loss 0.5620044469833374\n","[Training Epoch 0] Batch 664, Loss 0.5493110418319702\n","[Training Epoch 0] Batch 665, Loss 0.5603268146514893\n","[Training Epoch 0] Batch 666, Loss 0.5421676635742188\n","[Training Epoch 0] Batch 667, Loss 0.5614045858383179\n","[Training Epoch 0] Batch 668, Loss 0.549601674079895\n","[Training Epoch 0] Batch 669, Loss 0.5441896319389343\n","[Training Epoch 0] Batch 670, Loss 0.5533468723297119\n","[Training Epoch 0] Batch 671, Loss 0.5368335247039795\n","[Training Epoch 0] Batch 672, Loss 0.5560908317565918\n","[Training Epoch 0] Batch 673, Loss 0.5456528067588806\n","[Training Epoch 0] Batch 674, Loss 0.5422401428222656\n","[Training Epoch 0] Batch 675, Loss 0.5506965517997742\n","[Training Epoch 0] Batch 676, Loss 0.5531362295150757\n","[Training Epoch 0] Batch 677, Loss 0.5622228980064392\n","[Training Epoch 0] Batch 678, Loss 0.5460202693939209\n","[Training Epoch 0] Batch 679, Loss 0.5601514577865601\n","[Training Epoch 0] Batch 680, Loss 0.5677733421325684\n","[Training Epoch 0] Batch 681, Loss 0.5460971593856812\n","[Training Epoch 0] Batch 682, Loss 0.5558280944824219\n","[Training Epoch 0] Batch 683, Loss 0.5516635179519653\n","[Training Epoch 0] Batch 684, Loss 0.5499575138092041\n","[Training Epoch 0] Batch 685, Loss 0.5350342988967896\n","[Training Epoch 0] Batch 686, Loss 0.5520294904708862\n","[Training Epoch 0] Batch 687, Loss 0.5398821830749512\n","[Training Epoch 0] Batch 688, Loss 0.5363581776618958\n","[Training Epoch 0] Batch 689, Loss 0.5469411015510559\n","[Training Epoch 0] Batch 690, Loss 0.5548501014709473\n","[Training Epoch 0] Batch 691, Loss 0.5585199594497681\n","[Training Epoch 0] Batch 692, Loss 0.5533062219619751\n","[Training Epoch 0] Batch 693, Loss 0.542266845703125\n","[Training Epoch 0] Batch 694, Loss 0.5462484359741211\n","[Training Epoch 0] Batch 695, Loss 0.5526281595230103\n","[Training Epoch 0] Batch 696, Loss 0.5392439365386963\n","[Training Epoch 0] Batch 697, Loss 0.5384478569030762\n","[Training Epoch 0] Batch 698, Loss 0.5460155010223389\n","[Training Epoch 0] Batch 699, Loss 0.5525380969047546\n","[Training Epoch 0] Batch 700, Loss 0.5454107522964478\n","[Training Epoch 0] Batch 701, Loss 0.5394860506057739\n","[Training Epoch 0] Batch 702, Loss 0.5361994504928589\n","[Training Epoch 0] Batch 703, Loss 0.5533897876739502\n","[Training Epoch 0] Batch 704, Loss 0.5357815027236938\n","[Training Epoch 0] Batch 705, Loss 0.5508791208267212\n","[Training Epoch 0] Batch 706, Loss 0.5448518991470337\n","[Training Epoch 0] Batch 707, Loss 0.5390254259109497\n","[Training Epoch 0] Batch 708, Loss 0.5680912733078003\n","[Training Epoch 0] Batch 709, Loss 0.5476269721984863\n","[Training Epoch 0] Batch 710, Loss 0.5630896687507629\n","[Training Epoch 0] Batch 711, Loss 0.5418708324432373\n","[Training Epoch 0] Batch 712, Loss 0.5371417999267578\n","[Training Epoch 0] Batch 713, Loss 0.5494202375411987\n","[Training Epoch 0] Batch 714, Loss 0.5522444844245911\n","[Training Epoch 0] Batch 715, Loss 0.5609764456748962\n","[Training Epoch 0] Batch 716, Loss 0.552047848701477\n","[Training Epoch 0] Batch 717, Loss 0.5463852882385254\n","[Training Epoch 0] Batch 718, Loss 0.5323410034179688\n","[Training Epoch 0] Batch 719, Loss 0.5406820178031921\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 0] HR = 0.0643, NDCG = 0.0341\n","Model saved at epoch 0\n","Epoch 1 starts !\n","--------------------------------------------------------------------------------\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/data.py:150: DeprecationWarning: Sampling from a set deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n","[Training Epoch 1] Batch 0, Loss 0.535616397857666\n","[Training Epoch 1] Batch 1, Loss 0.5498355627059937\n","[Training Epoch 1] Batch 2, Loss 0.5445427894592285\n","[Training Epoch 1] Batch 3, Loss 0.5394344329833984\n","[Training Epoch 1] Batch 4, Loss 0.539726972579956\n","[Training Epoch 1] Batch 5, Loss 0.5561271905899048\n","[Training Epoch 1] Batch 6, Loss 0.5633630752563477\n","[Training Epoch 1] Batch 7, Loss 0.5413078665733337\n","[Training Epoch 1] Batch 8, Loss 0.5425112247467041\n","[Training Epoch 1] Batch 9, Loss 0.5430355668067932\n","[Training Epoch 1] Batch 10, Loss 0.5381733179092407\n","[Training Epoch 1] Batch 11, Loss 0.5636252760887146\n","[Training Epoch 1] Batch 12, Loss 0.5455393195152283\n","[Training Epoch 1] Batch 13, Loss 0.5292028188705444\n","[Training Epoch 1] Batch 14, Loss 0.5532118082046509\n","[Training Epoch 1] Batch 15, Loss 0.5413409471511841\n","[Training Epoch 1] Batch 16, Loss 0.5359720587730408\n","[Training Epoch 1] Batch 17, Loss 0.5558993816375732\n","[Training Epoch 1] Batch 18, Loss 0.5533684492111206\n","[Training Epoch 1] Batch 19, Loss 0.5458790063858032\n","[Training Epoch 1] Batch 20, Loss 0.5326452255249023\n","[Training Epoch 1] Batch 21, Loss 0.5521400570869446\n","[Training Epoch 1] Batch 22, Loss 0.5464640855789185\n","[Training Epoch 1] Batch 23, Loss 0.5470458269119263\n","[Training Epoch 1] Batch 24, Loss 0.545716404914856\n","[Training Epoch 1] Batch 25, Loss 0.5542032718658447\n","[Training Epoch 1] Batch 26, Loss 0.5514262914657593\n","[Training Epoch 1] Batch 27, Loss 0.5313113331794739\n","[Training Epoch 1] Batch 28, Loss 0.5398654341697693\n","[Training Epoch 1] Batch 29, Loss 0.5412424802780151\n","[Training Epoch 1] Batch 30, Loss 0.5533698797225952\n","[Training Epoch 1] Batch 31, Loss 0.5527522563934326\n","[Training Epoch 1] Batch 32, Loss 0.5429447889328003\n","[Training Epoch 1] Batch 33, Loss 0.5411813259124756\n","[Training Epoch 1] Batch 34, Loss 0.5462808609008789\n","[Training Epoch 1] Batch 35, Loss 0.5420924425125122\n","[Training Epoch 1] Batch 36, Loss 0.5348847508430481\n","[Training Epoch 1] Batch 37, Loss 0.5394056439399719\n","[Training Epoch 1] Batch 38, Loss 0.5465409159660339\n","[Training Epoch 1] Batch 39, Loss 0.5474188923835754\n","[Training Epoch 1] Batch 40, Loss 0.5398945808410645\n","[Training Epoch 1] Batch 41, Loss 0.5412776470184326\n","[Training Epoch 1] Batch 42, Loss 0.5309824347496033\n","[Training Epoch 1] Batch 43, Loss 0.519192099571228\n","[Training Epoch 1] Batch 44, Loss 0.530634880065918\n","[Training Epoch 1] Batch 45, Loss 0.5476489067077637\n","[Training Epoch 1] Batch 46, Loss 0.5383023023605347\n","[Training Epoch 1] Batch 47, Loss 0.5324757695198059\n","[Training Epoch 1] Batch 48, Loss 0.5413217544555664\n","[Training Epoch 1] Batch 49, Loss 0.5422908067703247\n","[Training Epoch 1] Batch 50, Loss 0.5406105518341064\n","[Training Epoch 1] Batch 51, Loss 0.562233567237854\n","[Training Epoch 1] Batch 52, Loss 0.5566462874412537\n","[Training Epoch 1] Batch 53, Loss 0.5394644737243652\n","[Training Epoch 1] Batch 54, Loss 0.5324812531471252\n","[Training Epoch 1] Batch 55, Loss 0.5472018122673035\n","[Training Epoch 1] Batch 56, Loss 0.546804666519165\n","[Training Epoch 1] Batch 57, Loss 0.5321924090385437\n","[Training Epoch 1] Batch 58, Loss 0.5400824546813965\n","[Training Epoch 1] Batch 59, Loss 0.5483084917068481\n","[Training Epoch 1] Batch 60, Loss 0.5431685447692871\n","[Training Epoch 1] Batch 61, Loss 0.5342224836349487\n","[Training Epoch 1] Batch 62, Loss 0.537634015083313\n","[Training Epoch 1] Batch 63, Loss 0.5332063436508179\n","[Training Epoch 1] Batch 64, Loss 0.5539442300796509\n","[Training Epoch 1] Batch 65, Loss 0.536480188369751\n","[Training Epoch 1] Batch 66, Loss 0.5456480979919434\n","[Training Epoch 1] Batch 67, Loss 0.5354949831962585\n","[Training Epoch 1] Batch 68, Loss 0.5388508439064026\n","[Training Epoch 1] Batch 69, Loss 0.5381215810775757\n","[Training Epoch 1] Batch 70, Loss 0.5247615575790405\n","[Training Epoch 1] Batch 71, Loss 0.5476770401000977\n","[Training Epoch 1] Batch 72, Loss 0.5578277707099915\n","[Training Epoch 1] Batch 73, Loss 0.5393102169036865\n","[Training Epoch 1] Batch 74, Loss 0.5462543964385986\n","[Training Epoch 1] Batch 75, Loss 0.540618896484375\n","[Training Epoch 1] Batch 76, Loss 0.5470005869865417\n","[Training Epoch 1] Batch 77, Loss 0.5424201488494873\n","[Training Epoch 1] Batch 78, Loss 0.5253649950027466\n","[Training Epoch 1] Batch 79, Loss 0.5279535055160522\n","[Training Epoch 1] Batch 80, Loss 0.5274063348770142\n","[Training Epoch 1] Batch 81, Loss 0.5465449690818787\n","[Training Epoch 1] Batch 82, Loss 0.5472389459609985\n","[Training Epoch 1] Batch 83, Loss 0.5360407829284668\n","[Training Epoch 1] Batch 84, Loss 0.5490549802780151\n","[Training Epoch 1] Batch 85, Loss 0.5385047793388367\n","[Training Epoch 1] Batch 86, Loss 0.5284927487373352\n","[Training Epoch 1] Batch 87, Loss 0.5264151692390442\n","[Training Epoch 1] Batch 88, Loss 0.5352301001548767\n","[Training Epoch 1] Batch 89, Loss 0.5361409187316895\n","[Training Epoch 1] Batch 90, Loss 0.5317774415016174\n","[Training Epoch 1] Batch 91, Loss 0.5499684810638428\n","[Training Epoch 1] Batch 92, Loss 0.5314457416534424\n","[Training Epoch 1] Batch 93, Loss 0.5356041193008423\n","[Training Epoch 1] Batch 94, Loss 0.5325260162353516\n","[Training Epoch 1] Batch 95, Loss 0.5537400245666504\n","[Training Epoch 1] Batch 96, Loss 0.5299097299575806\n","[Training Epoch 1] Batch 97, Loss 0.5352681279182434\n","[Training Epoch 1] Batch 98, Loss 0.5199286937713623\n","[Training Epoch 1] Batch 99, Loss 0.5355643033981323\n","[Training Epoch 1] Batch 100, Loss 0.5528755187988281\n","[Training Epoch 1] Batch 101, Loss 0.5426240563392639\n","[Training Epoch 1] Batch 102, Loss 0.5467721819877625\n","[Training Epoch 1] Batch 103, Loss 0.5388461947441101\n","[Training Epoch 1] Batch 104, Loss 0.533912718296051\n","[Training Epoch 1] Batch 105, Loss 0.5194877982139587\n","[Training Epoch 1] Batch 106, Loss 0.5353976488113403\n","[Training Epoch 1] Batch 107, Loss 0.5149968862533569\n","[Training Epoch 1] Batch 108, Loss 0.5412920713424683\n","[Training Epoch 1] Batch 109, Loss 0.5285378694534302\n","[Training Epoch 1] Batch 110, Loss 0.5508657693862915\n","[Training Epoch 1] Batch 111, Loss 0.5320660471916199\n","[Training Epoch 1] Batch 112, Loss 0.5450972318649292\n","[Training Epoch 1] Batch 113, Loss 0.5372437834739685\n","[Training Epoch 1] Batch 114, Loss 0.5322533845901489\n","[Training Epoch 1] Batch 115, Loss 0.5396403670310974\n","[Training Epoch 1] Batch 116, Loss 0.5328649282455444\n","[Training Epoch 1] Batch 117, Loss 0.5349546074867249\n","[Training Epoch 1] Batch 118, Loss 0.5467489361763\n","[Training Epoch 1] Batch 119, Loss 0.5448101162910461\n","[Training Epoch 1] Batch 120, Loss 0.534520149230957\n","[Training Epoch 1] Batch 121, Loss 0.5464673638343811\n","[Training Epoch 1] Batch 122, Loss 0.5578016042709351\n","[Training Epoch 1] Batch 123, Loss 0.5325167179107666\n","[Training Epoch 1] Batch 124, Loss 0.5436762571334839\n","[Training Epoch 1] Batch 125, Loss 0.5227172374725342\n","[Training Epoch 1] Batch 126, Loss 0.5423463582992554\n","[Training Epoch 1] Batch 127, Loss 0.5267030000686646\n","[Training Epoch 1] Batch 128, Loss 0.5314065217971802\n","[Training Epoch 1] Batch 129, Loss 0.5333702564239502\n","[Training Epoch 1] Batch 130, Loss 0.538826048374176\n","[Training Epoch 1] Batch 131, Loss 0.5285661220550537\n","[Training Epoch 1] Batch 132, Loss 0.5431176424026489\n","[Training Epoch 1] Batch 133, Loss 0.5346143245697021\n","[Training Epoch 1] Batch 134, Loss 0.5389998555183411\n","[Training Epoch 1] Batch 135, Loss 0.5510121583938599\n","[Training Epoch 1] Batch 136, Loss 0.5387394428253174\n","[Training Epoch 1] Batch 137, Loss 0.5291281938552856\n","[Training Epoch 1] Batch 138, Loss 0.5417168736457825\n","[Training Epoch 1] Batch 139, Loss 0.5401488542556763\n","[Training Epoch 1] Batch 140, Loss 0.5601751804351807\n","[Training Epoch 1] Batch 141, Loss 0.5291194915771484\n","[Training Epoch 1] Batch 142, Loss 0.5354928374290466\n","[Training Epoch 1] Batch 143, Loss 0.5489213466644287\n","[Training Epoch 1] Batch 144, Loss 0.5366694927215576\n","[Training Epoch 1] Batch 145, Loss 0.5389019250869751\n","[Training Epoch 1] Batch 146, Loss 0.5153408050537109\n","[Training Epoch 1] Batch 147, Loss 0.5364933013916016\n","[Training Epoch 1] Batch 148, Loss 0.5312713384628296\n","[Training Epoch 1] Batch 149, Loss 0.5198280811309814\n","[Training Epoch 1] Batch 150, Loss 0.5349637269973755\n","[Training Epoch 1] Batch 151, Loss 0.5329009294509888\n","[Training Epoch 1] Batch 152, Loss 0.5112800598144531\n","[Training Epoch 1] Batch 153, Loss 0.5546686053276062\n","[Training Epoch 1] Batch 154, Loss 0.5401322245597839\n","[Training Epoch 1] Batch 155, Loss 0.5220792293548584\n","[Training Epoch 1] Batch 156, Loss 0.5237143039703369\n","[Training Epoch 1] Batch 157, Loss 0.5473406910896301\n","[Training Epoch 1] Batch 158, Loss 0.5283914804458618\n","[Training Epoch 1] Batch 159, Loss 0.5470847487449646\n","[Training Epoch 1] Batch 160, Loss 0.5258004665374756\n","[Training Epoch 1] Batch 161, Loss 0.5355725288391113\n","[Training Epoch 1] Batch 162, Loss 0.5357414484024048\n","[Training Epoch 1] Batch 163, Loss 0.5401073694229126\n","[Training Epoch 1] Batch 164, Loss 0.5551294088363647\n","[Training Epoch 1] Batch 165, Loss 0.5436716675758362\n","[Training Epoch 1] Batch 166, Loss 0.5274592638015747\n","[Training Epoch 1] Batch 167, Loss 0.5552566051483154\n","[Training Epoch 1] Batch 168, Loss 0.5340456962585449\n","[Training Epoch 1] Batch 169, Loss 0.5443332195281982\n","[Training Epoch 1] Batch 170, Loss 0.5329192280769348\n","[Training Epoch 1] Batch 171, Loss 0.5272844433784485\n","[Training Epoch 1] Batch 172, Loss 0.515896737575531\n","[Training Epoch 1] Batch 173, Loss 0.521588921546936\n","[Training Epoch 1] Batch 174, Loss 0.535564124584198\n","[Training Epoch 1] Batch 175, Loss 0.5243001580238342\n","[Training Epoch 1] Batch 176, Loss 0.5308666229248047\n","[Training Epoch 1] Batch 177, Loss 0.5398732423782349\n","[Training Epoch 1] Batch 178, Loss 0.5317980051040649\n","[Training Epoch 1] Batch 179, Loss 0.5331281423568726\n","[Training Epoch 1] Batch 180, Loss 0.5391467213630676\n","[Training Epoch 1] Batch 181, Loss 0.5459187626838684\n","[Training Epoch 1] Batch 182, Loss 0.5164779424667358\n","[Training Epoch 1] Batch 183, Loss 0.5245780348777771\n","[Training Epoch 1] Batch 184, Loss 0.535514771938324\n","[Training Epoch 1] Batch 185, Loss 0.5382571220397949\n","[Training Epoch 1] Batch 186, Loss 0.5243571996688843\n","[Training Epoch 1] Batch 187, Loss 0.5350663661956787\n","[Training Epoch 1] Batch 188, Loss 0.5274622440338135\n","[Training Epoch 1] Batch 189, Loss 0.5352013111114502\n","[Training Epoch 1] Batch 190, Loss 0.5160846710205078\n","[Training Epoch 1] Batch 191, Loss 0.530505895614624\n","[Training Epoch 1] Batch 192, Loss 0.5272431373596191\n","[Training Epoch 1] Batch 193, Loss 0.5306142568588257\n","[Training Epoch 1] Batch 194, Loss 0.5321622490882874\n","[Training Epoch 1] Batch 195, Loss 0.5206918120384216\n","[Training Epoch 1] Batch 196, Loss 0.5210384726524353\n","[Training Epoch 1] Batch 197, Loss 0.5323381423950195\n","[Training Epoch 1] Batch 198, Loss 0.5159101486206055\n","[Training Epoch 1] Batch 199, Loss 0.5332124829292297\n","[Training Epoch 1] Batch 200, Loss 0.5284045934677124\n","[Training Epoch 1] Batch 201, Loss 0.5332525968551636\n","[Training Epoch 1] Batch 202, Loss 0.5321066379547119\n","[Training Epoch 1] Batch 203, Loss 0.5240917205810547\n","[Training Epoch 1] Batch 204, Loss 0.5305686593055725\n","[Training Epoch 1] Batch 205, Loss 0.5357197523117065\n","[Training Epoch 1] Batch 206, Loss 0.5392297506332397\n","[Training Epoch 1] Batch 207, Loss 0.558561384677887\n","[Training Epoch 1] Batch 208, Loss 0.5209430456161499\n","[Training Epoch 1] Batch 209, Loss 0.534157931804657\n","[Training Epoch 1] Batch 210, Loss 0.5166739225387573\n","[Training Epoch 1] Batch 211, Loss 0.5374716520309448\n","[Training Epoch 1] Batch 212, Loss 0.5378162860870361\n","[Training Epoch 1] Batch 213, Loss 0.5308156609535217\n","[Training Epoch 1] Batch 214, Loss 0.5207501649856567\n","[Training Epoch 1] Batch 215, Loss 0.517126739025116\n","[Training Epoch 1] Batch 216, Loss 0.5335171222686768\n","[Training Epoch 1] Batch 217, Loss 0.5453864932060242\n","[Training Epoch 1] Batch 218, Loss 0.5233603715896606\n","[Training Epoch 1] Batch 219, Loss 0.5421292781829834\n","[Training Epoch 1] Batch 220, Loss 0.5214250087738037\n","[Training Epoch 1] Batch 221, Loss 0.5124450325965881\n","[Training Epoch 1] Batch 222, Loss 0.5359221696853638\n","[Training Epoch 1] Batch 223, Loss 0.5340245962142944\n","[Training Epoch 1] Batch 224, Loss 0.5337996482849121\n","[Training Epoch 1] Batch 225, Loss 0.5428532958030701\n","[Training Epoch 1] Batch 226, Loss 0.5324826836585999\n","[Training Epoch 1] Batch 227, Loss 0.5306510925292969\n","[Training Epoch 1] Batch 228, Loss 0.5329931974411011\n","[Training Epoch 1] Batch 229, Loss 0.5185083150863647\n","[Training Epoch 1] Batch 230, Loss 0.5489733219146729\n","[Training Epoch 1] Batch 231, Loss 0.5472674369812012\n","[Training Epoch 1] Batch 232, Loss 0.5333507061004639\n","[Training Epoch 1] Batch 233, Loss 0.5250693559646606\n","[Training Epoch 1] Batch 234, Loss 0.5150197744369507\n","[Training Epoch 1] Batch 235, Loss 0.5318255424499512\n","[Training Epoch 1] Batch 236, Loss 0.5285424590110779\n","[Training Epoch 1] Batch 237, Loss 0.5370138883590698\n","[Training Epoch 1] Batch 238, Loss 0.5356273055076599\n","[Training Epoch 1] Batch 239, Loss 0.5233300924301147\n","[Training Epoch 1] Batch 240, Loss 0.534397304058075\n","[Training Epoch 1] Batch 241, Loss 0.5256690979003906\n","[Training Epoch 1] Batch 242, Loss 0.5160639882087708\n","[Training Epoch 1] Batch 243, Loss 0.5091187357902527\n","[Training Epoch 1] Batch 244, Loss 0.5230544805526733\n","[Training Epoch 1] Batch 245, Loss 0.5239495635032654\n","[Training Epoch 1] Batch 246, Loss 0.5315942764282227\n","[Training Epoch 1] Batch 247, Loss 0.5511619448661804\n","[Training Epoch 1] Batch 248, Loss 0.5423150658607483\n","[Training Epoch 1] Batch 249, Loss 0.5308205485343933\n","[Training Epoch 1] Batch 250, Loss 0.5180339813232422\n","[Training Epoch 1] Batch 251, Loss 0.5226984024047852\n","[Training Epoch 1] Batch 252, Loss 0.515862226486206\n","[Training Epoch 1] Batch 253, Loss 0.5405223965644836\n","[Training Epoch 1] Batch 254, Loss 0.5223172903060913\n","[Training Epoch 1] Batch 255, Loss 0.527349591255188\n","[Training Epoch 1] Batch 256, Loss 0.5435709357261658\n","[Training Epoch 1] Batch 257, Loss 0.5327780246734619\n","[Training Epoch 1] Batch 258, Loss 0.5125369429588318\n","[Training Epoch 1] Batch 259, Loss 0.5146183371543884\n","[Training Epoch 1] Batch 260, Loss 0.5295268297195435\n","[Training Epoch 1] Batch 261, Loss 0.5244741439819336\n","[Training Epoch 1] Batch 262, Loss 0.5394923686981201\n","[Training Epoch 1] Batch 263, Loss 0.519373893737793\n","[Training Epoch 1] Batch 264, Loss 0.5150107145309448\n","[Training Epoch 1] Batch 265, Loss 0.5357988476753235\n","[Training Epoch 1] Batch 266, Loss 0.5240461826324463\n","[Training Epoch 1] Batch 267, Loss 0.5223336219787598\n","[Training Epoch 1] Batch 268, Loss 0.5240122079849243\n","[Training Epoch 1] Batch 269, Loss 0.5471637845039368\n","[Training Epoch 1] Batch 270, Loss 0.5295525193214417\n","[Training Epoch 1] Batch 271, Loss 0.5297013521194458\n","[Training Epoch 1] Batch 272, Loss 0.5279374718666077\n","[Training Epoch 1] Batch 273, Loss 0.5325344204902649\n","[Training Epoch 1] Batch 274, Loss 0.5167806148529053\n","[Training Epoch 1] Batch 275, Loss 0.5286372900009155\n","[Training Epoch 1] Batch 276, Loss 0.5343741178512573\n","[Training Epoch 1] Batch 277, Loss 0.5208327174186707\n","[Training Epoch 1] Batch 278, Loss 0.5227338075637817\n","[Training Epoch 1] Batch 279, Loss 0.528432309627533\n","[Training Epoch 1] Batch 280, Loss 0.5106978416442871\n","[Training Epoch 1] Batch 281, Loss 0.5102239847183228\n","[Training Epoch 1] Batch 282, Loss 0.5049057006835938\n","[Training Epoch 1] Batch 283, Loss 0.5235446691513062\n","[Training Epoch 1] Batch 284, Loss 0.5363336205482483\n","[Training Epoch 1] Batch 285, Loss 0.5062976479530334\n","[Training Epoch 1] Batch 286, Loss 0.529517650604248\n","[Training Epoch 1] Batch 287, Loss 0.517169713973999\n","[Training Epoch 1] Batch 288, Loss 0.527305006980896\n","[Training Epoch 1] Batch 289, Loss 0.5230893492698669\n","[Training Epoch 1] Batch 290, Loss 0.5102885365486145\n","[Training Epoch 1] Batch 291, Loss 0.5267945528030396\n","[Training Epoch 1] Batch 292, Loss 0.5267365574836731\n","[Training Epoch 1] Batch 293, Loss 0.5268027782440186\n","[Training Epoch 1] Batch 294, Loss 0.5158941745758057\n","[Training Epoch 1] Batch 295, Loss 0.5181897282600403\n","[Training Epoch 1] Batch 296, Loss 0.5162217617034912\n","[Training Epoch 1] Batch 297, Loss 0.5272971391677856\n","[Training Epoch 1] Batch 298, Loss 0.5231391191482544\n","[Training Epoch 1] Batch 299, Loss 0.514828085899353\n","[Training Epoch 1] Batch 300, Loss 0.526449978351593\n","[Training Epoch 1] Batch 301, Loss 0.5158350467681885\n","[Training Epoch 1] Batch 302, Loss 0.5373371839523315\n","[Training Epoch 1] Batch 303, Loss 0.5295066237449646\n","[Training Epoch 1] Batch 304, Loss 0.5267543792724609\n","[Training Epoch 1] Batch 305, Loss 0.5405248403549194\n","[Training Epoch 1] Batch 306, Loss 0.5214543342590332\n","[Training Epoch 1] Batch 307, Loss 0.5272029042243958\n","[Training Epoch 1] Batch 308, Loss 0.5170685648918152\n","[Training Epoch 1] Batch 309, Loss 0.5115008354187012\n","[Training Epoch 1] Batch 310, Loss 0.5206536650657654\n","[Training Epoch 1] Batch 311, Loss 0.522592306137085\n","[Training Epoch 1] Batch 312, Loss 0.5241174101829529\n","[Training Epoch 1] Batch 313, Loss 0.5268357992172241\n","[Training Epoch 1] Batch 314, Loss 0.5170931816101074\n","[Training Epoch 1] Batch 315, Loss 0.5385351181030273\n","[Training Epoch 1] Batch 316, Loss 0.5334209203720093\n","[Training Epoch 1] Batch 317, Loss 0.5176454782485962\n","[Training Epoch 1] Batch 318, Loss 0.5299126505851746\n","[Training Epoch 1] Batch 319, Loss 0.5065672397613525\n","[Training Epoch 1] Batch 320, Loss 0.5279256701469421\n","[Training Epoch 1] Batch 321, Loss 0.5251537561416626\n","[Training Epoch 1] Batch 322, Loss 0.5169773101806641\n","[Training Epoch 1] Batch 323, Loss 0.5166234970092773\n","[Training Epoch 1] Batch 324, Loss 0.5355079174041748\n","[Training Epoch 1] Batch 325, Loss 0.5078588128089905\n","[Training Epoch 1] Batch 326, Loss 0.5217918157577515\n","[Training Epoch 1] Batch 327, Loss 0.5309675335884094\n","[Training Epoch 1] Batch 328, Loss 0.5198854207992554\n","[Training Epoch 1] Batch 329, Loss 0.5266362428665161\n","[Training Epoch 1] Batch 330, Loss 0.5281569957733154\n","[Training Epoch 1] Batch 331, Loss 0.5167261958122253\n","[Training Epoch 1] Batch 332, Loss 0.5248737335205078\n","[Training Epoch 1] Batch 333, Loss 0.5047793388366699\n","[Training Epoch 1] Batch 334, Loss 0.5290184617042542\n","[Training Epoch 1] Batch 335, Loss 0.5341312289237976\n","[Training Epoch 1] Batch 336, Loss 0.5383707284927368\n","[Training Epoch 1] Batch 337, Loss 0.5185064673423767\n","[Training Epoch 1] Batch 338, Loss 0.535519540309906\n","[Training Epoch 1] Batch 339, Loss 0.5061925053596497\n","[Training Epoch 1] Batch 340, Loss 0.525234580039978\n","[Training Epoch 1] Batch 341, Loss 0.5413827300071716\n","[Training Epoch 1] Batch 342, Loss 0.5363292694091797\n","[Training Epoch 1] Batch 343, Loss 0.5275321006774902\n","[Training Epoch 1] Batch 344, Loss 0.5239700675010681\n","[Training Epoch 1] Batch 345, Loss 0.5084783434867859\n","[Training Epoch 1] Batch 346, Loss 0.5109392404556274\n","[Training Epoch 1] Batch 347, Loss 0.5197596549987793\n","[Training Epoch 1] Batch 348, Loss 0.5195884704589844\n","[Training Epoch 1] Batch 349, Loss 0.5410560965538025\n","[Training Epoch 1] Batch 350, Loss 0.5212984085083008\n","[Training Epoch 1] Batch 351, Loss 0.5151892900466919\n","[Training Epoch 1] Batch 352, Loss 0.5201191902160645\n","[Training Epoch 1] Batch 353, Loss 0.5204350352287292\n","[Training Epoch 1] Batch 354, Loss 0.5141875743865967\n","[Training Epoch 1] Batch 355, Loss 0.5081897974014282\n","[Training Epoch 1] Batch 356, Loss 0.519262433052063\n","[Training Epoch 1] Batch 357, Loss 0.5031849145889282\n","[Training Epoch 1] Batch 358, Loss 0.5278770327568054\n","[Training Epoch 1] Batch 359, Loss 0.5227336883544922\n","[Training Epoch 1] Batch 360, Loss 0.5276065468788147\n","[Training Epoch 1] Batch 361, Loss 0.5223351120948792\n","[Training Epoch 1] Batch 362, Loss 0.5011758208274841\n","[Training Epoch 1] Batch 363, Loss 0.5345542430877686\n","[Training Epoch 1] Batch 364, Loss 0.530076265335083\n","[Training Epoch 1] Batch 365, Loss 0.5011035799980164\n","[Training Epoch 1] Batch 366, Loss 0.5125373601913452\n","[Training Epoch 1] Batch 367, Loss 0.5299054980278015\n","[Training Epoch 1] Batch 368, Loss 0.5273601412773132\n","[Training Epoch 1] Batch 369, Loss 0.5263375043869019\n","[Training Epoch 1] Batch 370, Loss 0.5271053314208984\n","[Training Epoch 1] Batch 371, Loss 0.5103368759155273\n","[Training Epoch 1] Batch 372, Loss 0.5315943956375122\n","[Training Epoch 1] Batch 373, Loss 0.520872950553894\n","[Training Epoch 1] Batch 374, Loss 0.5172302722930908\n","[Training Epoch 1] Batch 375, Loss 0.5155497789382935\n","[Training Epoch 1] Batch 376, Loss 0.515571117401123\n","[Training Epoch 1] Batch 377, Loss 0.5252236723899841\n","[Training Epoch 1] Batch 378, Loss 0.5064805746078491\n","[Training Epoch 1] Batch 379, Loss 0.5128673315048218\n","[Training Epoch 1] Batch 380, Loss 0.5314569473266602\n","[Training Epoch 1] Batch 381, Loss 0.5215564966201782\n","[Training Epoch 1] Batch 382, Loss 0.5312209725379944\n","[Training Epoch 1] Batch 383, Loss 0.5126426815986633\n","[Training Epoch 1] Batch 384, Loss 0.525818943977356\n","[Training Epoch 1] Batch 385, Loss 0.5101465582847595\n","[Training Epoch 1] Batch 386, Loss 0.5043227076530457\n","[Training Epoch 1] Batch 387, Loss 0.5175503492355347\n","[Training Epoch 1] Batch 388, Loss 0.5287481546401978\n","[Training Epoch 1] Batch 389, Loss 0.5283495783805847\n","[Training Epoch 1] Batch 390, Loss 0.518354058265686\n","[Training Epoch 1] Batch 391, Loss 0.5289475917816162\n","[Training Epoch 1] Batch 392, Loss 0.5129802227020264\n","[Training Epoch 1] Batch 393, Loss 0.5384607911109924\n","[Training Epoch 1] Batch 394, Loss 0.5440163612365723\n","[Training Epoch 1] Batch 395, Loss 0.5199227333068848\n","[Training Epoch 1] Batch 396, Loss 0.5269270539283752\n","[Training Epoch 1] Batch 397, Loss 0.5261365175247192\n","[Training Epoch 1] Batch 398, Loss 0.5315213203430176\n","[Training Epoch 1] Batch 399, Loss 0.5232430696487427\n","[Training Epoch 1] Batch 400, Loss 0.5204276442527771\n","[Training Epoch 1] Batch 401, Loss 0.5296298265457153\n","[Training Epoch 1] Batch 402, Loss 0.5061127543449402\n","[Training Epoch 1] Batch 403, Loss 0.5071982145309448\n","[Training Epoch 1] Batch 404, Loss 0.5007964372634888\n","[Training Epoch 1] Batch 405, Loss 0.5077511072158813\n","[Training Epoch 1] Batch 406, Loss 0.511427104473114\n","[Training Epoch 1] Batch 407, Loss 0.507839560508728\n","[Training Epoch 1] Batch 408, Loss 0.534683346748352\n","[Training Epoch 1] Batch 409, Loss 0.5311533808708191\n","[Training Epoch 1] Batch 410, Loss 0.5202199220657349\n","[Training Epoch 1] Batch 411, Loss 0.5137900114059448\n","[Training Epoch 1] Batch 412, Loss 0.5281672477722168\n","[Training Epoch 1] Batch 413, Loss 0.5274412631988525\n","[Training Epoch 1] Batch 414, Loss 0.5324888229370117\n","[Training Epoch 1] Batch 415, Loss 0.505371630191803\n","[Training Epoch 1] Batch 416, Loss 0.5188559889793396\n","[Training Epoch 1] Batch 417, Loss 0.527946949005127\n","[Training Epoch 1] Batch 418, Loss 0.5154000520706177\n","[Training Epoch 1] Batch 419, Loss 0.5197474956512451\n","[Training Epoch 1] Batch 420, Loss 0.5352663397789001\n","[Training Epoch 1] Batch 421, Loss 0.5108434557914734\n","[Training Epoch 1] Batch 422, Loss 0.5231466889381409\n","[Training Epoch 1] Batch 423, Loss 0.5151708126068115\n","[Training Epoch 1] Batch 424, Loss 0.5232807397842407\n","[Training Epoch 1] Batch 425, Loss 0.5287163257598877\n","[Training Epoch 1] Batch 426, Loss 0.5249935388565063\n","[Training Epoch 1] Batch 427, Loss 0.5168161392211914\n","[Training Epoch 1] Batch 428, Loss 0.5095455646514893\n","[Training Epoch 1] Batch 429, Loss 0.5282706022262573\n","[Training Epoch 1] Batch 430, Loss 0.5221048593521118\n","[Training Epoch 1] Batch 431, Loss 0.5247658491134644\n","[Training Epoch 1] Batch 432, Loss 0.50830078125\n","[Training Epoch 1] Batch 433, Loss 0.5182698965072632\n","[Training Epoch 1] Batch 434, Loss 0.4990251362323761\n","[Training Epoch 1] Batch 435, Loss 0.5156738758087158\n","[Training Epoch 1] Batch 436, Loss 0.5134260654449463\n","[Training Epoch 1] Batch 437, Loss 0.5502142906188965\n","[Training Epoch 1] Batch 438, Loss 0.5107080340385437\n","[Training Epoch 1] Batch 439, Loss 0.5125606060028076\n","[Training Epoch 1] Batch 440, Loss 0.5142548084259033\n","[Training Epoch 1] Batch 441, Loss 0.5214319229125977\n","[Training Epoch 1] Batch 442, Loss 0.5115557909011841\n","[Training Epoch 1] Batch 443, Loss 0.5270266532897949\n","[Training Epoch 1] Batch 444, Loss 0.5130887031555176\n","[Training Epoch 1] Batch 445, Loss 0.5252728462219238\n","[Training Epoch 1] Batch 446, Loss 0.5397422909736633\n","[Training Epoch 1] Batch 447, Loss 0.5240950584411621\n","[Training Epoch 1] Batch 448, Loss 0.5092984437942505\n","[Training Epoch 1] Batch 449, Loss 0.5028522610664368\n","[Training Epoch 1] Batch 450, Loss 0.5018702745437622\n","[Training Epoch 1] Batch 451, Loss 0.5211803913116455\n","[Training Epoch 1] Batch 452, Loss 0.4979078769683838\n","[Training Epoch 1] Batch 453, Loss 0.5193811655044556\n","[Training Epoch 1] Batch 454, Loss 0.5221657752990723\n","[Training Epoch 1] Batch 455, Loss 0.5100122094154358\n","[Training Epoch 1] Batch 456, Loss 0.5302468538284302\n","[Training Epoch 1] Batch 457, Loss 0.5205810070037842\n","[Training Epoch 1] Batch 458, Loss 0.5154941082000732\n","[Training Epoch 1] Batch 459, Loss 0.4967150390148163\n","[Training Epoch 1] Batch 460, Loss 0.5251672267913818\n","[Training Epoch 1] Batch 461, Loss 0.5108642578125\n","[Training Epoch 1] Batch 462, Loss 0.5327747464179993\n","[Training Epoch 1] Batch 463, Loss 0.5244478583335876\n","[Training Epoch 1] Batch 464, Loss 0.4938831925392151\n","[Training Epoch 1] Batch 465, Loss 0.501216471195221\n","[Training Epoch 1] Batch 466, Loss 0.5287607908248901\n","[Training Epoch 1] Batch 467, Loss 0.5115413665771484\n","[Training Epoch 1] Batch 468, Loss 0.5131640434265137\n","[Training Epoch 1] Batch 469, Loss 0.5272141695022583\n","[Training Epoch 1] Batch 470, Loss 0.516080915927887\n","[Training Epoch 1] Batch 471, Loss 0.5288739204406738\n","[Training Epoch 1] Batch 472, Loss 0.5244500041007996\n","[Training Epoch 1] Batch 473, Loss 0.5129930973052979\n","[Training Epoch 1] Batch 474, Loss 0.499761700630188\n","[Training Epoch 1] Batch 475, Loss 0.5239057540893555\n","[Training Epoch 1] Batch 476, Loss 0.5193946361541748\n","[Training Epoch 1] Batch 477, Loss 0.5152655839920044\n","[Training Epoch 1] Batch 478, Loss 0.5026775598526001\n","[Training Epoch 1] Batch 479, Loss 0.4949679970741272\n","[Training Epoch 1] Batch 480, Loss 0.5219390392303467\n","[Training Epoch 1] Batch 481, Loss 0.5184224843978882\n","[Training Epoch 1] Batch 482, Loss 0.5229261517524719\n","[Training Epoch 1] Batch 483, Loss 0.5080549716949463\n","[Training Epoch 1] Batch 484, Loss 0.5067307353019714\n","[Training Epoch 1] Batch 485, Loss 0.517169713973999\n","[Training Epoch 1] Batch 486, Loss 0.5226916074752808\n","[Training Epoch 1] Batch 487, Loss 0.5160889625549316\n","[Training Epoch 1] Batch 488, Loss 0.5349143743515015\n","[Training Epoch 1] Batch 489, Loss 0.5143544673919678\n","[Training Epoch 1] Batch 490, Loss 0.5265958309173584\n","[Training Epoch 1] Batch 491, Loss 0.5092718601226807\n","[Training Epoch 1] Batch 492, Loss 0.5160337686538696\n","[Training Epoch 1] Batch 493, Loss 0.5152788758277893\n","[Training Epoch 1] Batch 494, Loss 0.5091023445129395\n","[Training Epoch 1] Batch 495, Loss 0.509988009929657\n","[Training Epoch 1] Batch 496, Loss 0.4989282488822937\n","[Training Epoch 1] Batch 497, Loss 0.5194252729415894\n","[Training Epoch 1] Batch 498, Loss 0.5224639177322388\n","[Training Epoch 1] Batch 499, Loss 0.5240585207939148\n","[Training Epoch 1] Batch 500, Loss 0.526821494102478\n","[Training Epoch 1] Batch 501, Loss 0.5210189819335938\n","[Training Epoch 1] Batch 502, Loss 0.505309522151947\n","[Training Epoch 1] Batch 503, Loss 0.5008116960525513\n","[Training Epoch 1] Batch 504, Loss 0.5041844844818115\n","[Training Epoch 1] Batch 505, Loss 0.4963703155517578\n","[Training Epoch 1] Batch 506, Loss 0.4880751371383667\n","[Training Epoch 1] Batch 507, Loss 0.5532974600791931\n","[Training Epoch 1] Batch 508, Loss 0.508399486541748\n","[Training Epoch 1] Batch 509, Loss 0.5200864672660828\n","[Training Epoch 1] Batch 510, Loss 0.5188140869140625\n","[Training Epoch 1] Batch 511, Loss 0.5140120983123779\n","[Training Epoch 1] Batch 512, Loss 0.5160616040229797\n","[Training Epoch 1] Batch 513, Loss 0.5275771021842957\n","[Training Epoch 1] Batch 514, Loss 0.5135051608085632\n","[Training Epoch 1] Batch 515, Loss 0.5036871433258057\n","[Training Epoch 1] Batch 516, Loss 0.5046534538269043\n","[Training Epoch 1] Batch 517, Loss 0.5139111280441284\n","[Training Epoch 1] Batch 518, Loss 0.5169950723648071\n","[Training Epoch 1] Batch 519, Loss 0.5125779509544373\n","[Training Epoch 1] Batch 520, Loss 0.5205817222595215\n","[Training Epoch 1] Batch 521, Loss 0.5133252143859863\n","[Training Epoch 1] Batch 522, Loss 0.5087628960609436\n","[Training Epoch 1] Batch 523, Loss 0.5453654527664185\n","[Training Epoch 1] Batch 524, Loss 0.49667924642562866\n","[Training Epoch 1] Batch 525, Loss 0.5106319189071655\n","[Training Epoch 1] Batch 526, Loss 0.498390793800354\n","[Training Epoch 1] Batch 527, Loss 0.4905456602573395\n","[Training Epoch 1] Batch 528, Loss 0.4895307123661041\n","[Training Epoch 1] Batch 529, Loss 0.5240834951400757\n","[Training Epoch 1] Batch 530, Loss 0.5222172737121582\n","[Training Epoch 1] Batch 531, Loss 0.504378080368042\n","[Training Epoch 1] Batch 532, Loss 0.5089043378829956\n","[Training Epoch 1] Batch 533, Loss 0.5115484595298767\n","[Training Epoch 1] Batch 534, Loss 0.5047537684440613\n","[Training Epoch 1] Batch 535, Loss 0.5093706846237183\n","[Training Epoch 1] Batch 536, Loss 0.48640701174736023\n","[Training Epoch 1] Batch 537, Loss 0.5309730768203735\n","[Training Epoch 1] Batch 538, Loss 0.5067762136459351\n","[Training Epoch 1] Batch 539, Loss 0.5054973363876343\n","[Training Epoch 1] Batch 540, Loss 0.5094930529594421\n","[Training Epoch 1] Batch 541, Loss 0.5306158065795898\n","[Training Epoch 1] Batch 542, Loss 0.5151249766349792\n","[Training Epoch 1] Batch 543, Loss 0.5121908187866211\n","[Training Epoch 1] Batch 544, Loss 0.5104109048843384\n","[Training Epoch 1] Batch 545, Loss 0.511966347694397\n","[Training Epoch 1] Batch 546, Loss 0.5168786644935608\n","[Training Epoch 1] Batch 547, Loss 0.5295791625976562\n","[Training Epoch 1] Batch 548, Loss 0.49452343583106995\n","[Training Epoch 1] Batch 549, Loss 0.5216548442840576\n","[Training Epoch 1] Batch 550, Loss 0.5060780048370361\n","[Training Epoch 1] Batch 551, Loss 0.5175368785858154\n","[Training Epoch 1] Batch 552, Loss 0.5117467641830444\n","[Training Epoch 1] Batch 553, Loss 0.5098588466644287\n","[Training Epoch 1] Batch 554, Loss 0.516594409942627\n","[Training Epoch 1] Batch 555, Loss 0.505828857421875\n","[Training Epoch 1] Batch 556, Loss 0.5313254594802856\n","[Training Epoch 1] Batch 557, Loss 0.5116149187088013\n","[Training Epoch 1] Batch 558, Loss 0.5115773677825928\n","[Training Epoch 1] Batch 559, Loss 0.49696797132492065\n","[Training Epoch 1] Batch 560, Loss 0.49500805139541626\n","[Training Epoch 1] Batch 561, Loss 0.4977267384529114\n","[Training Epoch 1] Batch 562, Loss 0.5163567066192627\n","[Training Epoch 1] Batch 563, Loss 0.520342230796814\n","[Training Epoch 1] Batch 564, Loss 0.5027892589569092\n","[Training Epoch 1] Batch 565, Loss 0.49869415163993835\n","[Training Epoch 1] Batch 566, Loss 0.4985096752643585\n","[Training Epoch 1] Batch 567, Loss 0.5210645198822021\n","[Training Epoch 1] Batch 568, Loss 0.5288189649581909\n","[Training Epoch 1] Batch 569, Loss 0.5514377951622009\n","[Training Epoch 1] Batch 570, Loss 0.5188463926315308\n","[Training Epoch 1] Batch 571, Loss 0.5269176959991455\n","[Training Epoch 1] Batch 572, Loss 0.5191537141799927\n","[Training Epoch 1] Batch 573, Loss 0.5003897547721863\n","[Training Epoch 1] Batch 574, Loss 0.5109572410583496\n","[Training Epoch 1] Batch 575, Loss 0.5346326231956482\n","[Training Epoch 1] Batch 576, Loss 0.5091090202331543\n","[Training Epoch 1] Batch 577, Loss 0.5128082633018494\n","[Training Epoch 1] Batch 578, Loss 0.5050790309906006\n","[Training Epoch 1] Batch 579, Loss 0.5186431407928467\n","[Training Epoch 1] Batch 580, Loss 0.5198619961738586\n","[Training Epoch 1] Batch 581, Loss 0.501981794834137\n","[Training Epoch 1] Batch 582, Loss 0.49873101711273193\n","[Training Epoch 1] Batch 583, Loss 0.49705857038497925\n","[Training Epoch 1] Batch 584, Loss 0.5244177579879761\n","[Training Epoch 1] Batch 585, Loss 0.5114820003509521\n","[Training Epoch 1] Batch 586, Loss 0.5186275839805603\n","[Training Epoch 1] Batch 587, Loss 0.5225920677185059\n","[Training Epoch 1] Batch 588, Loss 0.5078165531158447\n","[Training Epoch 1] Batch 589, Loss 0.5294916033744812\n","[Training Epoch 1] Batch 590, Loss 0.5252221822738647\n","[Training Epoch 1] Batch 591, Loss 0.5146312117576599\n","[Training Epoch 1] Batch 592, Loss 0.5085473656654358\n","[Training Epoch 1] Batch 593, Loss 0.516446590423584\n","[Training Epoch 1] Batch 594, Loss 0.49948784708976746\n","[Training Epoch 1] Batch 595, Loss 0.513503909111023\n","[Training Epoch 1] Batch 596, Loss 0.5380878448486328\n","[Training Epoch 1] Batch 597, Loss 0.5272270441055298\n","[Training Epoch 1] Batch 598, Loss 0.5150370597839355\n","[Training Epoch 1] Batch 599, Loss 0.5201030969619751\n","[Training Epoch 1] Batch 600, Loss 0.5209181904792786\n","[Training Epoch 1] Batch 601, Loss 0.49044060707092285\n","[Training Epoch 1] Batch 602, Loss 0.5118876099586487\n","[Training Epoch 1] Batch 603, Loss 0.5143253207206726\n","[Training Epoch 1] Batch 604, Loss 0.506566047668457\n","[Training Epoch 1] Batch 605, Loss 0.5017622709274292\n","[Training Epoch 1] Batch 606, Loss 0.5001016855239868\n","[Training Epoch 1] Batch 607, Loss 0.5031958222389221\n","[Training Epoch 1] Batch 608, Loss 0.4873450994491577\n","[Training Epoch 1] Batch 609, Loss 0.4929884076118469\n","[Training Epoch 1] Batch 610, Loss 0.535922646522522\n","[Training Epoch 1] Batch 611, Loss 0.5487301349639893\n","[Training Epoch 1] Batch 612, Loss 0.4949256479740143\n","[Training Epoch 1] Batch 613, Loss 0.5321571826934814\n","[Training Epoch 1] Batch 614, Loss 0.49892452359199524\n","[Training Epoch 1] Batch 615, Loss 0.5101315379142761\n","[Training Epoch 1] Batch 616, Loss 0.5008459091186523\n","[Training Epoch 1] Batch 617, Loss 0.49766427278518677\n","[Training Epoch 1] Batch 618, Loss 0.525911808013916\n","[Training Epoch 1] Batch 619, Loss 0.49191296100616455\n","[Training Epoch 1] Batch 620, Loss 0.5411261320114136\n","[Training Epoch 1] Batch 621, Loss 0.4764559268951416\n","[Training Epoch 1] Batch 622, Loss 0.5461143255233765\n","[Training Epoch 1] Batch 623, Loss 0.5188987255096436\n","[Training Epoch 1] Batch 624, Loss 0.5285556316375732\n","[Training Epoch 1] Batch 625, Loss 0.4904654920101166\n","[Training Epoch 1] Batch 626, Loss 0.5282979607582092\n","[Training Epoch 1] Batch 627, Loss 0.5122475624084473\n","[Training Epoch 1] Batch 628, Loss 0.4943719506263733\n","[Training Epoch 1] Batch 629, Loss 0.5204530954360962\n","[Training Epoch 1] Batch 630, Loss 0.5175492763519287\n","[Training Epoch 1] Batch 631, Loss 0.530733048915863\n","[Training Epoch 1] Batch 632, Loss 0.511612057685852\n","[Training Epoch 1] Batch 633, Loss 0.5123097896575928\n","[Training Epoch 1] Batch 634, Loss 0.48931634426116943\n","[Training Epoch 1] Batch 635, Loss 0.49912208318710327\n","[Training Epoch 1] Batch 636, Loss 0.5002532005310059\n","[Training Epoch 1] Batch 637, Loss 0.5082749128341675\n","[Training Epoch 1] Batch 638, Loss 0.4806334376335144\n","[Training Epoch 1] Batch 639, Loss 0.5300965309143066\n","[Training Epoch 1] Batch 640, Loss 0.5213141441345215\n","[Training Epoch 1] Batch 641, Loss 0.523261308670044\n","[Training Epoch 1] Batch 642, Loss 0.5101686120033264\n","[Training Epoch 1] Batch 643, Loss 0.49684756994247437\n","[Training Epoch 1] Batch 644, Loss 0.5101330280303955\n","[Training Epoch 1] Batch 645, Loss 0.509921669960022\n","[Training Epoch 1] Batch 646, Loss 0.5136258602142334\n","[Training Epoch 1] Batch 647, Loss 0.497728168964386\n","[Training Epoch 1] Batch 648, Loss 0.5036929845809937\n","[Training Epoch 1] Batch 649, Loss 0.5047607421875\n","[Training Epoch 1] Batch 650, Loss 0.502964437007904\n","[Training Epoch 1] Batch 651, Loss 0.5049055814743042\n","[Training Epoch 1] Batch 652, Loss 0.5391831398010254\n","[Training Epoch 1] Batch 653, Loss 0.5190221667289734\n","[Training Epoch 1] Batch 654, Loss 0.5138158798217773\n","[Training Epoch 1] Batch 655, Loss 0.5151164531707764\n","[Training Epoch 1] Batch 656, Loss 0.5034822821617126\n","[Training Epoch 1] Batch 657, Loss 0.5023892521858215\n","[Training Epoch 1] Batch 658, Loss 0.5134680271148682\n","[Training Epoch 1] Batch 659, Loss 0.5005228519439697\n","[Training Epoch 1] Batch 660, Loss 0.5033421516418457\n","[Training Epoch 1] Batch 661, Loss 0.4994177520275116\n","[Training Epoch 1] Batch 662, Loss 0.4928589463233948\n","[Training Epoch 1] Batch 663, Loss 0.520614743232727\n","[Training Epoch 1] Batch 664, Loss 0.4970169961452484\n","[Training Epoch 1] Batch 665, Loss 0.5271515846252441\n","[Training Epoch 1] Batch 666, Loss 0.530010461807251\n","[Training Epoch 1] Batch 667, Loss 0.5218997001647949\n","[Training Epoch 1] Batch 668, Loss 0.5023642778396606\n","[Training Epoch 1] Batch 669, Loss 0.5245479345321655\n","[Training Epoch 1] Batch 670, Loss 0.5099897384643555\n","[Training Epoch 1] Batch 671, Loss 0.5170696377754211\n","[Training Epoch 1] Batch 672, Loss 0.5174415111541748\n","[Training Epoch 1] Batch 673, Loss 0.5239475965499878\n","[Training Epoch 1] Batch 674, Loss 0.5028872489929199\n","[Training Epoch 1] Batch 675, Loss 0.4979840815067291\n","[Training Epoch 1] Batch 676, Loss 0.5213029384613037\n","[Training Epoch 1] Batch 677, Loss 0.5118709802627563\n","[Training Epoch 1] Batch 678, Loss 0.5112206935882568\n","[Training Epoch 1] Batch 679, Loss 0.49967315793037415\n","[Training Epoch 1] Batch 680, Loss 0.48854392766952515\n","[Training Epoch 1] Batch 681, Loss 0.49556824564933777\n","[Training Epoch 1] Batch 682, Loss 0.5147963762283325\n","[Training Epoch 1] Batch 683, Loss 0.5008985996246338\n","[Training Epoch 1] Batch 684, Loss 0.5119451880455017\n","[Training Epoch 1] Batch 685, Loss 0.5020371675491333\n","[Training Epoch 1] Batch 686, Loss 0.5111981630325317\n","[Training Epoch 1] Batch 687, Loss 0.5086173415184021\n","[Training Epoch 1] Batch 688, Loss 0.546423614025116\n","[Training Epoch 1] Batch 689, Loss 0.5097918510437012\n","[Training Epoch 1] Batch 690, Loss 0.5250846147537231\n","[Training Epoch 1] Batch 691, Loss 0.5131855010986328\n","[Training Epoch 1] Batch 692, Loss 0.5078111886978149\n","[Training Epoch 1] Batch 693, Loss 0.5067427158355713\n","[Training Epoch 1] Batch 694, Loss 0.5127852559089661\n","[Training Epoch 1] Batch 695, Loss 0.5076351165771484\n","[Training Epoch 1] Batch 696, Loss 0.5107818841934204\n","[Training Epoch 1] Batch 697, Loss 0.5147933959960938\n","[Training Epoch 1] Batch 698, Loss 0.5182485580444336\n","[Training Epoch 1] Batch 699, Loss 0.5140259265899658\n","[Training Epoch 1] Batch 700, Loss 0.5145554542541504\n","[Training Epoch 1] Batch 701, Loss 0.5115762948989868\n","[Training Epoch 1] Batch 702, Loss 0.5190877318382263\n","[Training Epoch 1] Batch 703, Loss 0.5143639445304871\n","[Training Epoch 1] Batch 704, Loss 0.5096108317375183\n","[Training Epoch 1] Batch 705, Loss 0.49997469782829285\n","[Training Epoch 1] Batch 706, Loss 0.4846453368663788\n","[Training Epoch 1] Batch 707, Loss 0.49504491686820984\n","[Training Epoch 1] Batch 708, Loss 0.5261318683624268\n","[Training Epoch 1] Batch 709, Loss 0.5167942643165588\n","[Training Epoch 1] Batch 710, Loss 0.511958658695221\n","[Training Epoch 1] Batch 711, Loss 0.4948699474334717\n","[Training Epoch 1] Batch 712, Loss 0.5227726101875305\n","[Training Epoch 1] Batch 713, Loss 0.5051031112670898\n","[Training Epoch 1] Batch 714, Loss 0.5086424946784973\n","[Training Epoch 1] Batch 715, Loss 0.5168615579605103\n","[Training Epoch 1] Batch 716, Loss 0.5137183666229248\n","[Training Epoch 1] Batch 717, Loss 0.5050132274627686\n","[Training Epoch 1] Batch 718, Loss 0.5122781991958618\n","[Training Epoch 1] Batch 719, Loss 0.509523332118988\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 1] HR = 0.0614, NDCG = 0.0317\n","Epoch 2 starts !\n","--------------------------------------------------------------------------------\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/data.py:150: DeprecationWarning: Sampling from a set deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n","[Training Epoch 2] Batch 0, Loss 0.5124646425247192\n","[Training Epoch 2] Batch 1, Loss 0.5091911554336548\n","[Training Epoch 2] Batch 2, Loss 0.4996154308319092\n","[Training Epoch 2] Batch 3, Loss 0.5352504253387451\n","[Training Epoch 2] Batch 4, Loss 0.5207562446594238\n","[Training Epoch 2] Batch 5, Loss 0.5255290269851685\n","[Training Epoch 2] Batch 6, Loss 0.49647119641304016\n","[Training Epoch 2] Batch 7, Loss 0.5236684083938599\n","[Training Epoch 2] Batch 8, Loss 0.5308933258056641\n","[Training Epoch 2] Batch 9, Loss 0.5337157845497131\n","[Training Epoch 2] Batch 10, Loss 0.5217839479446411\n","[Training Epoch 2] Batch 11, Loss 0.5098037123680115\n","[Training Epoch 2] Batch 12, Loss 0.5380845069885254\n","[Training Epoch 2] Batch 13, Loss 0.5026517510414124\n","[Training Epoch 2] Batch 14, Loss 0.5171082615852356\n","[Training Epoch 2] Batch 15, Loss 0.4986532926559448\n","[Training Epoch 2] Batch 16, Loss 0.5096246004104614\n","[Training Epoch 2] Batch 17, Loss 0.512957751750946\n","[Training Epoch 2] Batch 18, Loss 0.5100332498550415\n","[Training Epoch 2] Batch 19, Loss 0.49093368649482727\n","[Training Epoch 2] Batch 20, Loss 0.5107491612434387\n","[Training Epoch 2] Batch 21, Loss 0.49728667736053467\n","[Training Epoch 2] Batch 22, Loss 0.5069130063056946\n","[Training Epoch 2] Batch 23, Loss 0.5054734349250793\n","[Training Epoch 2] Batch 24, Loss 0.5250438451766968\n","[Training Epoch 2] Batch 25, Loss 0.5023036003112793\n","[Training Epoch 2] Batch 26, Loss 0.5086410045623779\n","[Training Epoch 2] Batch 27, Loss 0.5178273916244507\n","[Training Epoch 2] Batch 28, Loss 0.5118895769119263\n","[Training Epoch 2] Batch 29, Loss 0.5096237659454346\n","[Training Epoch 2] Batch 30, Loss 0.5224287509918213\n","[Training Epoch 2] Batch 31, Loss 0.49291497468948364\n","[Training Epoch 2] Batch 32, Loss 0.5220082998275757\n","[Training Epoch 2] Batch 33, Loss 0.5150502920150757\n","[Training Epoch 2] Batch 34, Loss 0.4947621524333954\n","[Training Epoch 2] Batch 35, Loss 0.4955995976924896\n","[Training Epoch 2] Batch 36, Loss 0.5050467848777771\n","[Training Epoch 2] Batch 37, Loss 0.502936840057373\n","[Training Epoch 2] Batch 38, Loss 0.5083825588226318\n","[Training Epoch 2] Batch 39, Loss 0.532667875289917\n","[Training Epoch 2] Batch 40, Loss 0.509480357170105\n","[Training Epoch 2] Batch 41, Loss 0.5189692378044128\n","[Training Epoch 2] Batch 42, Loss 0.5134867429733276\n","[Training Epoch 2] Batch 43, Loss 0.5281503200531006\n","[Training Epoch 2] Batch 44, Loss 0.5004289746284485\n","[Training Epoch 2] Batch 45, Loss 0.4944266676902771\n","[Training Epoch 2] Batch 46, Loss 0.5052989721298218\n","[Training Epoch 2] Batch 47, Loss 0.507434070110321\n","[Training Epoch 2] Batch 48, Loss 0.5103555917739868\n","[Training Epoch 2] Batch 49, Loss 0.5170024633407593\n","[Training Epoch 2] Batch 50, Loss 0.5156546831130981\n","[Training Epoch 2] Batch 51, Loss 0.5061386227607727\n","[Training Epoch 2] Batch 52, Loss 0.4983740448951721\n","[Training Epoch 2] Batch 53, Loss 0.5045214295387268\n","[Training Epoch 2] Batch 54, Loss 0.48343750834465027\n","[Training Epoch 2] Batch 55, Loss 0.5037721395492554\n","[Training Epoch 2] Batch 56, Loss 0.5197478532791138\n","[Training Epoch 2] Batch 57, Loss 0.537981390953064\n","[Training Epoch 2] Batch 58, Loss 0.504508376121521\n","[Training Epoch 2] Batch 59, Loss 0.485781192779541\n","[Training Epoch 2] Batch 60, Loss 0.498912513256073\n","[Training Epoch 2] Batch 61, Loss 0.5247185230255127\n","[Training Epoch 2] Batch 62, Loss 0.495183527469635\n","[Training Epoch 2] Batch 63, Loss 0.5015919208526611\n","[Training Epoch 2] Batch 64, Loss 0.5150858163833618\n","[Training Epoch 2] Batch 65, Loss 0.513189435005188\n","[Training Epoch 2] Batch 66, Loss 0.5089421272277832\n","[Training Epoch 2] Batch 67, Loss 0.5193067789077759\n","[Training Epoch 2] Batch 68, Loss 0.5207112431526184\n","[Training Epoch 2] Batch 69, Loss 0.4896658658981323\n","[Training Epoch 2] Batch 70, Loss 0.49892526865005493\n","[Training Epoch 2] Batch 71, Loss 0.5246529579162598\n","[Training Epoch 2] Batch 72, Loss 0.5228005647659302\n","[Training Epoch 2] Batch 73, Loss 0.5023305416107178\n","[Training Epoch 2] Batch 74, Loss 0.5310788750648499\n","[Training Epoch 2] Batch 75, Loss 0.5108903050422668\n","[Training Epoch 2] Batch 76, Loss 0.48169106245040894\n","[Training Epoch 2] Batch 77, Loss 0.499031126499176\n","[Training Epoch 2] Batch 78, Loss 0.48828572034835815\n","[Training Epoch 2] Batch 79, Loss 0.4954845905303955\n","[Training Epoch 2] Batch 80, Loss 0.5089383125305176\n","[Training Epoch 2] Batch 81, Loss 0.5201438665390015\n","[Training Epoch 2] Batch 82, Loss 0.5157662630081177\n","[Training Epoch 2] Batch 83, Loss 0.49339091777801514\n","[Training Epoch 2] Batch 84, Loss 0.5373437404632568\n","[Training Epoch 2] Batch 85, Loss 0.5062310099601746\n","[Training Epoch 2] Batch 86, Loss 0.5096595287322998\n","[Training Epoch 2] Batch 87, Loss 0.5160638689994812\n","[Training Epoch 2] Batch 88, Loss 0.4964491128921509\n","[Training Epoch 2] Batch 89, Loss 0.5061185359954834\n","[Training Epoch 2] Batch 90, Loss 0.48908284306526184\n","[Training Epoch 2] Batch 91, Loss 0.5191685557365417\n","[Training Epoch 2] Batch 92, Loss 0.528156042098999\n","[Training Epoch 2] Batch 93, Loss 0.4929500222206116\n","[Training Epoch 2] Batch 94, Loss 0.5125635266304016\n","[Training Epoch 2] Batch 95, Loss 0.5035640001296997\n","[Training Epoch 2] Batch 96, Loss 0.4916132092475891\n","[Training Epoch 2] Batch 97, Loss 0.4991876482963562\n","[Training Epoch 2] Batch 98, Loss 0.4848040044307709\n","[Training Epoch 2] Batch 99, Loss 0.5127438306808472\n","[Training Epoch 2] Batch 100, Loss 0.5093883275985718\n","[Training Epoch 2] Batch 101, Loss 0.5191661715507507\n","[Training Epoch 2] Batch 102, Loss 0.5070804357528687\n","[Training Epoch 2] Batch 103, Loss 0.49452662467956543\n","[Training Epoch 2] Batch 104, Loss 0.4977944493293762\n","[Training Epoch 2] Batch 105, Loss 0.5004273653030396\n","[Training Epoch 2] Batch 106, Loss 0.49631014466285706\n","[Training Epoch 2] Batch 107, Loss 0.4977978467941284\n","[Training Epoch 2] Batch 108, Loss 0.533063530921936\n","[Training Epoch 2] Batch 109, Loss 0.5000262260437012\n","[Training Epoch 2] Batch 110, Loss 0.4958569407463074\n","[Training Epoch 2] Batch 111, Loss 0.4974299669265747\n","[Training Epoch 2] Batch 112, Loss 0.4774439036846161\n","[Training Epoch 2] Batch 113, Loss 0.502406120300293\n","[Training Epoch 2] Batch 114, Loss 0.5307999849319458\n","[Training Epoch 2] Batch 115, Loss 0.5153964161872864\n","[Training Epoch 2] Batch 116, Loss 0.49274200201034546\n","[Training Epoch 2] Batch 117, Loss 0.5067704319953918\n","[Training Epoch 2] Batch 118, Loss 0.4785163104534149\n","[Training Epoch 2] Batch 119, Loss 0.504647970199585\n","[Training Epoch 2] Batch 120, Loss 0.5158849954605103\n","[Training Epoch 2] Batch 121, Loss 0.5198476314544678\n","[Training Epoch 2] Batch 122, Loss 0.48482340574264526\n","[Training Epoch 2] Batch 123, Loss 0.4839797019958496\n","[Training Epoch 2] Batch 124, Loss 0.48323315382003784\n","[Training Epoch 2] Batch 125, Loss 0.52535480260849\n","[Training Epoch 2] Batch 126, Loss 0.5142654180526733\n","[Training Epoch 2] Batch 127, Loss 0.48125743865966797\n","[Training Epoch 2] Batch 128, Loss 0.507870614528656\n","[Training Epoch 2] Batch 129, Loss 0.48803284764289856\n","[Training Epoch 2] Batch 130, Loss 0.49877074360847473\n","[Training Epoch 2] Batch 131, Loss 0.5023021101951599\n","[Training Epoch 2] Batch 132, Loss 0.5208033323287964\n","[Training Epoch 2] Batch 133, Loss 0.5162264704704285\n","[Training Epoch 2] Batch 134, Loss 0.511016845703125\n","[Training Epoch 2] Batch 135, Loss 0.5154134035110474\n","[Training Epoch 2] Batch 136, Loss 0.5239156484603882\n","[Training Epoch 2] Batch 137, Loss 0.5076209306716919\n","[Training Epoch 2] Batch 138, Loss 0.48964619636535645\n","[Training Epoch 2] Batch 139, Loss 0.512176513671875\n","[Training Epoch 2] Batch 140, Loss 0.5221298933029175\n","[Training Epoch 2] Batch 141, Loss 0.5007668733596802\n","[Training Epoch 2] Batch 142, Loss 0.5139874219894409\n","[Training Epoch 2] Batch 143, Loss 0.4852062165737152\n","[Training Epoch 2] Batch 144, Loss 0.47636085748672485\n","[Training Epoch 2] Batch 145, Loss 0.5066476464271545\n","[Training Epoch 2] Batch 146, Loss 0.5131895542144775\n","[Training Epoch 2] Batch 147, Loss 0.5272645354270935\n","[Training Epoch 2] Batch 148, Loss 0.5026111006736755\n","[Training Epoch 2] Batch 149, Loss 0.5006752014160156\n","[Training Epoch 2] Batch 150, Loss 0.5105010867118835\n","[Training Epoch 2] Batch 151, Loss 0.5026925206184387\n","[Training Epoch 2] Batch 152, Loss 0.5240593552589417\n","[Training Epoch 2] Batch 153, Loss 0.5019391179084778\n","[Training Epoch 2] Batch 154, Loss 0.5172015428543091\n","[Training Epoch 2] Batch 155, Loss 0.5236228704452515\n","[Training Epoch 2] Batch 156, Loss 0.5093907117843628\n","[Training Epoch 2] Batch 157, Loss 0.4963175356388092\n","[Training Epoch 2] Batch 158, Loss 0.507152795791626\n","[Training Epoch 2] Batch 159, Loss 0.5026313662528992\n","[Training Epoch 2] Batch 160, Loss 0.48709386587142944\n","[Training Epoch 2] Batch 161, Loss 0.5138108730316162\n","[Training Epoch 2] Batch 162, Loss 0.4832804203033447\n","[Training Epoch 2] Batch 163, Loss 0.5078862905502319\n","[Training Epoch 2] Batch 164, Loss 0.49722182750701904\n","[Training Epoch 2] Batch 165, Loss 0.5074385404586792\n","[Training Epoch 2] Batch 166, Loss 0.493844598531723\n","[Training Epoch 2] Batch 167, Loss 0.5127782821655273\n","[Training Epoch 2] Batch 168, Loss 0.5160036087036133\n","[Training Epoch 2] Batch 169, Loss 0.506034255027771\n","[Training Epoch 2] Batch 170, Loss 0.49275147914886475\n","[Training Epoch 2] Batch 171, Loss 0.48520326614379883\n","[Training Epoch 2] Batch 172, Loss 0.5168619155883789\n","[Training Epoch 2] Batch 173, Loss 0.5161089897155762\n","[Training Epoch 2] Batch 174, Loss 0.5024778842926025\n","[Training Epoch 2] Batch 175, Loss 0.46382442116737366\n","[Training Epoch 2] Batch 176, Loss 0.4971565902233124\n","[Training Epoch 2] Batch 177, Loss 0.49124854803085327\n","[Training Epoch 2] Batch 178, Loss 0.5030609369277954\n","[Training Epoch 2] Batch 179, Loss 0.5194342136383057\n","[Training Epoch 2] Batch 180, Loss 0.529114842414856\n","[Training Epoch 2] Batch 181, Loss 0.5167758464813232\n","[Training Epoch 2] Batch 182, Loss 0.5115845203399658\n","[Training Epoch 2] Batch 183, Loss 0.49420517683029175\n","[Training Epoch 2] Batch 184, Loss 0.5155560970306396\n","[Training Epoch 2] Batch 185, Loss 0.4970521330833435\n","[Training Epoch 2] Batch 186, Loss 0.5056714415550232\n","[Training Epoch 2] Batch 187, Loss 0.5154576301574707\n","[Training Epoch 2] Batch 188, Loss 0.5146793127059937\n","[Training Epoch 2] Batch 189, Loss 0.5258711576461792\n","[Training Epoch 2] Batch 190, Loss 0.5111794471740723\n","[Training Epoch 2] Batch 191, Loss 0.4925926923751831\n","[Training Epoch 2] Batch 192, Loss 0.49781447649002075\n","[Training Epoch 2] Batch 193, Loss 0.5358254909515381\n","[Training Epoch 2] Batch 194, Loss 0.4922489523887634\n","[Training Epoch 2] Batch 195, Loss 0.4902479350566864\n","[Training Epoch 2] Batch 196, Loss 0.5077946186065674\n","[Training Epoch 2] Batch 197, Loss 0.517281711101532\n","[Training Epoch 2] Batch 198, Loss 0.509161114692688\n","[Training Epoch 2] Batch 199, Loss 0.5169713497161865\n","[Training Epoch 2] Batch 200, Loss 0.5188153982162476\n","[Training Epoch 2] Batch 201, Loss 0.5267907381057739\n","[Training Epoch 2] Batch 202, Loss 0.49767887592315674\n","[Training Epoch 2] Batch 203, Loss 0.5151312351226807\n","[Training Epoch 2] Batch 204, Loss 0.49763476848602295\n","[Training Epoch 2] Batch 205, Loss 0.5056041479110718\n","[Training Epoch 2] Batch 206, Loss 0.49534252285957336\n","[Training Epoch 2] Batch 207, Loss 0.4749506711959839\n","[Training Epoch 2] Batch 208, Loss 0.5039903521537781\n","[Training Epoch 2] Batch 209, Loss 0.5277612209320068\n","[Training Epoch 2] Batch 210, Loss 0.4988623261451721\n","[Training Epoch 2] Batch 211, Loss 0.5198982954025269\n","[Training Epoch 2] Batch 212, Loss 0.5211809873580933\n","[Training Epoch 2] Batch 213, Loss 0.5157108306884766\n","[Training Epoch 2] Batch 214, Loss 0.5051125288009644\n","[Training Epoch 2] Batch 215, Loss 0.4895031452178955\n","[Training Epoch 2] Batch 216, Loss 0.5131638050079346\n","[Training Epoch 2] Batch 217, Loss 0.48615068197250366\n","[Training Epoch 2] Batch 218, Loss 0.5257359743118286\n","[Training Epoch 2] Batch 219, Loss 0.5029114484786987\n","[Training Epoch 2] Batch 220, Loss 0.501915693283081\n","[Training Epoch 2] Batch 221, Loss 0.48641282320022583\n","[Training Epoch 2] Batch 222, Loss 0.5331329107284546\n","[Training Epoch 2] Batch 223, Loss 0.5232784748077393\n","[Training Epoch 2] Batch 224, Loss 0.5017297267913818\n","[Training Epoch 2] Batch 225, Loss 0.500706672668457\n","[Training Epoch 2] Batch 226, Loss 0.508629560470581\n","[Training Epoch 2] Batch 227, Loss 0.5478506684303284\n","[Training Epoch 2] Batch 228, Loss 0.49153074622154236\n","[Training Epoch 2] Batch 229, Loss 0.5209375023841858\n","[Training Epoch 2] Batch 230, Loss 0.5204360485076904\n","[Training Epoch 2] Batch 231, Loss 0.5025333166122437\n","[Training Epoch 2] Batch 232, Loss 0.515444278717041\n","[Training Epoch 2] Batch 233, Loss 0.507797360420227\n","[Training Epoch 2] Batch 234, Loss 0.5062836408615112\n","[Training Epoch 2] Batch 235, Loss 0.5041715502738953\n","[Training Epoch 2] Batch 236, Loss 0.5400763750076294\n","[Training Epoch 2] Batch 237, Loss 0.4781322777271271\n","[Training Epoch 2] Batch 238, Loss 0.5106714963912964\n","[Training Epoch 2] Batch 239, Loss 0.48714888095855713\n","[Training Epoch 2] Batch 240, Loss 0.5239467024803162\n","[Training Epoch 2] Batch 241, Loss 0.5264707803726196\n","[Training Epoch 2] Batch 242, Loss 0.49141624569892883\n","[Training Epoch 2] Batch 243, Loss 0.5410906076431274\n","[Training Epoch 2] Batch 244, Loss 0.5088725090026855\n","[Training Epoch 2] Batch 245, Loss 0.5127578973770142\n","[Training Epoch 2] Batch 246, Loss 0.5343044400215149\n","[Training Epoch 2] Batch 247, Loss 0.4913473129272461\n","[Training Epoch 2] Batch 248, Loss 0.5211372971534729\n","[Training Epoch 2] Batch 249, Loss 0.5207245349884033\n","[Training Epoch 2] Batch 250, Loss 0.5180661082267761\n","[Training Epoch 2] Batch 251, Loss 0.5007018446922302\n","[Training Epoch 2] Batch 252, Loss 0.5194664001464844\n","[Training Epoch 2] Batch 253, Loss 0.5136944055557251\n","[Training Epoch 2] Batch 254, Loss 0.4995197653770447\n","[Training Epoch 2] Batch 255, Loss 0.5195733308792114\n","[Training Epoch 2] Batch 256, Loss 0.5071887969970703\n","[Training Epoch 2] Batch 257, Loss 0.491519570350647\n","[Training Epoch 2] Batch 258, Loss 0.4990329444408417\n","[Training Epoch 2] Batch 259, Loss 0.47623568773269653\n","[Training Epoch 2] Batch 260, Loss 0.49784594774246216\n","[Training Epoch 2] Batch 261, Loss 0.5049690008163452\n","[Training Epoch 2] Batch 262, Loss 0.509636640548706\n","[Training Epoch 2] Batch 263, Loss 0.5073667764663696\n","[Training Epoch 2] Batch 264, Loss 0.4902358651161194\n","[Training Epoch 2] Batch 265, Loss 0.49494022130966187\n","[Training Epoch 2] Batch 266, Loss 0.4934401512145996\n","[Training Epoch 2] Batch 267, Loss 0.5161492228507996\n","[Training Epoch 2] Batch 268, Loss 0.5169276595115662\n","[Training Epoch 2] Batch 269, Loss 0.49794816970825195\n","[Training Epoch 2] Batch 270, Loss 0.4700416922569275\n","[Training Epoch 2] Batch 271, Loss 0.5027194023132324\n","[Training Epoch 2] Batch 272, Loss 0.5135526657104492\n","[Training Epoch 2] Batch 273, Loss 0.48180413246154785\n","[Training Epoch 2] Batch 274, Loss 0.5298100709915161\n","[Training Epoch 2] Batch 275, Loss 0.5023536682128906\n","[Training Epoch 2] Batch 276, Loss 0.5344257950782776\n","[Training Epoch 2] Batch 277, Loss 0.509337306022644\n","[Training Epoch 2] Batch 278, Loss 0.48537397384643555\n","[Training Epoch 2] Batch 279, Loss 0.49440833926200867\n","[Training Epoch 2] Batch 280, Loss 0.5157907605171204\n","[Training Epoch 2] Batch 281, Loss 0.49533718824386597\n","[Training Epoch 2] Batch 282, Loss 0.48219186067581177\n","[Training Epoch 2] Batch 283, Loss 0.4955497086048126\n","[Training Epoch 2] Batch 284, Loss 0.512589156627655\n","[Training Epoch 2] Batch 285, Loss 0.4849932789802551\n","[Training Epoch 2] Batch 286, Loss 0.5033295750617981\n","[Training Epoch 2] Batch 287, Loss 0.4828157126903534\n","[Training Epoch 2] Batch 288, Loss 0.5054106712341309\n","[Training Epoch 2] Batch 289, Loss 0.5011768341064453\n","[Training Epoch 2] Batch 290, Loss 0.5203877687454224\n","[Training Epoch 2] Batch 291, Loss 0.4825272560119629\n","[Training Epoch 2] Batch 292, Loss 0.493918240070343\n","[Training Epoch 2] Batch 293, Loss 0.5137251615524292\n","[Training Epoch 2] Batch 294, Loss 0.5182028412818909\n","[Training Epoch 2] Batch 295, Loss 0.5208161473274231\n","[Training Epoch 2] Batch 296, Loss 0.5068480968475342\n","[Training Epoch 2] Batch 297, Loss 0.48723986744880676\n","[Training Epoch 2] Batch 298, Loss 0.49859708547592163\n","[Training Epoch 2] Batch 299, Loss 0.4918988347053528\n","[Training Epoch 2] Batch 300, Loss 0.4793345630168915\n","[Training Epoch 2] Batch 301, Loss 0.4996861219406128\n","[Training Epoch 2] Batch 302, Loss 0.4964064955711365\n","[Training Epoch 2] Batch 303, Loss 0.5227257013320923\n","[Training Epoch 2] Batch 304, Loss 0.48676687479019165\n","[Training Epoch 2] Batch 305, Loss 0.48350903391838074\n","[Training Epoch 2] Batch 306, Loss 0.49962371587753296\n","[Training Epoch 2] Batch 307, Loss 0.5112406611442566\n","[Training Epoch 2] Batch 308, Loss 0.5065531730651855\n","[Training Epoch 2] Batch 309, Loss 0.48795557022094727\n","[Training Epoch 2] Batch 310, Loss 0.48586058616638184\n","[Training Epoch 2] Batch 311, Loss 0.4868108332157135\n","[Training Epoch 2] Batch 312, Loss 0.4997512996196747\n","[Training Epoch 2] Batch 313, Loss 0.5096432566642761\n","[Training Epoch 2] Batch 314, Loss 0.5065481662750244\n","[Training Epoch 2] Batch 315, Loss 0.5004739165306091\n","[Training Epoch 2] Batch 316, Loss 0.5054019689559937\n","[Training Epoch 2] Batch 317, Loss 0.48105406761169434\n","[Training Epoch 2] Batch 318, Loss 0.4939129054546356\n","[Training Epoch 2] Batch 319, Loss 0.5041185617446899\n","[Training Epoch 2] Batch 320, Loss 0.5028517246246338\n","[Training Epoch 2] Batch 321, Loss 0.5181944370269775\n","[Training Epoch 2] Batch 322, Loss 0.5200127959251404\n","[Training Epoch 2] Batch 323, Loss 0.49007394909858704\n","[Training Epoch 2] Batch 324, Loss 0.4810563623905182\n","[Training Epoch 2] Batch 325, Loss 0.5074876546859741\n","[Training Epoch 2] Batch 326, Loss 0.49821507930755615\n","[Training Epoch 2] Batch 327, Loss 0.5040073394775391\n","[Training Epoch 2] Batch 328, Loss 0.49475085735321045\n","[Training Epoch 2] Batch 329, Loss 0.49660083651542664\n","[Training Epoch 2] Batch 330, Loss 0.5466904044151306\n","[Training Epoch 2] Batch 331, Loss 0.49220719933509827\n","[Training Epoch 2] Batch 332, Loss 0.5039259195327759\n","[Training Epoch 2] Batch 333, Loss 0.4935285449028015\n","[Training Epoch 2] Batch 334, Loss 0.49819570779800415\n","[Training Epoch 2] Batch 335, Loss 0.5213645100593567\n","[Training Epoch 2] Batch 336, Loss 0.47639086842536926\n","[Training Epoch 2] Batch 337, Loss 0.5201789140701294\n","[Training Epoch 2] Batch 338, Loss 0.5307421088218689\n","[Training Epoch 2] Batch 339, Loss 0.4887135922908783\n","[Training Epoch 2] Batch 340, Loss 0.506209671497345\n","[Training Epoch 2] Batch 341, Loss 0.5470174551010132\n","[Training Epoch 2] Batch 342, Loss 0.5096613168716431\n","[Training Epoch 2] Batch 343, Loss 0.4757779538631439\n","[Training Epoch 2] Batch 344, Loss 0.5130267143249512\n","[Training Epoch 2] Batch 345, Loss 0.5200334787368774\n","[Training Epoch 2] Batch 346, Loss 0.5052489042282104\n","[Training Epoch 2] Batch 347, Loss 0.5349791646003723\n","[Training Epoch 2] Batch 348, Loss 0.4990575909614563\n","[Training Epoch 2] Batch 349, Loss 0.5282028913497925\n","[Training Epoch 2] Batch 350, Loss 0.5038122534751892\n","[Training Epoch 2] Batch 351, Loss 0.49891528487205505\n","[Training Epoch 2] Batch 352, Loss 0.5072708129882812\n","[Training Epoch 2] Batch 353, Loss 0.513164222240448\n","[Training Epoch 2] Batch 354, Loss 0.5061295032501221\n","[Training Epoch 2] Batch 355, Loss 0.4860476851463318\n","[Training Epoch 2] Batch 356, Loss 0.5248026847839355\n","[Training Epoch 2] Batch 357, Loss 0.481747567653656\n","[Training Epoch 2] Batch 358, Loss 0.4825406074523926\n","[Training Epoch 2] Batch 359, Loss 0.4977132976055145\n","[Training Epoch 2] Batch 360, Loss 0.5234103202819824\n","[Training Epoch 2] Batch 361, Loss 0.5166363716125488\n","[Training Epoch 2] Batch 362, Loss 0.4861883521080017\n","[Training Epoch 2] Batch 363, Loss 0.5225125551223755\n","[Training Epoch 2] Batch 364, Loss 0.49817466735839844\n","[Training Epoch 2] Batch 365, Loss 0.5177278518676758\n","[Training Epoch 2] Batch 366, Loss 0.49672091007232666\n","[Training Epoch 2] Batch 367, Loss 0.49546676874160767\n","[Training Epoch 2] Batch 368, Loss 0.5012015700340271\n","[Training Epoch 2] Batch 369, Loss 0.4865230619907379\n","[Training Epoch 2] Batch 370, Loss 0.47686779499053955\n","[Training Epoch 2] Batch 371, Loss 0.5012316703796387\n","[Training Epoch 2] Batch 372, Loss 0.5093371868133545\n","[Training Epoch 2] Batch 373, Loss 0.508080780506134\n","[Training Epoch 2] Batch 374, Loss 0.537643551826477\n","[Training Epoch 2] Batch 375, Loss 0.4921503961086273\n","[Training Epoch 2] Batch 376, Loss 0.5013221502304077\n","[Training Epoch 2] Batch 377, Loss 0.5141392350196838\n","[Training Epoch 2] Batch 378, Loss 0.5151633024215698\n","[Training Epoch 2] Batch 379, Loss 0.4881994128227234\n","[Training Epoch 2] Batch 380, Loss 0.49057474732398987\n","[Training Epoch 2] Batch 381, Loss 0.5144650936126709\n","[Training Epoch 2] Batch 382, Loss 0.4932817816734314\n","[Training Epoch 2] Batch 383, Loss 0.4975258708000183\n","[Training Epoch 2] Batch 384, Loss 0.5069363713264465\n","[Training Epoch 2] Batch 385, Loss 0.4999828338623047\n","[Training Epoch 2] Batch 386, Loss 0.5023252964019775\n","[Training Epoch 2] Batch 387, Loss 0.5141569375991821\n","[Training Epoch 2] Batch 388, Loss 0.5164322853088379\n","[Training Epoch 2] Batch 389, Loss 0.5104957818984985\n","[Training Epoch 2] Batch 390, Loss 0.5327823162078857\n","[Training Epoch 2] Batch 391, Loss 0.5176454186439514\n","[Training Epoch 2] Batch 392, Loss 0.5069580078125\n","[Training Epoch 2] Batch 393, Loss 0.4999346435070038\n","[Training Epoch 2] Batch 394, Loss 0.4998081922531128\n","[Training Epoch 2] Batch 395, Loss 0.491871178150177\n","[Training Epoch 2] Batch 396, Loss 0.48911672830581665\n","[Training Epoch 2] Batch 397, Loss 0.4845088720321655\n","[Training Epoch 2] Batch 398, Loss 0.5126044750213623\n","[Training Epoch 2] Batch 399, Loss 0.512641429901123\n","[Training Epoch 2] Batch 400, Loss 0.49639856815338135\n","[Training Epoch 2] Batch 401, Loss 0.48353874683380127\n","[Training Epoch 2] Batch 402, Loss 0.5138958692550659\n","[Training Epoch 2] Batch 403, Loss 0.49251267313957214\n","[Training Epoch 2] Batch 404, Loss 0.5280956625938416\n","[Training Epoch 2] Batch 405, Loss 0.49855589866638184\n","[Training Epoch 2] Batch 406, Loss 0.5327451229095459\n","[Training Epoch 2] Batch 407, Loss 0.4927575886249542\n","[Training Epoch 2] Batch 408, Loss 0.5150717496871948\n","[Training Epoch 2] Batch 409, Loss 0.47844040393829346\n","[Training Epoch 2] Batch 410, Loss 0.49377158284187317\n","[Training Epoch 2] Batch 411, Loss 0.5195461511611938\n","[Training Epoch 2] Batch 412, Loss 0.5126156806945801\n","[Training Epoch 2] Batch 413, Loss 0.5016589164733887\n","[Training Epoch 2] Batch 414, Loss 0.504564642906189\n","[Training Epoch 2] Batch 415, Loss 0.5163156986236572\n","[Training Epoch 2] Batch 416, Loss 0.4914340376853943\n","[Training Epoch 2] Batch 417, Loss 0.4808545410633087\n","[Training Epoch 2] Batch 418, Loss 0.49963730573654175\n","[Training Epoch 2] Batch 419, Loss 0.5007401704788208\n","[Training Epoch 2] Batch 420, Loss 0.519016683101654\n","[Training Epoch 2] Batch 421, Loss 0.4927980303764343\n","[Training Epoch 2] Batch 422, Loss 0.48924773931503296\n","[Training Epoch 2] Batch 423, Loss 0.5376978516578674\n","[Training Epoch 2] Batch 424, Loss 0.4879339337348938\n","[Training Epoch 2] Batch 425, Loss 0.49143072962760925\n","[Training Epoch 2] Batch 426, Loss 0.5106455087661743\n","[Training Epoch 2] Batch 427, Loss 0.49854081869125366\n","[Training Epoch 2] Batch 428, Loss 0.5137345790863037\n","[Training Epoch 2] Batch 429, Loss 0.5088456273078918\n","[Training Epoch 2] Batch 430, Loss 0.5019598603248596\n","[Training Epoch 2] Batch 431, Loss 0.5314654111862183\n","[Training Epoch 2] Batch 432, Loss 0.4721214771270752\n","[Training Epoch 2] Batch 433, Loss 0.5113826990127563\n","[Training Epoch 2] Batch 434, Loss 0.48774123191833496\n","[Training Epoch 2] Batch 435, Loss 0.4874863028526306\n","[Training Epoch 2] Batch 436, Loss 0.5019760727882385\n","[Training Epoch 2] Batch 437, Loss 0.49843069911003113\n","[Training Epoch 2] Batch 438, Loss 0.4959694743156433\n","[Training Epoch 2] Batch 439, Loss 0.49720633029937744\n","[Training Epoch 2] Batch 440, Loss 0.487716943025589\n","[Training Epoch 2] Batch 441, Loss 0.48876601457595825\n","[Training Epoch 2] Batch 442, Loss 0.480085551738739\n","[Training Epoch 2] Batch 443, Loss 0.5066728591918945\n","[Training Epoch 2] Batch 444, Loss 0.5077589154243469\n","[Training Epoch 2] Batch 445, Loss 0.5210698246955872\n","[Training Epoch 2] Batch 446, Loss 0.47661134600639343\n","[Training Epoch 2] Batch 447, Loss 0.521721363067627\n","[Training Epoch 2] Batch 448, Loss 0.5028278231620789\n","[Training Epoch 2] Batch 449, Loss 0.5122594833374023\n","[Training Epoch 2] Batch 450, Loss 0.48136574029922485\n","[Training Epoch 2] Batch 451, Loss 0.4953547418117523\n","[Training Epoch 2] Batch 452, Loss 0.4994063675403595\n","[Training Epoch 2] Batch 453, Loss 0.5220192074775696\n","[Training Epoch 2] Batch 454, Loss 0.4981432855129242\n","[Training Epoch 2] Batch 455, Loss 0.4982033371925354\n","[Training Epoch 2] Batch 456, Loss 0.5087255239486694\n","[Training Epoch 2] Batch 457, Loss 0.49546104669570923\n","[Training Epoch 2] Batch 458, Loss 0.5098574757575989\n","[Training Epoch 2] Batch 459, Loss 0.5027773380279541\n","[Training Epoch 2] Batch 460, Loss 0.5056219100952148\n","[Training Epoch 2] Batch 461, Loss 0.47534382343292236\n","[Training Epoch 2] Batch 462, Loss 0.4997802972793579\n","[Training Epoch 2] Batch 463, Loss 0.5029765367507935\n","[Training Epoch 2] Batch 464, Loss 0.47098588943481445\n","[Training Epoch 2] Batch 465, Loss 0.509788453578949\n","[Training Epoch 2] Batch 466, Loss 0.45610499382019043\n","[Training Epoch 2] Batch 467, Loss 0.5169723033905029\n","[Training Epoch 2] Batch 468, Loss 0.4860396981239319\n","[Training Epoch 2] Batch 469, Loss 0.48760223388671875\n","[Training Epoch 2] Batch 470, Loss 0.5013494491577148\n","[Training Epoch 2] Batch 471, Loss 0.5038658380508423\n","[Training Epoch 2] Batch 472, Loss 0.5157743096351624\n","[Training Epoch 2] Batch 473, Loss 0.49316278100013733\n","[Training Epoch 2] Batch 474, Loss 0.4824531078338623\n","[Training Epoch 2] Batch 475, Loss 0.5180769562721252\n","[Training Epoch 2] Batch 476, Loss 0.5085729956626892\n","[Training Epoch 2] Batch 477, Loss 0.5080523490905762\n","[Training Epoch 2] Batch 478, Loss 0.5241476893424988\n","[Training Epoch 2] Batch 479, Loss 0.5265980958938599\n","[Training Epoch 2] Batch 480, Loss 0.493042528629303\n","[Training Epoch 2] Batch 481, Loss 0.5012844800949097\n","[Training Epoch 2] Batch 482, Loss 0.5114295482635498\n","[Training Epoch 2] Batch 483, Loss 0.5115358829498291\n","[Training Epoch 2] Batch 484, Loss 0.5160199403762817\n","[Training Epoch 2] Batch 485, Loss 0.4944566488265991\n","[Training Epoch 2] Batch 486, Loss 0.5068904757499695\n","[Training Epoch 2] Batch 487, Loss 0.49233031272888184\n","[Training Epoch 2] Batch 488, Loss 0.5067732334136963\n","[Training Epoch 2] Batch 489, Loss 0.5089436173439026\n","[Training Epoch 2] Batch 490, Loss 0.5076673626899719\n","[Training Epoch 2] Batch 491, Loss 0.5086055994033813\n","[Training Epoch 2] Batch 492, Loss 0.48915672302246094\n","[Training Epoch 2] Batch 493, Loss 0.5067615509033203\n","[Training Epoch 2] Batch 494, Loss 0.5095802545547485\n","[Training Epoch 2] Batch 495, Loss 0.47986894845962524\n","[Training Epoch 2] Batch 496, Loss 0.4991481900215149\n","[Training Epoch 2] Batch 497, Loss 0.5193687677383423\n","[Training Epoch 2] Batch 498, Loss 0.510297417640686\n","[Training Epoch 2] Batch 499, Loss 0.48823976516723633\n","[Training Epoch 2] Batch 500, Loss 0.5000761151313782\n","[Training Epoch 2] Batch 501, Loss 0.5048121213912964\n","[Training Epoch 2] Batch 502, Loss 0.508596658706665\n","[Training Epoch 2] Batch 503, Loss 0.5253639221191406\n","[Training Epoch 2] Batch 504, Loss 0.5060141086578369\n","[Training Epoch 2] Batch 505, Loss 0.5085339546203613\n","[Training Epoch 2] Batch 506, Loss 0.4967718720436096\n","[Training Epoch 2] Batch 507, Loss 0.4939661920070648\n","[Training Epoch 2] Batch 508, Loss 0.4890780448913574\n","[Training Epoch 2] Batch 509, Loss 0.5167944431304932\n","[Training Epoch 2] Batch 510, Loss 0.4868648648262024\n","[Training Epoch 2] Batch 511, Loss 0.5117073059082031\n","[Training Epoch 2] Batch 512, Loss 0.4940641224384308\n","[Training Epoch 2] Batch 513, Loss 0.4903111457824707\n","[Training Epoch 2] Batch 514, Loss 0.5022149085998535\n","[Training Epoch 2] Batch 515, Loss 0.5318872928619385\n","[Training Epoch 2] Batch 516, Loss 0.5120681524276733\n","[Training Epoch 2] Batch 517, Loss 0.47357457876205444\n","[Training Epoch 2] Batch 518, Loss 0.503731369972229\n","[Training Epoch 2] Batch 519, Loss 0.5198366641998291\n","[Training Epoch 2] Batch 520, Loss 0.5071185827255249\n","[Training Epoch 2] Batch 521, Loss 0.49764320254325867\n","[Training Epoch 2] Batch 522, Loss 0.5168875455856323\n","[Training Epoch 2] Batch 523, Loss 0.502450704574585\n","[Training Epoch 2] Batch 524, Loss 0.4767276644706726\n","[Training Epoch 2] Batch 525, Loss 0.4683179557323456\n","[Training Epoch 2] Batch 526, Loss 0.49646589159965515\n","[Training Epoch 2] Batch 527, Loss 0.5036317706108093\n","[Training Epoch 2] Batch 528, Loss 0.5205975770950317\n","[Training Epoch 2] Batch 529, Loss 0.5359346866607666\n","[Training Epoch 2] Batch 530, Loss 0.4937248229980469\n","[Training Epoch 2] Batch 531, Loss 0.5209285616874695\n","[Training Epoch 2] Batch 532, Loss 0.5134281516075134\n","[Training Epoch 2] Batch 533, Loss 0.5099895000457764\n","[Training Epoch 2] Batch 534, Loss 0.49170857667922974\n","[Training Epoch 2] Batch 535, Loss 0.49446576833724976\n","[Training Epoch 2] Batch 536, Loss 0.5038160085678101\n","[Training Epoch 2] Batch 537, Loss 0.48784464597702026\n","[Training Epoch 2] Batch 538, Loss 0.48773953318595886\n","[Training Epoch 2] Batch 539, Loss 0.5024638772010803\n","[Training Epoch 2] Batch 540, Loss 0.5401079654693604\n","[Training Epoch 2] Batch 541, Loss 0.49514469504356384\n","[Training Epoch 2] Batch 542, Loss 0.5085322856903076\n","[Training Epoch 2] Batch 543, Loss 0.498733252286911\n","[Training Epoch 2] Batch 544, Loss 0.5307170748710632\n","[Training Epoch 2] Batch 545, Loss 0.5239213705062866\n","[Training Epoch 2] Batch 546, Loss 0.5072592496871948\n","[Training Epoch 2] Batch 547, Loss 0.46357113122940063\n","[Training Epoch 2] Batch 548, Loss 0.5073812007904053\n","[Training Epoch 2] Batch 549, Loss 0.5061230063438416\n","[Training Epoch 2] Batch 550, Loss 0.47915664315223694\n","[Training Epoch 2] Batch 551, Loss 0.5124841332435608\n","[Training Epoch 2] Batch 552, Loss 0.5215457677841187\n","[Training Epoch 2] Batch 553, Loss 0.4986247420310974\n","[Training Epoch 2] Batch 554, Loss 0.49864304065704346\n","[Training Epoch 2] Batch 555, Loss 0.504945695400238\n","[Training Epoch 2] Batch 556, Loss 0.5095447301864624\n","[Training Epoch 2] Batch 557, Loss 0.5130696296691895\n","[Training Epoch 2] Batch 558, Loss 0.4971236288547516\n","[Training Epoch 2] Batch 559, Loss 0.5131278038024902\n","[Training Epoch 2] Batch 560, Loss 0.49008870124816895\n","[Training Epoch 2] Batch 561, Loss 0.5154334306716919\n","[Training Epoch 2] Batch 562, Loss 0.5045083165168762\n","[Training Epoch 2] Batch 563, Loss 0.47767508029937744\n","[Training Epoch 2] Batch 564, Loss 0.5035182237625122\n","[Training Epoch 2] Batch 565, Loss 0.5170729160308838\n","[Training Epoch 2] Batch 566, Loss 0.49343523383140564\n","[Training Epoch 2] Batch 567, Loss 0.49866941571235657\n","[Training Epoch 2] Batch 568, Loss 0.5178219079971313\n","[Training Epoch 2] Batch 569, Loss 0.4852828085422516\n","[Training Epoch 2] Batch 570, Loss 0.5131656527519226\n","[Training Epoch 2] Batch 571, Loss 0.4804335832595825\n","[Training Epoch 2] Batch 572, Loss 0.5326564311981201\n","[Training Epoch 2] Batch 573, Loss 0.48280757665634155\n","[Training Epoch 2] Batch 574, Loss 0.494861364364624\n","[Training Epoch 2] Batch 575, Loss 0.5264192819595337\n","[Training Epoch 2] Batch 576, Loss 0.4815414249897003\n","[Training Epoch 2] Batch 577, Loss 0.4972567558288574\n","[Training Epoch 2] Batch 578, Loss 0.52382892370224\n","[Training Epoch 2] Batch 579, Loss 0.5058720707893372\n","[Training Epoch 2] Batch 580, Loss 0.5034307241439819\n","[Training Epoch 2] Batch 581, Loss 0.5215699672698975\n","[Training Epoch 2] Batch 582, Loss 0.4971773624420166\n","[Training Epoch 2] Batch 583, Loss 0.49212783575057983\n","[Training Epoch 2] Batch 584, Loss 0.5218417048454285\n","[Training Epoch 2] Batch 585, Loss 0.4972365200519562\n","[Training Epoch 2] Batch 586, Loss 0.4887103736400604\n","[Training Epoch 2] Batch 587, Loss 0.47508659958839417\n","[Training Epoch 2] Batch 588, Loss 0.494681179523468\n","[Training Epoch 2] Batch 589, Loss 0.5031367540359497\n","[Training Epoch 2] Batch 590, Loss 0.4842745363712311\n","[Training Epoch 2] Batch 591, Loss 0.49481409788131714\n","[Training Epoch 2] Batch 592, Loss 0.4835778772830963\n","[Training Epoch 2] Batch 593, Loss 0.5204160213470459\n","[Training Epoch 2] Batch 594, Loss 0.5141603946685791\n","[Training Epoch 2] Batch 595, Loss 0.4874810576438904\n","[Training Epoch 2] Batch 596, Loss 0.5030624270439148\n","[Training Epoch 2] Batch 597, Loss 0.5218214392662048\n","[Training Epoch 2] Batch 598, Loss 0.49844443798065186\n","[Training Epoch 2] Batch 599, Loss 0.4777703881263733\n","[Training Epoch 2] Batch 600, Loss 0.4983479082584381\n","[Training Epoch 2] Batch 601, Loss 0.5057140588760376\n","[Training Epoch 2] Batch 602, Loss 0.48336437344551086\n","[Training Epoch 2] Batch 603, Loss 0.5132932662963867\n","[Training Epoch 2] Batch 604, Loss 0.5226342678070068\n","[Training Epoch 2] Batch 605, Loss 0.5266802906990051\n","[Training Epoch 2] Batch 606, Loss 0.48241227865219116\n","[Training Epoch 2] Batch 607, Loss 0.4810218811035156\n","[Training Epoch 2] Batch 608, Loss 0.5064153075218201\n","[Training Epoch 2] Batch 609, Loss 0.47605714201927185\n","[Training Epoch 2] Batch 610, Loss 0.5028490424156189\n","[Training Epoch 2] Batch 611, Loss 0.5299782156944275\n","[Training Epoch 2] Batch 612, Loss 0.5102392435073853\n","[Training Epoch 2] Batch 613, Loss 0.4774070978164673\n","[Training Epoch 2] Batch 614, Loss 0.4771050810813904\n","[Training Epoch 2] Batch 615, Loss 0.4983718991279602\n","[Training Epoch 2] Batch 616, Loss 0.5256656408309937\n","[Training Epoch 2] Batch 617, Loss 0.4737861156463623\n","[Training Epoch 2] Batch 618, Loss 0.5121954679489136\n","[Training Epoch 2] Batch 619, Loss 0.48414918780326843\n","[Training Epoch 2] Batch 620, Loss 0.482211172580719\n","[Training Epoch 2] Batch 621, Loss 0.5136885643005371\n","[Training Epoch 2] Batch 622, Loss 0.49942493438720703\n","[Training Epoch 2] Batch 623, Loss 0.5159385800361633\n","[Training Epoch 2] Batch 624, Loss 0.4933759868144989\n","[Training Epoch 2] Batch 625, Loss 0.5042580962181091\n","[Training Epoch 2] Batch 626, Loss 0.5320520401000977\n","[Training Epoch 2] Batch 627, Loss 0.4873116612434387\n","[Training Epoch 2] Batch 628, Loss 0.49205005168914795\n","[Training Epoch 2] Batch 629, Loss 0.5205912590026855\n","[Training Epoch 2] Batch 630, Loss 0.4830668270587921\n","[Training Epoch 2] Batch 631, Loss 0.4996057152748108\n","[Training Epoch 2] Batch 632, Loss 0.5016841888427734\n","[Training Epoch 2] Batch 633, Loss 0.4772951602935791\n","[Training Epoch 2] Batch 634, Loss 0.5159419775009155\n","[Training Epoch 2] Batch 635, Loss 0.4980391263961792\n","[Training Epoch 2] Batch 636, Loss 0.4982379674911499\n","[Training Epoch 2] Batch 637, Loss 0.47964268922805786\n","[Training Epoch 2] Batch 638, Loss 0.5041579604148865\n","[Training Epoch 2] Batch 639, Loss 0.5207439064979553\n","[Training Epoch 2] Batch 640, Loss 0.49179041385650635\n","[Training Epoch 2] Batch 641, Loss 0.49568992853164673\n","[Training Epoch 2] Batch 642, Loss 0.5040707588195801\n","[Training Epoch 2] Batch 643, Loss 0.49325740337371826\n","[Training Epoch 2] Batch 644, Loss 0.5083048939704895\n","[Training Epoch 2] Batch 645, Loss 0.5140500068664551\n","[Training Epoch 2] Batch 646, Loss 0.5028685927391052\n","[Training Epoch 2] Batch 647, Loss 0.5073422193527222\n","[Training Epoch 2] Batch 648, Loss 0.5043272972106934\n","[Training Epoch 2] Batch 649, Loss 0.513950526714325\n","[Training Epoch 2] Batch 650, Loss 0.5033339262008667\n","[Training Epoch 2] Batch 651, Loss 0.5168747305870056\n","[Training Epoch 2] Batch 652, Loss 0.5117358565330505\n","[Training Epoch 2] Batch 653, Loss 0.4871620535850525\n","[Training Epoch 2] Batch 654, Loss 0.4908340573310852\n","[Training Epoch 2] Batch 655, Loss 0.5266377329826355\n","[Training Epoch 2] Batch 656, Loss 0.49556154012680054\n","[Training Epoch 2] Batch 657, Loss 0.4707420766353607\n","[Training Epoch 2] Batch 658, Loss 0.5191737413406372\n","[Training Epoch 2] Batch 659, Loss 0.47459399700164795\n","[Training Epoch 2] Batch 660, Loss 0.5105614066123962\n","[Training Epoch 2] Batch 661, Loss 0.4995139241218567\n","[Training Epoch 2] Batch 662, Loss 0.49556246399879456\n","[Training Epoch 2] Batch 663, Loss 0.510629415512085\n","[Training Epoch 2] Batch 664, Loss 0.4966332018375397\n","[Training Epoch 2] Batch 665, Loss 0.4807429909706116\n","[Training Epoch 2] Batch 666, Loss 0.5016871690750122\n","[Training Epoch 2] Batch 667, Loss 0.4929272532463074\n","[Training Epoch 2] Batch 668, Loss 0.4993830919265747\n","[Training Epoch 2] Batch 669, Loss 0.5043637752532959\n","[Training Epoch 2] Batch 670, Loss 0.4868626594543457\n","[Training Epoch 2] Batch 671, Loss 0.479153573513031\n","[Training Epoch 2] Batch 672, Loss 0.5327101945877075\n","[Training Epoch 2] Batch 673, Loss 0.49536681175231934\n","[Training Epoch 2] Batch 674, Loss 0.47182971239089966\n","[Training Epoch 2] Batch 675, Loss 0.5178757905960083\n","[Training Epoch 2] Batch 676, Loss 0.47938644886016846\n","[Training Epoch 2] Batch 677, Loss 0.5215710401535034\n","[Training Epoch 2] Batch 678, Loss 0.475685179233551\n","[Training Epoch 2] Batch 679, Loss 0.480502724647522\n","[Training Epoch 2] Batch 680, Loss 0.49645382165908813\n","[Training Epoch 2] Batch 681, Loss 0.5339369773864746\n","[Training Epoch 2] Batch 682, Loss 0.49155953526496887\n","[Training Epoch 2] Batch 683, Loss 0.5240597128868103\n","[Training Epoch 2] Batch 684, Loss 0.4991723895072937\n","[Training Epoch 2] Batch 685, Loss 0.4842109978199005\n","[Training Epoch 2] Batch 686, Loss 0.4779377579689026\n","[Training Epoch 2] Batch 687, Loss 0.5116843581199646\n","[Training Epoch 2] Batch 688, Loss 0.48946553468704224\n","[Training Epoch 2] Batch 689, Loss 0.517990231513977\n","[Training Epoch 2] Batch 690, Loss 0.49140316247940063\n","[Training Epoch 2] Batch 691, Loss 0.49797719717025757\n","[Training Epoch 2] Batch 692, Loss 0.5026563405990601\n","[Training Epoch 2] Batch 693, Loss 0.468912810087204\n","[Training Epoch 2] Batch 694, Loss 0.49717193841934204\n","[Training Epoch 2] Batch 695, Loss 0.4851727783679962\n","[Training Epoch 2] Batch 696, Loss 0.4892340898513794\n","[Training Epoch 2] Batch 697, Loss 0.5351011753082275\n","[Training Epoch 2] Batch 698, Loss 0.4962695240974426\n","[Training Epoch 2] Batch 699, Loss 0.48168641328811646\n","[Training Epoch 2] Batch 700, Loss 0.4929544925689697\n","[Training Epoch 2] Batch 701, Loss 0.509303092956543\n","[Training Epoch 2] Batch 702, Loss 0.49650490283966064\n","[Training Epoch 2] Batch 703, Loss 0.4917687177658081\n","[Training Epoch 2] Batch 704, Loss 0.5128423571586609\n","[Training Epoch 2] Batch 705, Loss 0.5202579498291016\n","[Training Epoch 2] Batch 706, Loss 0.525321364402771\n","[Training Epoch 2] Batch 707, Loss 0.4814560115337372\n","[Training Epoch 2] Batch 708, Loss 0.5055123567581177\n","[Training Epoch 2] Batch 709, Loss 0.49809885025024414\n","[Training Epoch 2] Batch 710, Loss 0.5117291212081909\n","[Training Epoch 2] Batch 711, Loss 0.4998632073402405\n","[Training Epoch 2] Batch 712, Loss 0.5239962339401245\n","[Training Epoch 2] Batch 713, Loss 0.5250160694122314\n","[Training Epoch 2] Batch 714, Loss 0.5140517950057983\n","[Training Epoch 2] Batch 715, Loss 0.5001780390739441\n","[Training Epoch 2] Batch 716, Loss 0.5016314387321472\n","[Training Epoch 2] Batch 717, Loss 0.5104764699935913\n","[Training Epoch 2] Batch 718, Loss 0.4831172227859497\n","[Training Epoch 2] Batch 719, Loss 0.4593932032585144\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 2] HR = 0.0615, NDCG = 0.0323\n","Epoch 3 starts !\n","--------------------------------------------------------------------------------\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/data.py:150: DeprecationWarning: Sampling from a set deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n","[Training Epoch 3] Batch 0, Loss 0.5189614295959473\n","[Training Epoch 3] Batch 1, Loss 0.49416136741638184\n","[Training Epoch 3] Batch 2, Loss 0.4867296814918518\n","[Training Epoch 3] Batch 3, Loss 0.511647641658783\n","[Training Epoch 3] Batch 4, Loss 0.5051873922348022\n","[Training Epoch 3] Batch 5, Loss 0.49140697717666626\n","[Training Epoch 3] Batch 6, Loss 0.5299216508865356\n","[Training Epoch 3] Batch 7, Loss 0.5516312122344971\n","[Training Epoch 3] Batch 8, Loss 0.5039336681365967\n","[Training Epoch 3] Batch 9, Loss 0.5264641046524048\n","[Training Epoch 3] Batch 10, Loss 0.4813110828399658\n","[Training Epoch 3] Batch 11, Loss 0.5163592100143433\n","[Training Epoch 3] Batch 12, Loss 0.4890245497226715\n","[Training Epoch 3] Batch 13, Loss 0.5075756311416626\n","[Training Epoch 3] Batch 14, Loss 0.48024991154670715\n","[Training Epoch 3] Batch 15, Loss 0.48611050844192505\n","[Training Epoch 3] Batch 16, Loss 0.47920435667037964\n","[Training Epoch 3] Batch 17, Loss 0.5178516507148743\n","[Training Epoch 3] Batch 18, Loss 0.5084319710731506\n","[Training Epoch 3] Batch 19, Loss 0.511831521987915\n","[Training Epoch 3] Batch 20, Loss 0.5027323961257935\n","[Training Epoch 3] Batch 21, Loss 0.4865220785140991\n","[Training Epoch 3] Batch 22, Loss 0.5073192119598389\n","[Training Epoch 3] Batch 23, Loss 0.4995754361152649\n","[Training Epoch 3] Batch 24, Loss 0.5390512943267822\n","[Training Epoch 3] Batch 25, Loss 0.48415035009384155\n","[Training Epoch 3] Batch 26, Loss 0.463620126247406\n","[Training Epoch 3] Batch 27, Loss 0.5052274465560913\n","[Training Epoch 3] Batch 28, Loss 0.4990296959877014\n","[Training Epoch 3] Batch 29, Loss 0.5379163026809692\n","[Training Epoch 3] Batch 30, Loss 0.5010742545127869\n","[Training Epoch 3] Batch 31, Loss 0.4885319471359253\n","[Training Epoch 3] Batch 32, Loss 0.4876285791397095\n","[Training Epoch 3] Batch 33, Loss 0.49124616384506226\n","[Training Epoch 3] Batch 34, Loss 0.4988226890563965\n","[Training Epoch 3] Batch 35, Loss 0.5076488852500916\n","[Training Epoch 3] Batch 36, Loss 0.4863457679748535\n","[Training Epoch 3] Batch 37, Loss 0.5075826048851013\n","[Training Epoch 3] Batch 38, Loss 0.4852589964866638\n","[Training Epoch 3] Batch 39, Loss 0.5252110958099365\n","[Training Epoch 3] Batch 40, Loss 0.5038533210754395\n","[Training Epoch 3] Batch 41, Loss 0.4886372983455658\n","[Training Epoch 3] Batch 42, Loss 0.5025749206542969\n","[Training Epoch 3] Batch 43, Loss 0.5125844478607178\n","[Training Epoch 3] Batch 44, Loss 0.5198216438293457\n","[Training Epoch 3] Batch 45, Loss 0.5015293955802917\n","[Training Epoch 3] Batch 46, Loss 0.49910661578178406\n","[Training Epoch 3] Batch 47, Loss 0.4903709888458252\n","[Training Epoch 3] Batch 48, Loss 0.46591460704803467\n","[Training Epoch 3] Batch 49, Loss 0.49763110280036926\n","[Training Epoch 3] Batch 50, Loss 0.5228548645973206\n","[Training Epoch 3] Batch 51, Loss 0.49786126613616943\n","[Training Epoch 3] Batch 52, Loss 0.5375091433525085\n","[Training Epoch 3] Batch 53, Loss 0.5091480016708374\n","[Training Epoch 3] Batch 54, Loss 0.5091148614883423\n","[Training Epoch 3] Batch 55, Loss 0.5054038763046265\n","[Training Epoch 3] Batch 56, Loss 0.507418155670166\n","[Training Epoch 3] Batch 57, Loss 0.5090561509132385\n","[Training Epoch 3] Batch 58, Loss 0.5115305781364441\n","[Training Epoch 3] Batch 59, Loss 0.49124157428741455\n","[Training Epoch 3] Batch 60, Loss 0.5101594924926758\n","[Training Epoch 3] Batch 61, Loss 0.4954295754432678\n","[Training Epoch 3] Batch 62, Loss 0.5282354354858398\n","[Training Epoch 3] Batch 63, Loss 0.5026267170906067\n","[Training Epoch 3] Batch 64, Loss 0.4999608099460602\n","[Training Epoch 3] Batch 65, Loss 0.538468599319458\n","[Training Epoch 3] Batch 66, Loss 0.486037939786911\n","[Training Epoch 3] Batch 67, Loss 0.4758530259132385\n","[Training Epoch 3] Batch 68, Loss 0.5002710819244385\n","[Training Epoch 3] Batch 69, Loss 0.4865124225616455\n","[Training Epoch 3] Batch 70, Loss 0.48599231243133545\n","[Training Epoch 3] Batch 71, Loss 0.5090292096138\n","[Training Epoch 3] Batch 72, Loss 0.47255799174308777\n","[Training Epoch 3] Batch 73, Loss 0.4998047351837158\n","[Training Epoch 3] Batch 74, Loss 0.49197232723236084\n","[Training Epoch 3] Batch 75, Loss 0.5063951015472412\n","[Training Epoch 3] Batch 76, Loss 0.4860478341579437\n","[Training Epoch 3] Batch 77, Loss 0.49907273054122925\n","[Training Epoch 3] Batch 78, Loss 0.44700339436531067\n","[Training Epoch 3] Batch 79, Loss 0.5139958262443542\n","[Training Epoch 3] Batch 80, Loss 0.5053555965423584\n","[Training Epoch 3] Batch 81, Loss 0.521298348903656\n","[Training Epoch 3] Batch 82, Loss 0.48588836193084717\n","[Training Epoch 3] Batch 83, Loss 0.4734019637107849\n","[Training Epoch 3] Batch 84, Loss 0.49006038904190063\n","[Training Epoch 3] Batch 85, Loss 0.5232124328613281\n","[Training Epoch 3] Batch 86, Loss 0.5062091946601868\n","[Training Epoch 3] Batch 87, Loss 0.4990408420562744\n","[Training Epoch 3] Batch 88, Loss 0.5000007748603821\n","[Training Epoch 3] Batch 89, Loss 0.5314337611198425\n","[Training Epoch 3] Batch 90, Loss 0.4983680248260498\n","[Training Epoch 3] Batch 91, Loss 0.5141948461532593\n","[Training Epoch 3] Batch 92, Loss 0.49828892946243286\n","[Training Epoch 3] Batch 93, Loss 0.531008780002594\n","[Training Epoch 3] Batch 94, Loss 0.5002025365829468\n","[Training Epoch 3] Batch 95, Loss 0.5036166310310364\n","[Training Epoch 3] Batch 96, Loss 0.49742352962493896\n","[Training Epoch 3] Batch 97, Loss 0.502914547920227\n","[Training Epoch 3] Batch 98, Loss 0.5113325715065002\n","[Training Epoch 3] Batch 99, Loss 0.5050459504127502\n","[Training Epoch 3] Batch 100, Loss 0.486507773399353\n","[Training Epoch 3] Batch 101, Loss 0.5080975294113159\n","[Training Epoch 3] Batch 102, Loss 0.5232999324798584\n","[Training Epoch 3] Batch 103, Loss 0.5025818347930908\n","[Training Epoch 3] Batch 104, Loss 0.47981810569763184\n","[Training Epoch 3] Batch 105, Loss 0.5328390598297119\n","[Training Epoch 3] Batch 106, Loss 0.5054075717926025\n","[Training Epoch 3] Batch 107, Loss 0.5241507291793823\n","[Training Epoch 3] Batch 108, Loss 0.4797457754611969\n","[Training Epoch 3] Batch 109, Loss 0.4963512420654297\n","[Training Epoch 3] Batch 110, Loss 0.4875161945819855\n","[Training Epoch 3] Batch 111, Loss 0.4911803901195526\n","[Training Epoch 3] Batch 112, Loss 0.4720388650894165\n","[Training Epoch 3] Batch 113, Loss 0.4912763237953186\n","[Training Epoch 3] Batch 114, Loss 0.5216208696365356\n","[Training Epoch 3] Batch 115, Loss 0.5021406412124634\n","[Training Epoch 3] Batch 116, Loss 0.5098917484283447\n","[Training Epoch 3] Batch 117, Loss 0.4745393991470337\n","[Training Epoch 3] Batch 118, Loss 0.5051857829093933\n","[Training Epoch 3] Batch 119, Loss 0.5127302408218384\n","[Training Epoch 3] Batch 120, Loss 0.47175705432891846\n","[Training Epoch 3] Batch 121, Loss 0.5008596777915955\n","[Training Epoch 3] Batch 122, Loss 0.5469335913658142\n","[Training Epoch 3] Batch 123, Loss 0.5163573026657104\n","[Training Epoch 3] Batch 124, Loss 0.5027120113372803\n","[Training Epoch 3] Batch 125, Loss 0.5077359676361084\n","[Training Epoch 3] Batch 126, Loss 0.5137882828712463\n","[Training Epoch 3] Batch 127, Loss 0.5102069973945618\n","[Training Epoch 3] Batch 128, Loss 0.4863358438014984\n","[Training Epoch 3] Batch 129, Loss 0.5063008069992065\n","[Training Epoch 3] Batch 130, Loss 0.5227493643760681\n","[Training Epoch 3] Batch 131, Loss 0.469063401222229\n","[Training Epoch 3] Batch 132, Loss 0.5072520971298218\n","[Training Epoch 3] Batch 133, Loss 0.5151498317718506\n","[Training Epoch 3] Batch 134, Loss 0.5163392424583435\n","[Training Epoch 3] Batch 135, Loss 0.5266464948654175\n","[Training Epoch 3] Batch 136, Loss 0.5024292469024658\n","[Training Epoch 3] Batch 137, Loss 0.5073611736297607\n","[Training Epoch 3] Batch 138, Loss 0.5168021321296692\n","[Training Epoch 3] Batch 139, Loss 0.5254746675491333\n","[Training Epoch 3] Batch 140, Loss 0.48200562596321106\n","[Training Epoch 3] Batch 141, Loss 0.47713255882263184\n","[Training Epoch 3] Batch 142, Loss 0.512776255607605\n","[Training Epoch 3] Batch 143, Loss 0.5117594599723816\n","[Training Epoch 3] Batch 144, Loss 0.4809040427207947\n","[Training Epoch 3] Batch 145, Loss 0.48084375262260437\n","[Training Epoch 3] Batch 146, Loss 0.531558632850647\n","[Training Epoch 3] Batch 147, Loss 0.4985928237438202\n","[Training Epoch 3] Batch 148, Loss 0.4807153344154358\n","[Training Epoch 3] Batch 149, Loss 0.5075300335884094\n","[Training Epoch 3] Batch 150, Loss 0.5033557415008545\n","[Training Epoch 3] Batch 151, Loss 0.4998355507850647\n","[Training Epoch 3] Batch 152, Loss 0.5128120183944702\n","[Training Epoch 3] Batch 153, Loss 0.4871126413345337\n","[Training Epoch 3] Batch 154, Loss 0.507403552532196\n","[Training Epoch 3] Batch 155, Loss 0.5126535892486572\n","[Training Epoch 3] Batch 156, Loss 0.5149177312850952\n","[Training Epoch 3] Batch 157, Loss 0.48719075322151184\n","[Training Epoch 3] Batch 158, Loss 0.5169249773025513\n","[Training Epoch 3] Batch 159, Loss 0.5037705898284912\n","[Training Epoch 3] Batch 160, Loss 0.5141801834106445\n","[Training Epoch 3] Batch 161, Loss 0.5045623779296875\n","[Training Epoch 3] Batch 162, Loss 0.4883667528629303\n","[Training Epoch 3] Batch 163, Loss 0.5009983777999878\n","[Training Epoch 3] Batch 164, Loss 0.4900209307670593\n","[Training Epoch 3] Batch 165, Loss 0.51540207862854\n","[Training Epoch 3] Batch 166, Loss 0.503190279006958\n","[Training Epoch 3] Batch 167, Loss 0.48192715644836426\n","[Training Epoch 3] Batch 168, Loss 0.5218076109886169\n","[Training Epoch 3] Batch 169, Loss 0.47321566939353943\n","[Training Epoch 3] Batch 170, Loss 0.49604108929634094\n","[Training Epoch 3] Batch 171, Loss 0.4935287535190582\n","[Training Epoch 3] Batch 172, Loss 0.4910673201084137\n","[Training Epoch 3] Batch 173, Loss 0.5076019167900085\n","[Training Epoch 3] Batch 174, Loss 0.5101237297058105\n","[Training Epoch 3] Batch 175, Loss 0.5252773761749268\n","[Training Epoch 3] Batch 176, Loss 0.5079686641693115\n","[Training Epoch 3] Batch 177, Loss 0.506109356880188\n","[Training Epoch 3] Batch 178, Loss 0.46874380111694336\n","[Training Epoch 3] Batch 179, Loss 0.523928165435791\n","[Training Epoch 3] Batch 180, Loss 0.5177652835845947\n","[Training Epoch 3] Batch 181, Loss 0.4689699411392212\n","[Training Epoch 3] Batch 182, Loss 0.5024246573448181\n","[Training Epoch 3] Batch 183, Loss 0.46788668632507324\n","[Training Epoch 3] Batch 184, Loss 0.47627437114715576\n","[Training Epoch 3] Batch 185, Loss 0.46656280755996704\n","[Training Epoch 3] Batch 186, Loss 0.4987417459487915\n","[Training Epoch 3] Batch 187, Loss 0.4871295094490051\n","[Training Epoch 3] Batch 188, Loss 0.5035781264305115\n","[Training Epoch 3] Batch 189, Loss 0.5014588236808777\n","[Training Epoch 3] Batch 190, Loss 0.5027965903282166\n","[Training Epoch 3] Batch 191, Loss 0.5037407279014587\n","[Training Epoch 3] Batch 192, Loss 0.5138450860977173\n","[Training Epoch 3] Batch 193, Loss 0.5049371719360352\n","[Training Epoch 3] Batch 194, Loss 0.5025599002838135\n","[Training Epoch 3] Batch 195, Loss 0.4727932810783386\n","[Training Epoch 3] Batch 196, Loss 0.506496787071228\n","[Training Epoch 3] Batch 197, Loss 0.5038988590240479\n","[Training Epoch 3] Batch 198, Loss 0.5303797721862793\n","[Training Epoch 3] Batch 199, Loss 0.4860002100467682\n","[Training Epoch 3] Batch 200, Loss 0.49960267543792725\n","[Training Epoch 3] Batch 201, Loss 0.5037875771522522\n","[Training Epoch 3] Batch 202, Loss 0.512578010559082\n","[Training Epoch 3] Batch 203, Loss 0.4840810298919678\n","[Training Epoch 3] Batch 204, Loss 0.5240116119384766\n","[Training Epoch 3] Batch 205, Loss 0.5410234928131104\n","[Training Epoch 3] Batch 206, Loss 0.5151761770248413\n","[Training Epoch 3] Batch 207, Loss 0.51356041431427\n","[Training Epoch 3] Batch 208, Loss 0.5034497976303101\n","[Training Epoch 3] Batch 209, Loss 0.5139634609222412\n","[Training Epoch 3] Batch 210, Loss 0.4903862476348877\n","[Training Epoch 3] Batch 211, Loss 0.49175894260406494\n","[Training Epoch 3] Batch 212, Loss 0.5009994506835938\n","[Training Epoch 3] Batch 213, Loss 0.49768224358558655\n","[Training Epoch 3] Batch 214, Loss 0.49367767572402954\n","[Training Epoch 3] Batch 215, Loss 0.4924786686897278\n","[Training Epoch 3] Batch 216, Loss 0.5100366473197937\n","[Training Epoch 3] Batch 217, Loss 0.4764476716518402\n","[Training Epoch 3] Batch 218, Loss 0.5097247362136841\n","[Training Epoch 3] Batch 219, Loss 0.5136129260063171\n","[Training Epoch 3] Batch 220, Loss 0.5143314599990845\n","[Training Epoch 3] Batch 221, Loss 0.49194133281707764\n","[Training Epoch 3] Batch 222, Loss 0.5063551664352417\n","[Training Epoch 3] Batch 223, Loss 0.510378360748291\n","[Training Epoch 3] Batch 224, Loss 0.4871940612792969\n","[Training Epoch 3] Batch 225, Loss 0.5150705575942993\n","[Training Epoch 3] Batch 226, Loss 0.47218841314315796\n","[Training Epoch 3] Batch 227, Loss 0.5181109309196472\n","[Training Epoch 3] Batch 228, Loss 0.5010312795639038\n","[Training Epoch 3] Batch 229, Loss 0.4882691502571106\n","[Training Epoch 3] Batch 230, Loss 0.5099445581436157\n","[Training Epoch 3] Batch 231, Loss 0.48174744844436646\n","[Training Epoch 3] Batch 232, Loss 0.5021686553955078\n","[Training Epoch 3] Batch 233, Loss 0.5181448459625244\n","[Training Epoch 3] Batch 234, Loss 0.5110073685646057\n","[Training Epoch 3] Batch 235, Loss 0.48651403188705444\n","[Training Epoch 3] Batch 236, Loss 0.5046851634979248\n","[Training Epoch 3] Batch 237, Loss 0.5009853839874268\n","[Training Epoch 3] Batch 238, Loss 0.5040159225463867\n","[Training Epoch 3] Batch 239, Loss 0.49819907546043396\n","[Training Epoch 3] Batch 240, Loss 0.5150419473648071\n","[Training Epoch 3] Batch 241, Loss 0.516182005405426\n","[Training Epoch 3] Batch 242, Loss 0.5051642060279846\n","[Training Epoch 3] Batch 243, Loss 0.48789742588996887\n","[Training Epoch 3] Batch 244, Loss 0.4738810360431671\n","[Training Epoch 3] Batch 245, Loss 0.4955756366252899\n","[Training Epoch 3] Batch 246, Loss 0.4994108974933624\n","[Training Epoch 3] Batch 247, Loss 0.5063979625701904\n","[Training Epoch 3] Batch 248, Loss 0.49354201555252075\n","[Training Epoch 3] Batch 249, Loss 0.5048904418945312\n","[Training Epoch 3] Batch 250, Loss 0.5177764892578125\n","[Training Epoch 3] Batch 251, Loss 0.49051859974861145\n","[Training Epoch 3] Batch 252, Loss 0.5022687315940857\n","[Training Epoch 3] Batch 253, Loss 0.504607081413269\n","[Training Epoch 3] Batch 254, Loss 0.495675653219223\n","[Training Epoch 3] Batch 255, Loss 0.48330000042915344\n","[Training Epoch 3] Batch 256, Loss 0.49689358472824097\n","[Training Epoch 3] Batch 257, Loss 0.49179500341415405\n","[Training Epoch 3] Batch 258, Loss 0.49863240122795105\n","[Training Epoch 3] Batch 259, Loss 0.5034052729606628\n","[Training Epoch 3] Batch 260, Loss 0.5073686838150024\n","[Training Epoch 3] Batch 261, Loss 0.4912879467010498\n","[Training Epoch 3] Batch 262, Loss 0.47382134199142456\n","[Training Epoch 3] Batch 263, Loss 0.5093362331390381\n","[Training Epoch 3] Batch 264, Loss 0.49165210127830505\n","[Training Epoch 3] Batch 265, Loss 0.46000605821609497\n","[Training Epoch 3] Batch 266, Loss 0.5232619643211365\n","[Training Epoch 3] Batch 267, Loss 0.4905090928077698\n","[Training Epoch 3] Batch 268, Loss 0.4938415288925171\n","[Training Epoch 3] Batch 269, Loss 0.5061290264129639\n","[Training Epoch 3] Batch 270, Loss 0.5187431573867798\n","[Training Epoch 3] Batch 271, Loss 0.5016481876373291\n","[Training Epoch 3] Batch 272, Loss 0.5186090469360352\n","[Training Epoch 3] Batch 273, Loss 0.504899799823761\n","[Training Epoch 3] Batch 274, Loss 0.4880305230617523\n","[Training Epoch 3] Batch 275, Loss 0.47147729992866516\n","[Training Epoch 3] Batch 276, Loss 0.506793200969696\n","[Training Epoch 3] Batch 277, Loss 0.5201208591461182\n","[Training Epoch 3] Batch 278, Loss 0.5074262619018555\n","[Training Epoch 3] Batch 279, Loss 0.4974932372570038\n","[Training Epoch 3] Batch 280, Loss 0.5051072835922241\n","[Training Epoch 3] Batch 281, Loss 0.5058689117431641\n","[Training Epoch 3] Batch 282, Loss 0.4868725836277008\n","[Training Epoch 3] Batch 283, Loss 0.549433708190918\n","[Training Epoch 3] Batch 284, Loss 0.47736096382141113\n","[Training Epoch 3] Batch 285, Loss 0.48714154958724976\n","[Training Epoch 3] Batch 286, Loss 0.5587782263755798\n","[Training Epoch 3] Batch 287, Loss 0.5202353000640869\n","[Training Epoch 3] Batch 288, Loss 0.5380450487136841\n","[Training Epoch 3] Batch 289, Loss 0.525359570980072\n","[Training Epoch 3] Batch 290, Loss 0.48223698139190674\n","[Training Epoch 3] Batch 291, Loss 0.495443195104599\n","[Training Epoch 3] Batch 292, Loss 0.5138825178146362\n","[Training Epoch 3] Batch 293, Loss 0.4892038106918335\n","[Training Epoch 3] Batch 294, Loss 0.49728327989578247\n","[Training Epoch 3] Batch 295, Loss 0.487917959690094\n","[Training Epoch 3] Batch 296, Loss 0.525198221206665\n","[Training Epoch 3] Batch 297, Loss 0.5124951601028442\n","[Training Epoch 3] Batch 298, Loss 0.5168114900588989\n","[Training Epoch 3] Batch 299, Loss 0.4996994137763977\n","[Training Epoch 3] Batch 300, Loss 0.4947955012321472\n","[Training Epoch 3] Batch 301, Loss 0.5378071665763855\n","[Training Epoch 3] Batch 302, Loss 0.5332182049751282\n","[Training Epoch 3] Batch 303, Loss 0.487792432308197\n","[Training Epoch 3] Batch 304, Loss 0.5018109083175659\n","[Training Epoch 3] Batch 305, Loss 0.4945077896118164\n","[Training Epoch 3] Batch 306, Loss 0.49297016859054565\n","[Training Epoch 3] Batch 307, Loss 0.5074203610420227\n","[Training Epoch 3] Batch 308, Loss 0.4825855791568756\n","[Training Epoch 3] Batch 309, Loss 0.48691558837890625\n","[Training Epoch 3] Batch 310, Loss 0.48562079668045044\n","[Training Epoch 3] Batch 311, Loss 0.48527488112449646\n","[Training Epoch 3] Batch 312, Loss 0.49433618783950806\n","[Training Epoch 3] Batch 313, Loss 0.49480146169662476\n","[Training Epoch 3] Batch 314, Loss 0.4803552031517029\n","[Training Epoch 3] Batch 315, Loss 0.46350711584091187\n","[Training Epoch 3] Batch 316, Loss 0.4762950837612152\n","[Training Epoch 3] Batch 317, Loss 0.5175614953041077\n","[Training Epoch 3] Batch 318, Loss 0.4741930067539215\n","[Training Epoch 3] Batch 319, Loss 0.5351077318191528\n","[Training Epoch 3] Batch 320, Loss 0.4865986108779907\n","[Training Epoch 3] Batch 321, Loss 0.5070081949234009\n","[Training Epoch 3] Batch 322, Loss 0.484161913394928\n","[Training Epoch 3] Batch 323, Loss 0.49296897649765015\n","[Training Epoch 3] Batch 324, Loss 0.5123318433761597\n","[Training Epoch 3] Batch 325, Loss 0.5121070146560669\n","[Training Epoch 3] Batch 326, Loss 0.4986761212348938\n","[Training Epoch 3] Batch 327, Loss 0.5322205424308777\n","[Training Epoch 3] Batch 328, Loss 0.517082154750824\n","[Training Epoch 3] Batch 329, Loss 0.4777536392211914\n","[Training Epoch 3] Batch 330, Loss 0.4711432456970215\n","[Training Epoch 3] Batch 331, Loss 0.5360188484191895\n","[Training Epoch 3] Batch 332, Loss 0.5000314116477966\n","[Training Epoch 3] Batch 333, Loss 0.4868287444114685\n","[Training Epoch 3] Batch 334, Loss 0.5147451758384705\n","[Training Epoch 3] Batch 335, Loss 0.5006779432296753\n","[Training Epoch 3] Batch 336, Loss 0.49053311347961426\n","[Training Epoch 3] Batch 337, Loss 0.49502435326576233\n","[Training Epoch 3] Batch 338, Loss 0.49054786562919617\n","[Training Epoch 3] Batch 339, Loss 0.49314895272254944\n","[Training Epoch 3] Batch 340, Loss 0.5112205743789673\n","[Training Epoch 3] Batch 341, Loss 0.5076154470443726\n","[Training Epoch 3] Batch 342, Loss 0.5061949491500854\n","[Training Epoch 3] Batch 343, Loss 0.5219659805297852\n","[Training Epoch 3] Batch 344, Loss 0.5018426179885864\n","[Training Epoch 3] Batch 345, Loss 0.512505829334259\n","[Training Epoch 3] Batch 346, Loss 0.5135835409164429\n","[Training Epoch 3] Batch 347, Loss 0.506294846534729\n","[Training Epoch 3] Batch 348, Loss 0.4981642961502075\n","[Training Epoch 3] Batch 349, Loss 0.4903290867805481\n","[Training Epoch 3] Batch 350, Loss 0.4994579553604126\n","[Training Epoch 3] Batch 351, Loss 0.4746335744857788\n","[Training Epoch 3] Batch 352, Loss 0.5034801363945007\n","[Training Epoch 3] Batch 353, Loss 0.5019926428794861\n","[Training Epoch 3] Batch 354, Loss 0.5008884072303772\n","[Training Epoch 3] Batch 355, Loss 0.5087483525276184\n","[Training Epoch 3] Batch 356, Loss 0.5190053582191467\n","[Training Epoch 3] Batch 357, Loss 0.47890704870224\n","[Training Epoch 3] Batch 358, Loss 0.4752991795539856\n","[Training Epoch 3] Batch 359, Loss 0.5267114639282227\n","[Training Epoch 3] Batch 360, Loss 0.5063684582710266\n","[Training Epoch 3] Batch 361, Loss 0.5277315378189087\n","[Training Epoch 3] Batch 362, Loss 0.5240116119384766\n","[Training Epoch 3] Batch 363, Loss 0.5165184140205383\n","[Training Epoch 3] Batch 364, Loss 0.4875388443470001\n","[Training Epoch 3] Batch 365, Loss 0.48794475197792053\n","[Training Epoch 3] Batch 366, Loss 0.5008975267410278\n","[Training Epoch 3] Batch 367, Loss 0.5100871324539185\n","[Training Epoch 3] Batch 368, Loss 0.49061843752861023\n","[Training Epoch 3] Batch 369, Loss 0.48808830976486206\n","[Training Epoch 3] Batch 370, Loss 0.5016274452209473\n","[Training Epoch 3] Batch 371, Loss 0.4996618926525116\n","[Training Epoch 3] Batch 372, Loss 0.49217307567596436\n","[Training Epoch 3] Batch 373, Loss 0.5095171928405762\n","[Training Epoch 3] Batch 374, Loss 0.5192692875862122\n","[Training Epoch 3] Batch 375, Loss 0.503416895866394\n","[Training Epoch 3] Batch 376, Loss 0.521704375743866\n","[Training Epoch 3] Batch 377, Loss 0.5071215629577637\n","[Training Epoch 3] Batch 378, Loss 0.5087956190109253\n","[Training Epoch 3] Batch 379, Loss 0.482735812664032\n","[Training Epoch 3] Batch 380, Loss 0.4750131666660309\n","[Training Epoch 3] Batch 381, Loss 0.52320396900177\n","[Training Epoch 3] Batch 382, Loss 0.5085751414299011\n","[Training Epoch 3] Batch 383, Loss 0.5004327297210693\n","[Training Epoch 3] Batch 384, Loss 0.49080127477645874\n","[Training Epoch 3] Batch 385, Loss 0.4968741238117218\n","[Training Epoch 3] Batch 386, Loss 0.47883301973342896\n","[Training Epoch 3] Batch 387, Loss 0.5119526386260986\n","[Training Epoch 3] Batch 388, Loss 0.47191131114959717\n","[Training Epoch 3] Batch 389, Loss 0.4793221950531006\n","[Training Epoch 3] Batch 390, Loss 0.5030254125595093\n","[Training Epoch 3] Batch 391, Loss 0.5009799599647522\n","[Training Epoch 3] Batch 392, Loss 0.5241421461105347\n","[Training Epoch 3] Batch 393, Loss 0.506336510181427\n","[Training Epoch 3] Batch 394, Loss 0.5010449886322021\n","[Training Epoch 3] Batch 395, Loss 0.5214487910270691\n","[Training Epoch 3] Batch 396, Loss 0.5178377032279968\n","[Training Epoch 3] Batch 397, Loss 0.499085009098053\n","[Training Epoch 3] Batch 398, Loss 0.499911367893219\n","[Training Epoch 3] Batch 399, Loss 0.5017855763435364\n","[Training Epoch 3] Batch 400, Loss 0.49154505133628845\n","[Training Epoch 3] Batch 401, Loss 0.5312933921813965\n","[Training Epoch 3] Batch 402, Loss 0.4955276548862457\n","[Training Epoch 3] Batch 403, Loss 0.5153308510780334\n","[Training Epoch 3] Batch 404, Loss 0.5386648178100586\n","[Training Epoch 3] Batch 405, Loss 0.521603524684906\n","[Training Epoch 3] Batch 406, Loss 0.4902098774909973\n","[Training Epoch 3] Batch 407, Loss 0.4892149567604065\n","[Training Epoch 3] Batch 408, Loss 0.5006287693977356\n","[Training Epoch 3] Batch 409, Loss 0.4902969002723694\n","[Training Epoch 3] Batch 410, Loss 0.5242583751678467\n","[Training Epoch 3] Batch 411, Loss 0.5009964108467102\n","[Training Epoch 3] Batch 412, Loss 0.5306624174118042\n","[Training Epoch 3] Batch 413, Loss 0.49966979026794434\n","[Training Epoch 3] Batch 414, Loss 0.5085299611091614\n","[Training Epoch 3] Batch 415, Loss 0.5294721126556396\n","[Training Epoch 3] Batch 416, Loss 0.5104051828384399\n","[Training Epoch 3] Batch 417, Loss 0.5267772078514099\n","[Training Epoch 3] Batch 418, Loss 0.49178117513656616\n","[Training Epoch 3] Batch 419, Loss 0.5224905014038086\n","[Training Epoch 3] Batch 420, Loss 0.48787879943847656\n","[Training Epoch 3] Batch 421, Loss 0.48633047938346863\n","[Training Epoch 3] Batch 422, Loss 0.5263314247131348\n","[Training Epoch 3] Batch 423, Loss 0.48664820194244385\n","[Training Epoch 3] Batch 424, Loss 0.4863365590572357\n","[Training Epoch 3] Batch 425, Loss 0.5326023697853088\n","[Training Epoch 3] Batch 426, Loss 0.48996633291244507\n","[Training Epoch 3] Batch 427, Loss 0.46459466218948364\n","[Training Epoch 3] Batch 428, Loss 0.4943658709526062\n","[Training Epoch 3] Batch 429, Loss 0.4876839518547058\n","[Training Epoch 3] Batch 430, Loss 0.4745403528213501\n","[Training Epoch 3] Batch 431, Loss 0.5059105753898621\n","[Training Epoch 3] Batch 432, Loss 0.499332070350647\n","[Training Epoch 3] Batch 433, Loss 0.5182900428771973\n","[Training Epoch 3] Batch 434, Loss 0.49305260181427\n","[Training Epoch 3] Batch 435, Loss 0.5141496062278748\n","[Training Epoch 3] Batch 436, Loss 0.49562689661979675\n","[Training Epoch 3] Batch 437, Loss 0.4954158067703247\n","[Training Epoch 3] Batch 438, Loss 0.4914184808731079\n","[Training Epoch 3] Batch 439, Loss 0.49169957637786865\n","[Training Epoch 3] Batch 440, Loss 0.4849070608615875\n","[Training Epoch 3] Batch 441, Loss 0.4902360141277313\n","[Training Epoch 3] Batch 442, Loss 0.5055904388427734\n","[Training Epoch 3] Batch 443, Loss 0.49959662556648254\n","[Training Epoch 3] Batch 444, Loss 0.496686190366745\n","[Training Epoch 3] Batch 445, Loss 0.5154263973236084\n","[Training Epoch 3] Batch 446, Loss 0.49114885926246643\n","[Training Epoch 3] Batch 447, Loss 0.49518734216690063\n","[Training Epoch 3] Batch 448, Loss 0.48104286193847656\n","[Training Epoch 3] Batch 449, Loss 0.4808470606803894\n","[Training Epoch 3] Batch 450, Loss 0.47685787081718445\n","[Training Epoch 3] Batch 451, Loss 0.5192530751228333\n","[Training Epoch 3] Batch 452, Loss 0.4827699065208435\n","[Training Epoch 3] Batch 453, Loss 0.4882410764694214\n","[Training Epoch 3] Batch 454, Loss 0.47831928730010986\n","[Training Epoch 3] Batch 455, Loss 0.5119727849960327\n","[Training Epoch 3] Batch 456, Loss 0.49848055839538574\n","[Training Epoch 3] Batch 457, Loss 0.47206103801727295\n","[Training Epoch 3] Batch 458, Loss 0.4864974319934845\n","[Training Epoch 3] Batch 459, Loss 0.5070477724075317\n","[Training Epoch 3] Batch 460, Loss 0.5254377722740173\n","[Training Epoch 3] Batch 461, Loss 0.47891297936439514\n","[Training Epoch 3] Batch 462, Loss 0.4883788526058197\n","[Training Epoch 3] Batch 463, Loss 0.5345690250396729\n","[Training Epoch 3] Batch 464, Loss 0.48636361956596375\n","[Training Epoch 3] Batch 465, Loss 0.4599993824958801\n","[Training Epoch 3] Batch 466, Loss 0.4983622133731842\n","[Training Epoch 3] Batch 467, Loss 0.4783056974411011\n","[Training Epoch 3] Batch 468, Loss 0.49053892493247986\n","[Training Epoch 3] Batch 469, Loss 0.532261312007904\n","[Training Epoch 3] Batch 470, Loss 0.4903772473335266\n","[Training Epoch 3] Batch 471, Loss 0.4781171679496765\n","[Training Epoch 3] Batch 472, Loss 0.4903624355792999\n","[Training Epoch 3] Batch 473, Loss 0.4975995421409607\n","[Training Epoch 3] Batch 474, Loss 0.4879717230796814\n","[Training Epoch 3] Batch 475, Loss 0.4950600266456604\n","[Training Epoch 3] Batch 476, Loss 0.47301724553108215\n","[Training Epoch 3] Batch 477, Loss 0.47292080521583557\n","[Training Epoch 3] Batch 478, Loss 0.49949756264686584\n","[Training Epoch 3] Batch 479, Loss 0.4747387170791626\n","[Training Epoch 3] Batch 480, Loss 0.5108997821807861\n","[Training Epoch 3] Batch 481, Loss 0.4981388449668884\n","[Training Epoch 3] Batch 482, Loss 0.5059785842895508\n","[Training Epoch 3] Batch 483, Loss 0.4890623688697815\n","[Training Epoch 3] Batch 484, Loss 0.5147114992141724\n","[Training Epoch 3] Batch 485, Loss 0.5032026767730713\n","[Training Epoch 3] Batch 486, Loss 0.524022102355957\n","[Training Epoch 3] Batch 487, Loss 0.47198161482810974\n","[Training Epoch 3] Batch 488, Loss 0.5126661062240601\n","[Training Epoch 3] Batch 489, Loss 0.47745025157928467\n","[Training Epoch 3] Batch 490, Loss 0.5084576606750488\n","[Training Epoch 3] Batch 491, Loss 0.47820353507995605\n","[Training Epoch 3] Batch 492, Loss 0.49427330493927\n","[Training Epoch 3] Batch 493, Loss 0.5125747323036194\n","[Training Epoch 3] Batch 494, Loss 0.49574756622314453\n","[Training Epoch 3] Batch 495, Loss 0.49400976300239563\n","[Training Epoch 3] Batch 496, Loss 0.49940794706344604\n","[Training Epoch 3] Batch 497, Loss 0.4902988374233246\n","[Training Epoch 3] Batch 498, Loss 0.5049383044242859\n","[Training Epoch 3] Batch 499, Loss 0.48973512649536133\n","[Training Epoch 3] Batch 500, Loss 0.49459612369537354\n","[Training Epoch 3] Batch 501, Loss 0.5045204162597656\n","[Training Epoch 3] Batch 502, Loss 0.4903912842273712\n","[Training Epoch 3] Batch 503, Loss 0.5008686780929565\n","[Training Epoch 3] Batch 504, Loss 0.4970146715641022\n","[Training Epoch 3] Batch 505, Loss 0.5256946086883545\n","[Training Epoch 3] Batch 506, Loss 0.5114807486534119\n","[Training Epoch 3] Batch 507, Loss 0.5083135366439819\n","[Training Epoch 3] Batch 508, Loss 0.4868815243244171\n","[Training Epoch 3] Batch 509, Loss 0.4861403703689575\n","[Training Epoch 3] Batch 510, Loss 0.4930231273174286\n","[Training Epoch 3] Batch 511, Loss 0.5073632001876831\n","[Training Epoch 3] Batch 512, Loss 0.4885328710079193\n","[Training Epoch 3] Batch 513, Loss 0.4966937303543091\n","[Training Epoch 3] Batch 514, Loss 0.4872802197933197\n","[Training Epoch 3] Batch 515, Loss 0.4766963720321655\n","[Training Epoch 3] Batch 516, Loss 0.5045785307884216\n","[Training Epoch 3] Batch 517, Loss 0.5022375583648682\n","[Training Epoch 3] Batch 518, Loss 0.5246443748474121\n","[Training Epoch 3] Batch 519, Loss 0.4995475709438324\n","[Training Epoch 3] Batch 520, Loss 0.5152578949928284\n","[Training Epoch 3] Batch 521, Loss 0.5279093384742737\n","[Training Epoch 3] Batch 522, Loss 0.49570250511169434\n","[Training Epoch 3] Batch 523, Loss 0.5099707841873169\n","[Training Epoch 3] Batch 524, Loss 0.5271138548851013\n","[Training Epoch 3] Batch 525, Loss 0.4995212256908417\n","[Training Epoch 3] Batch 526, Loss 0.517948567867279\n","[Training Epoch 3] Batch 527, Loss 0.4846158027648926\n","[Training Epoch 3] Batch 528, Loss 0.5482536554336548\n","[Training Epoch 3] Batch 529, Loss 0.524289071559906\n","[Training Epoch 3] Batch 530, Loss 0.5324274301528931\n","[Training Epoch 3] Batch 531, Loss 0.5089236497879028\n","[Training Epoch 3] Batch 532, Loss 0.524488091468811\n","[Training Epoch 3] Batch 533, Loss 0.49520087242126465\n","[Training Epoch 3] Batch 534, Loss 0.4961237907409668\n","[Training Epoch 3] Batch 535, Loss 0.49518218636512756\n","[Training Epoch 3] Batch 536, Loss 0.524775505065918\n","[Training Epoch 3] Batch 537, Loss 0.46683406829833984\n","[Training Epoch 3] Batch 538, Loss 0.5032343864440918\n","[Training Epoch 3] Batch 539, Loss 0.48891276121139526\n","[Training Epoch 3] Batch 540, Loss 0.47548913955688477\n","[Training Epoch 3] Batch 541, Loss 0.5006482601165771\n","[Training Epoch 3] Batch 542, Loss 0.5336350798606873\n","[Training Epoch 3] Batch 543, Loss 0.5059850811958313\n","[Training Epoch 3] Batch 544, Loss 0.4913461208343506\n","[Training Epoch 3] Batch 545, Loss 0.4833877682685852\n","[Training Epoch 3] Batch 546, Loss 0.4923536777496338\n","[Training Epoch 3] Batch 547, Loss 0.4810948371887207\n","[Training Epoch 3] Batch 548, Loss 0.4875541031360626\n","[Training Epoch 3] Batch 549, Loss 0.5296862721443176\n","[Training Epoch 3] Batch 550, Loss 0.5024337768554688\n","[Training Epoch 3] Batch 551, Loss 0.4875582456588745\n","[Training Epoch 3] Batch 552, Loss 0.4915453791618347\n","[Training Epoch 3] Batch 553, Loss 0.5012269616127014\n","[Training Epoch 3] Batch 554, Loss 0.4755482077598572\n","[Training Epoch 3] Batch 555, Loss 0.47695013880729675\n","[Training Epoch 3] Batch 556, Loss 0.49925822019577026\n","[Training Epoch 3] Batch 557, Loss 0.4863373041152954\n","[Training Epoch 3] Batch 558, Loss 0.4632159471511841\n","[Training Epoch 3] Batch 559, Loss 0.5154889225959778\n","[Training Epoch 3] Batch 560, Loss 0.5048481225967407\n","[Training Epoch 3] Batch 561, Loss 0.47545820474624634\n","[Training Epoch 3] Batch 562, Loss 0.46782636642456055\n","[Training Epoch 3] Batch 563, Loss 0.5135568380355835\n","[Training Epoch 3] Batch 564, Loss 0.49587732553482056\n","[Training Epoch 3] Batch 565, Loss 0.48071956634521484\n","[Training Epoch 3] Batch 566, Loss 0.5295433402061462\n","[Training Epoch 3] Batch 567, Loss 0.4992942214012146\n","[Training Epoch 3] Batch 568, Loss 0.5190972089767456\n","[Training Epoch 3] Batch 569, Loss 0.4829900562763214\n","[Training Epoch 3] Batch 570, Loss 0.5134286880493164\n","[Training Epoch 3] Batch 571, Loss 0.5027979016304016\n","[Training Epoch 3] Batch 572, Loss 0.47364339232444763\n","[Training Epoch 3] Batch 573, Loss 0.5193746089935303\n","[Training Epoch 3] Batch 574, Loss 0.5481326580047607\n","[Training Epoch 3] Batch 575, Loss 0.4992969036102295\n","[Training Epoch 3] Batch 576, Loss 0.5206637382507324\n","[Training Epoch 3] Batch 577, Loss 0.5003939867019653\n","[Training Epoch 3] Batch 578, Loss 0.4472963213920593\n","[Training Epoch 3] Batch 579, Loss 0.48936760425567627\n","[Training Epoch 3] Batch 580, Loss 0.5126835703849792\n","[Training Epoch 3] Batch 581, Loss 0.48465466499328613\n","[Training Epoch 3] Batch 582, Loss 0.501829206943512\n","[Training Epoch 3] Batch 583, Loss 0.5008241534233093\n","[Training Epoch 3] Batch 584, Loss 0.5173420906066895\n","[Training Epoch 3] Batch 585, Loss 0.5206788778305054\n","[Training Epoch 3] Batch 586, Loss 0.4816727638244629\n","[Training Epoch 3] Batch 587, Loss 0.5053539872169495\n","[Training Epoch 3] Batch 588, Loss 0.5148576498031616\n","[Training Epoch 3] Batch 589, Loss 0.495533287525177\n","[Training Epoch 3] Batch 590, Loss 0.5206562280654907\n","[Training Epoch 3] Batch 591, Loss 0.47436970472335815\n","[Training Epoch 3] Batch 592, Loss 0.48579728603363037\n","[Training Epoch 3] Batch 593, Loss 0.5221107006072998\n","[Training Epoch 3] Batch 594, Loss 0.486663818359375\n","[Training Epoch 3] Batch 595, Loss 0.511092483997345\n","[Training Epoch 3] Batch 596, Loss 0.47912970185279846\n","[Training Epoch 3] Batch 597, Loss 0.49553510546684265\n","[Training Epoch 3] Batch 598, Loss 0.5186277627944946\n","[Training Epoch 3] Batch 599, Loss 0.4773939251899719\n","[Training Epoch 3] Batch 600, Loss 0.5024589896202087\n","[Training Epoch 3] Batch 601, Loss 0.5031830072402954\n","[Training Epoch 3] Batch 602, Loss 0.4905567169189453\n","[Training Epoch 3] Batch 603, Loss 0.5268065929412842\n","[Training Epoch 3] Batch 604, Loss 0.5128929018974304\n","[Training Epoch 3] Batch 605, Loss 0.5012088418006897\n","[Training Epoch 3] Batch 606, Loss 0.49877291917800903\n","[Training Epoch 3] Batch 607, Loss 0.48490411043167114\n","[Training Epoch 3] Batch 608, Loss 0.47397738695144653\n","[Training Epoch 3] Batch 609, Loss 0.5022225975990295\n","[Training Epoch 3] Batch 610, Loss 0.5070890188217163\n","[Training Epoch 3] Batch 611, Loss 0.5142552852630615\n","[Training Epoch 3] Batch 612, Loss 0.5316963195800781\n","[Training Epoch 3] Batch 613, Loss 0.5011031627655029\n","[Training Epoch 3] Batch 614, Loss 0.4892030656337738\n","[Training Epoch 3] Batch 615, Loss 0.49014103412628174\n","[Training Epoch 3] Batch 616, Loss 0.5209344625473022\n","[Training Epoch 3] Batch 617, Loss 0.4988129734992981\n","[Training Epoch 3] Batch 618, Loss 0.5025933980941772\n","[Training Epoch 3] Batch 619, Loss 0.5039102435112\n","[Training Epoch 3] Batch 620, Loss 0.5084244012832642\n","[Training Epoch 3] Batch 621, Loss 0.5245137214660645\n","[Training Epoch 3] Batch 622, Loss 0.49248912930488586\n","[Training Epoch 3] Batch 623, Loss 0.4800109267234802\n","[Training Epoch 3] Batch 624, Loss 0.4965243339538574\n","[Training Epoch 3] Batch 625, Loss 0.4783656895160675\n","[Training Epoch 3] Batch 626, Loss 0.5225270986557007\n","[Training Epoch 3] Batch 627, Loss 0.47292807698249817\n","[Training Epoch 3] Batch 628, Loss 0.4978507459163666\n","[Training Epoch 3] Batch 629, Loss 0.48486095666885376\n","[Training Epoch 3] Batch 630, Loss 0.4727139472961426\n","[Training Epoch 3] Batch 631, Loss 0.46861815452575684\n","[Training Epoch 3] Batch 632, Loss 0.4833156168460846\n","[Training Epoch 3] Batch 633, Loss 0.48204636573791504\n","[Training Epoch 3] Batch 634, Loss 0.5193086862564087\n","[Training Epoch 3] Batch 635, Loss 0.4812033474445343\n","[Training Epoch 3] Batch 636, Loss 0.49420788884162903\n","[Training Epoch 3] Batch 637, Loss 0.5071215629577637\n","[Training Epoch 3] Batch 638, Loss 0.5039114952087402\n","[Training Epoch 3] Batch 639, Loss 0.5406285524368286\n","[Training Epoch 3] Batch 640, Loss 0.5068426728248596\n","[Training Epoch 3] Batch 641, Loss 0.5086288452148438\n","[Training Epoch 3] Batch 642, Loss 0.48590990900993347\n","[Training Epoch 3] Batch 643, Loss 0.4889340400695801\n","[Training Epoch 3] Batch 644, Loss 0.4984161853790283\n","[Training Epoch 3] Batch 645, Loss 0.5178224444389343\n","[Training Epoch 3] Batch 646, Loss 0.49596893787384033\n","[Training Epoch 3] Batch 647, Loss 0.5207562446594238\n","[Training Epoch 3] Batch 648, Loss 0.5054335594177246\n","[Training Epoch 3] Batch 649, Loss 0.5104215145111084\n","[Training Epoch 3] Batch 650, Loss 0.5113760828971863\n","[Training Epoch 3] Batch 651, Loss 0.4806326627731323\n","[Training Epoch 3] Batch 652, Loss 0.5139734148979187\n","[Training Epoch 3] Batch 653, Loss 0.5072690844535828\n","[Training Epoch 3] Batch 654, Loss 0.49299031496047974\n","[Training Epoch 3] Batch 655, Loss 0.5201793909072876\n","[Training Epoch 3] Batch 656, Loss 0.5220617055892944\n","[Training Epoch 3] Batch 657, Loss 0.5140129923820496\n","[Training Epoch 3] Batch 658, Loss 0.49285441637039185\n","[Training Epoch 3] Batch 659, Loss 0.4713549017906189\n","[Training Epoch 3] Batch 660, Loss 0.5182940363883972\n","[Training Epoch 3] Batch 661, Loss 0.4847191274166107\n","[Training Epoch 3] Batch 662, Loss 0.48518967628479004\n","[Training Epoch 3] Batch 663, Loss 0.5166991949081421\n","[Training Epoch 3] Batch 664, Loss 0.5033700466156006\n","[Training Epoch 3] Batch 665, Loss 0.5105715394020081\n","[Training Epoch 3] Batch 666, Loss 0.5098332166671753\n","[Training Epoch 3] Batch 667, Loss 0.5134884119033813\n","[Training Epoch 3] Batch 668, Loss 0.4901390075683594\n","[Training Epoch 3] Batch 669, Loss 0.4826136827468872\n","[Training Epoch 3] Batch 670, Loss 0.49058523774147034\n","[Training Epoch 3] Batch 671, Loss 0.504848301410675\n","[Training Epoch 3] Batch 672, Loss 0.5041128993034363\n","[Training Epoch 3] Batch 673, Loss 0.47666099667549133\n","[Training Epoch 3] Batch 674, Loss 0.48354992270469666\n","[Training Epoch 3] Batch 675, Loss 0.4756454527378082\n","[Training Epoch 3] Batch 676, Loss 0.46111297607421875\n","[Training Epoch 3] Batch 677, Loss 0.4673115611076355\n","[Training Epoch 3] Batch 678, Loss 0.48754119873046875\n","[Training Epoch 3] Batch 679, Loss 0.4895852208137512\n","[Training Epoch 3] Batch 680, Loss 0.4966229796409607\n","[Training Epoch 3] Batch 681, Loss 0.4991045594215393\n","[Training Epoch 3] Batch 682, Loss 0.5112231969833374\n","[Training Epoch 3] Batch 683, Loss 0.49913230538368225\n","[Training Epoch 3] Batch 684, Loss 0.523842453956604\n","[Training Epoch 3] Batch 685, Loss 0.4816007912158966\n","[Training Epoch 3] Batch 686, Loss 0.47666680812835693\n","[Training Epoch 3] Batch 687, Loss 0.5348877906799316\n","[Training Epoch 3] Batch 688, Loss 0.4860980808734894\n","[Training Epoch 3] Batch 689, Loss 0.49794694781303406\n","[Training Epoch 3] Batch 690, Loss 0.5191250443458557\n","[Training Epoch 3] Batch 691, Loss 0.47087928652763367\n","[Training Epoch 3] Batch 692, Loss 0.5125896334648132\n","[Training Epoch 3] Batch 693, Loss 0.5191311836242676\n","[Training Epoch 3] Batch 694, Loss 0.5006568431854248\n","[Training Epoch 3] Batch 695, Loss 0.5365623235702515\n","[Training Epoch 3] Batch 696, Loss 0.5434337854385376\n","[Training Epoch 3] Batch 697, Loss 0.5012385249137878\n","[Training Epoch 3] Batch 698, Loss 0.508636474609375\n","[Training Epoch 3] Batch 699, Loss 0.5183864831924438\n","[Training Epoch 3] Batch 700, Loss 0.4674297869205475\n","[Training Epoch 3] Batch 701, Loss 0.4886088967323303\n","[Training Epoch 3] Batch 702, Loss 0.4911971092224121\n","[Training Epoch 3] Batch 703, Loss 0.5020911693572998\n","[Training Epoch 3] Batch 704, Loss 0.5197173953056335\n","[Training Epoch 3] Batch 705, Loss 0.515997588634491\n","[Training Epoch 3] Batch 706, Loss 0.4702306389808655\n","[Training Epoch 3] Batch 707, Loss 0.49337515234947205\n","[Training Epoch 3] Batch 708, Loss 0.4904063045978546\n","[Training Epoch 3] Batch 709, Loss 0.4698770344257355\n","[Training Epoch 3] Batch 710, Loss 0.4909971356391907\n","[Training Epoch 3] Batch 711, Loss 0.5125420689582825\n","[Training Epoch 3] Batch 712, Loss 0.4956666827201843\n","[Training Epoch 3] Batch 713, Loss 0.502032995223999\n","[Training Epoch 3] Batch 714, Loss 0.5089002251625061\n","[Training Epoch 3] Batch 715, Loss 0.4709551930427551\n","[Training Epoch 3] Batch 716, Loss 0.5115667581558228\n","[Training Epoch 3] Batch 717, Loss 0.5243563652038574\n","[Training Epoch 3] Batch 718, Loss 0.4978111982345581\n","[Training Epoch 3] Batch 719, Loss 0.5103844404220581\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 3] HR = 0.0552, NDCG = 0.0280\n","Epoch 4 starts !\n","--------------------------------------------------------------------------------\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/data.py:150: DeprecationWarning: Sampling from a set deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n","[Training Epoch 4] Batch 0, Loss 0.49076855182647705\n","[Training Epoch 4] Batch 1, Loss 0.49819377064704895\n","[Training Epoch 4] Batch 2, Loss 0.47836047410964966\n","[Training Epoch 4] Batch 3, Loss 0.48579999804496765\n","[Training Epoch 4] Batch 4, Loss 0.5162574648857117\n","[Training Epoch 4] Batch 5, Loss 0.47000473737716675\n","[Training Epoch 4] Batch 6, Loss 0.5246837139129639\n","[Training Epoch 4] Batch 7, Loss 0.5056509971618652\n","[Training Epoch 4] Batch 8, Loss 0.5036701560020447\n","[Training Epoch 4] Batch 9, Loss 0.4897080659866333\n","[Training Epoch 4] Batch 10, Loss 0.5114613771438599\n","[Training Epoch 4] Batch 11, Loss 0.5265071988105774\n","[Training Epoch 4] Batch 12, Loss 0.5087453722953796\n","[Training Epoch 4] Batch 13, Loss 0.5103311538696289\n","[Training Epoch 4] Batch 14, Loss 0.5327545404434204\n","[Training Epoch 4] Batch 15, Loss 0.49260228872299194\n","[Training Epoch 4] Batch 16, Loss 0.5149423480033875\n","[Training Epoch 4] Batch 17, Loss 0.5178849101066589\n","[Training Epoch 4] Batch 18, Loss 0.506563127040863\n","[Training Epoch 4] Batch 19, Loss 0.505849301815033\n","[Training Epoch 4] Batch 20, Loss 0.484640508890152\n","[Training Epoch 4] Batch 21, Loss 0.507536768913269\n","[Training Epoch 4] Batch 22, Loss 0.48761609196662903\n","[Training Epoch 4] Batch 23, Loss 0.5215951800346375\n","[Training Epoch 4] Batch 24, Loss 0.4976627230644226\n","[Training Epoch 4] Batch 25, Loss 0.5049089789390564\n","[Training Epoch 4] Batch 26, Loss 0.5385322570800781\n","[Training Epoch 4] Batch 27, Loss 0.4807141423225403\n","[Training Epoch 4] Batch 28, Loss 0.5186265110969543\n","[Training Epoch 4] Batch 29, Loss 0.480888694524765\n","[Training Epoch 4] Batch 30, Loss 0.5324721336364746\n","[Training Epoch 4] Batch 31, Loss 0.5083123445510864\n","[Training Epoch 4] Batch 32, Loss 0.5012661218643188\n","[Training Epoch 4] Batch 33, Loss 0.5149087905883789\n","[Training Epoch 4] Batch 34, Loss 0.47661930322647095\n","[Training Epoch 4] Batch 35, Loss 0.499977707862854\n","[Training Epoch 4] Batch 36, Loss 0.49115824699401855\n","[Training Epoch 4] Batch 37, Loss 0.5300573110580444\n","[Training Epoch 4] Batch 38, Loss 0.4951702654361725\n","[Training Epoch 4] Batch 39, Loss 0.4887103736400604\n","[Training Epoch 4] Batch 40, Loss 0.480090856552124\n","[Training Epoch 4] Batch 41, Loss 0.5272108316421509\n","[Training Epoch 4] Batch 42, Loss 0.48958468437194824\n","[Training Epoch 4] Batch 43, Loss 0.512267529964447\n","[Training Epoch 4] Batch 44, Loss 0.5010833740234375\n","[Training Epoch 4] Batch 45, Loss 0.5092278122901917\n","[Training Epoch 4] Batch 46, Loss 0.5113557577133179\n","[Training Epoch 4] Batch 47, Loss 0.4608865976333618\n","[Training Epoch 4] Batch 48, Loss 0.49869221448898315\n","[Training Epoch 4] Batch 49, Loss 0.4870012104511261\n","[Training Epoch 4] Batch 50, Loss 0.5033384561538696\n","[Training Epoch 4] Batch 51, Loss 0.5194368362426758\n","[Training Epoch 4] Batch 52, Loss 0.48762112855911255\n","[Training Epoch 4] Batch 53, Loss 0.5424768328666687\n","[Training Epoch 4] Batch 54, Loss 0.4922523498535156\n","[Training Epoch 4] Batch 55, Loss 0.4957563877105713\n","[Training Epoch 4] Batch 56, Loss 0.4832700490951538\n","[Training Epoch 4] Batch 57, Loss 0.5008367300033569\n","[Training Epoch 4] Batch 58, Loss 0.503725528717041\n","[Training Epoch 4] Batch 59, Loss 0.4966481328010559\n","[Training Epoch 4] Batch 60, Loss 0.4818115830421448\n","[Training Epoch 4] Batch 61, Loss 0.5048210620880127\n","[Training Epoch 4] Batch 62, Loss 0.5238856673240662\n","[Training Epoch 4] Batch 63, Loss 0.481749564409256\n","[Training Epoch 4] Batch 64, Loss 0.4855295419692993\n","[Training Epoch 4] Batch 65, Loss 0.4789247512817383\n","[Training Epoch 4] Batch 66, Loss 0.4860585629940033\n","[Training Epoch 4] Batch 67, Loss 0.490780770778656\n","[Training Epoch 4] Batch 68, Loss 0.5324668884277344\n","[Training Epoch 4] Batch 69, Loss 0.4836958348751068\n","[Training Epoch 4] Batch 70, Loss 0.4869363605976105\n","[Training Epoch 4] Batch 71, Loss 0.5066103935241699\n","[Training Epoch 4] Batch 72, Loss 0.4796404540538788\n","[Training Epoch 4] Batch 73, Loss 0.5236366987228394\n","[Training Epoch 4] Batch 74, Loss 0.5259015560150146\n","[Training Epoch 4] Batch 75, Loss 0.4887080788612366\n","[Training Epoch 4] Batch 76, Loss 0.49789977073669434\n","[Training Epoch 4] Batch 77, Loss 0.5056246519088745\n","[Training Epoch 4] Batch 78, Loss 0.5005325675010681\n","[Training Epoch 4] Batch 79, Loss 0.480398565530777\n","[Training Epoch 4] Batch 80, Loss 0.496218204498291\n","[Training Epoch 4] Batch 81, Loss 0.49380242824554443\n","[Training Epoch 4] Batch 82, Loss 0.509232759475708\n","[Training Epoch 4] Batch 83, Loss 0.47905513644218445\n","[Training Epoch 4] Batch 84, Loss 0.507860541343689\n","[Training Epoch 4] Batch 85, Loss 0.5108349323272705\n","[Training Epoch 4] Batch 86, Loss 0.5004273653030396\n","[Training Epoch 4] Batch 87, Loss 0.5003840923309326\n","[Training Epoch 4] Batch 88, Loss 0.5114346742630005\n","[Training Epoch 4] Batch 89, Loss 0.5155837535858154\n","[Training Epoch 4] Batch 90, Loss 0.5061192512512207\n","[Training Epoch 4] Batch 91, Loss 0.47676610946655273\n","[Training Epoch 4] Batch 92, Loss 0.4853031039237976\n","[Training Epoch 4] Batch 93, Loss 0.4911264181137085\n","[Training Epoch 4] Batch 94, Loss 0.5248255133628845\n","[Training Epoch 4] Batch 95, Loss 0.5203590393066406\n","[Training Epoch 4] Batch 96, Loss 0.5070711970329285\n","[Training Epoch 4] Batch 97, Loss 0.45647794008255005\n","[Training Epoch 4] Batch 98, Loss 0.467502236366272\n","[Training Epoch 4] Batch 99, Loss 0.5193778872489929\n","[Training Epoch 4] Batch 100, Loss 0.5052777528762817\n","[Training Epoch 4] Batch 101, Loss 0.48959434032440186\n","[Training Epoch 4] Batch 102, Loss 0.4853273928165436\n","[Training Epoch 4] Batch 103, Loss 0.5062462091445923\n","[Training Epoch 4] Batch 104, Loss 0.47083747386932373\n","[Training Epoch 4] Batch 105, Loss 0.5087655782699585\n","[Training Epoch 4] Batch 106, Loss 0.49317678809165955\n","[Training Epoch 4] Batch 107, Loss 0.5008299350738525\n","[Training Epoch 4] Batch 108, Loss 0.49516576528549194\n","[Training Epoch 4] Batch 109, Loss 0.5102699398994446\n","[Training Epoch 4] Batch 110, Loss 0.5053266882896423\n","[Training Epoch 4] Batch 111, Loss 0.512182354927063\n","[Training Epoch 4] Batch 112, Loss 0.5004779100418091\n","[Training Epoch 4] Batch 113, Loss 0.4944112300872803\n","[Training Epoch 4] Batch 114, Loss 0.5281438827514648\n","[Training Epoch 4] Batch 115, Loss 0.5127907991409302\n","[Training Epoch 4] Batch 116, Loss 0.5017549991607666\n","[Training Epoch 4] Batch 117, Loss 0.5152248740196228\n","[Training Epoch 4] Batch 118, Loss 0.4745101034641266\n","[Training Epoch 4] Batch 119, Loss 0.5040176510810852\n","[Training Epoch 4] Batch 120, Loss 0.4948306977748871\n","[Training Epoch 4] Batch 121, Loss 0.4575440585613251\n","[Training Epoch 4] Batch 122, Loss 0.48360124230384827\n","[Training Epoch 4] Batch 123, Loss 0.5044060349464417\n","[Training Epoch 4] Batch 124, Loss 0.5471330881118774\n","[Training Epoch 4] Batch 125, Loss 0.470302551984787\n","[Training Epoch 4] Batch 126, Loss 0.4761611223220825\n","[Training Epoch 4] Batch 127, Loss 0.4745017886161804\n","[Training Epoch 4] Batch 128, Loss 0.5362018346786499\n","[Training Epoch 4] Batch 129, Loss 0.5322309732437134\n","[Training Epoch 4] Batch 130, Loss 0.5306122303009033\n","[Training Epoch 4] Batch 131, Loss 0.4916412830352783\n","[Training Epoch 4] Batch 132, Loss 0.4901769757270813\n","[Training Epoch 4] Batch 133, Loss 0.5033384561538696\n","[Training Epoch 4] Batch 134, Loss 0.4885413646697998\n","[Training Epoch 4] Batch 135, Loss 0.5399541854858398\n","[Training Epoch 4] Batch 136, Loss 0.535804808139801\n","[Training Epoch 4] Batch 137, Loss 0.49351662397384644\n","[Training Epoch 4] Batch 138, Loss 0.4847314655780792\n","[Training Epoch 4] Batch 139, Loss 0.49820375442504883\n","[Training Epoch 4] Batch 140, Loss 0.5072705149650574\n","[Training Epoch 4] Batch 141, Loss 0.4758225977420807\n","[Training Epoch 4] Batch 142, Loss 0.506171703338623\n","[Training Epoch 4] Batch 143, Loss 0.4996853470802307\n","[Training Epoch 4] Batch 144, Loss 0.48961204290390015\n","[Training Epoch 4] Batch 145, Loss 0.5258299112319946\n","[Training Epoch 4] Batch 146, Loss 0.4760535955429077\n","[Training Epoch 4] Batch 147, Loss 0.5282691717147827\n","[Training Epoch 4] Batch 148, Loss 0.48949846625328064\n","[Training Epoch 4] Batch 149, Loss 0.48225897550582886\n","[Training Epoch 4] Batch 150, Loss 0.4960080087184906\n","[Training Epoch 4] Batch 151, Loss 0.5113440752029419\n","[Training Epoch 4] Batch 152, Loss 0.5083862543106079\n","[Training Epoch 4] Batch 153, Loss 0.4956234097480774\n","[Training Epoch 4] Batch 154, Loss 0.5299137830734253\n","[Training Epoch 4] Batch 155, Loss 0.5070040225982666\n","[Training Epoch 4] Batch 156, Loss 0.49240952730178833\n","[Training Epoch 4] Batch 157, Loss 0.5188559293746948\n","[Training Epoch 4] Batch 158, Loss 0.48978084325790405\n","[Training Epoch 4] Batch 159, Loss 0.4909020960330963\n","[Training Epoch 4] Batch 160, Loss 0.5028791427612305\n","[Training Epoch 4] Batch 161, Loss 0.5057279467582703\n","[Training Epoch 4] Batch 162, Loss 0.47037529945373535\n","[Training Epoch 4] Batch 163, Loss 0.4952895939350128\n","[Training Epoch 4] Batch 164, Loss 0.5299479365348816\n","[Training Epoch 4] Batch 165, Loss 0.47702187299728394\n","[Training Epoch 4] Batch 166, Loss 0.49540987610816956\n","[Training Epoch 4] Batch 167, Loss 0.5061968564987183\n","[Training Epoch 4] Batch 168, Loss 0.5138488411903381\n","[Training Epoch 4] Batch 169, Loss 0.4884471893310547\n","[Training Epoch 4] Batch 170, Loss 0.47960320115089417\n","[Training Epoch 4] Batch 171, Loss 0.4667254090309143\n","[Training Epoch 4] Batch 172, Loss 0.4821698069572449\n","[Training Epoch 4] Batch 173, Loss 0.4589030146598816\n","[Training Epoch 4] Batch 174, Loss 0.46969014406204224\n","[Training Epoch 4] Batch 175, Loss 0.4911685883998871\n","[Training Epoch 4] Batch 176, Loss 0.48264583945274353\n","[Training Epoch 4] Batch 177, Loss 0.486178994178772\n","[Training Epoch 4] Batch 178, Loss 0.47049880027770996\n","[Training Epoch 4] Batch 179, Loss 0.49423784017562866\n","[Training Epoch 4] Batch 180, Loss 0.4992969036102295\n","[Training Epoch 4] Batch 181, Loss 0.5052502155303955\n","[Training Epoch 4] Batch 182, Loss 0.4829348027706146\n","[Training Epoch 4] Batch 183, Loss 0.5027759075164795\n","[Training Epoch 4] Batch 184, Loss 0.5031724572181702\n","[Training Epoch 4] Batch 185, Loss 0.5398682355880737\n","[Training Epoch 4] Batch 186, Loss 0.5117693543434143\n","[Training Epoch 4] Batch 187, Loss 0.49635761976242065\n","[Training Epoch 4] Batch 188, Loss 0.49157071113586426\n","[Training Epoch 4] Batch 189, Loss 0.4809960126876831\n","[Training Epoch 4] Batch 190, Loss 0.5129396915435791\n","[Training Epoch 4] Batch 191, Loss 0.5144939422607422\n","[Training Epoch 4] Batch 192, Loss 0.5068790912628174\n","[Training Epoch 4] Batch 193, Loss 0.4795036315917969\n","[Training Epoch 4] Batch 194, Loss 0.5378655195236206\n","[Training Epoch 4] Batch 195, Loss 0.5055908560752869\n","[Training Epoch 4] Batch 196, Loss 0.5018354654312134\n","[Training Epoch 4] Batch 197, Loss 0.49692612886428833\n","[Training Epoch 4] Batch 198, Loss 0.4799177050590515\n","[Training Epoch 4] Batch 199, Loss 0.49374252557754517\n","[Training Epoch 4] Batch 200, Loss 0.49029070138931274\n","[Training Epoch 4] Batch 201, Loss 0.5147701501846313\n","[Training Epoch 4] Batch 202, Loss 0.5032041072845459\n","[Training Epoch 4] Batch 203, Loss 0.5129067897796631\n","[Training Epoch 4] Batch 204, Loss 0.5302231907844543\n","[Training Epoch 4] Batch 205, Loss 0.49838969111442566\n","[Training Epoch 4] Batch 206, Loss 0.47951045632362366\n","[Training Epoch 4] Batch 207, Loss 0.5168512463569641\n","[Training Epoch 4] Batch 208, Loss 0.5079609155654907\n","[Training Epoch 4] Batch 209, Loss 0.47486644983291626\n","[Training Epoch 4] Batch 210, Loss 0.48498988151550293\n","[Training Epoch 4] Batch 211, Loss 0.4930371046066284\n","[Training Epoch 4] Batch 212, Loss 0.49433204531669617\n","[Training Epoch 4] Batch 213, Loss 0.5402553677558899\n","[Training Epoch 4] Batch 214, Loss 0.5039912462234497\n","[Training Epoch 4] Batch 215, Loss 0.4990086555480957\n","[Training Epoch 4] Batch 216, Loss 0.521101713180542\n","[Training Epoch 4] Batch 217, Loss 0.49696171283721924\n","[Training Epoch 4] Batch 218, Loss 0.5005301237106323\n","[Training Epoch 4] Batch 219, Loss 0.47638872265815735\n","[Training Epoch 4] Batch 220, Loss 0.47060492634773254\n","[Training Epoch 4] Batch 221, Loss 0.5120970606803894\n","[Training Epoch 4] Batch 222, Loss 0.4872300624847412\n","[Training Epoch 4] Batch 223, Loss 0.4863007068634033\n","[Training Epoch 4] Batch 224, Loss 0.4995204210281372\n","[Training Epoch 4] Batch 225, Loss 0.4960615932941437\n","[Training Epoch 4] Batch 226, Loss 0.506494402885437\n","[Training Epoch 4] Batch 227, Loss 0.5116223096847534\n","[Training Epoch 4] Batch 228, Loss 0.48846662044525146\n","[Training Epoch 4] Batch 229, Loss 0.4960893392562866\n","[Training Epoch 4] Batch 230, Loss 0.5213242769241333\n","[Training Epoch 4] Batch 231, Loss 0.4927711486816406\n","[Training Epoch 4] Batch 232, Loss 0.4792548418045044\n","[Training Epoch 4] Batch 233, Loss 0.5152018070220947\n","[Training Epoch 4] Batch 234, Loss 0.5094445943832397\n","[Training Epoch 4] Batch 235, Loss 0.4658527374267578\n","[Training Epoch 4] Batch 236, Loss 0.49335864186286926\n","[Training Epoch 4] Batch 237, Loss 0.5148844718933105\n","[Training Epoch 4] Batch 238, Loss 0.4855107069015503\n","[Training Epoch 4] Batch 239, Loss 0.48659586906433105\n","[Training Epoch 4] Batch 240, Loss 0.5032645463943481\n","[Training Epoch 4] Batch 241, Loss 0.46977922320365906\n","[Training Epoch 4] Batch 242, Loss 0.4726443588733673\n","[Training Epoch 4] Batch 243, Loss 0.5119873285293579\n","[Training Epoch 4] Batch 244, Loss 0.48731619119644165\n","[Training Epoch 4] Batch 245, Loss 0.510150671005249\n","[Training Epoch 4] Batch 246, Loss 0.5270755290985107\n","[Training Epoch 4] Batch 247, Loss 0.5001161694526672\n","[Training Epoch 4] Batch 248, Loss 0.48020249605178833\n","[Training Epoch 4] Batch 249, Loss 0.4875298738479614\n","[Training Epoch 4] Batch 250, Loss 0.4811524748802185\n","[Training Epoch 4] Batch 251, Loss 0.49454769492149353\n","[Training Epoch 4] Batch 252, Loss 0.5301178693771362\n","[Training Epoch 4] Batch 253, Loss 0.4751278758049011\n","[Training Epoch 4] Batch 254, Loss 0.5084917545318604\n","[Training Epoch 4] Batch 255, Loss 0.47743064165115356\n","[Training Epoch 4] Batch 256, Loss 0.5032191276550293\n","[Training Epoch 4] Batch 257, Loss 0.509222686290741\n","[Training Epoch 4] Batch 258, Loss 0.4970703125\n","[Training Epoch 4] Batch 259, Loss 0.5159494280815125\n","[Training Epoch 4] Batch 260, Loss 0.5116408467292786\n","[Training Epoch 4] Batch 261, Loss 0.5071116089820862\n","[Training Epoch 4] Batch 262, Loss 0.49546852707862854\n","[Training Epoch 4] Batch 263, Loss 0.5023183822631836\n","[Training Epoch 4] Batch 264, Loss 0.5411475896835327\n","[Training Epoch 4] Batch 265, Loss 0.500008225440979\n","[Training Epoch 4] Batch 266, Loss 0.4818119704723358\n","[Training Epoch 4] Batch 267, Loss 0.5030496120452881\n","[Training Epoch 4] Batch 268, Loss 0.5020158290863037\n","[Training Epoch 4] Batch 269, Loss 0.5097329616546631\n","[Training Epoch 4] Batch 270, Loss 0.5056219100952148\n","[Training Epoch 4] Batch 271, Loss 0.5178880095481873\n","[Training Epoch 4] Batch 272, Loss 0.5075482130050659\n","[Training Epoch 4] Batch 273, Loss 0.5036881566047668\n","[Training Epoch 4] Batch 274, Loss 0.4920949339866638\n","[Training Epoch 4] Batch 275, Loss 0.47637230157852173\n","[Training Epoch 4] Batch 276, Loss 0.49971094727516174\n","[Training Epoch 4] Batch 277, Loss 0.4736315608024597\n","[Training Epoch 4] Batch 278, Loss 0.4752727746963501\n","[Training Epoch 4] Batch 279, Loss 0.4657483696937561\n","[Training Epoch 4] Batch 280, Loss 0.511604905128479\n","[Training Epoch 4] Batch 281, Loss 0.4858474135398865\n","[Training Epoch 4] Batch 282, Loss 0.4926227033138275\n","[Training Epoch 4] Batch 283, Loss 0.5044716596603394\n","[Training Epoch 4] Batch 284, Loss 0.4854933023452759\n","[Training Epoch 4] Batch 285, Loss 0.5004399418830872\n","[Training Epoch 4] Batch 286, Loss 0.4969644546508789\n","[Training Epoch 4] Batch 287, Loss 0.488492488861084\n","[Training Epoch 4] Batch 288, Loss 0.5130845308303833\n","[Training Epoch 4] Batch 289, Loss 0.4957923889160156\n","[Training Epoch 4] Batch 290, Loss 0.49391859769821167\n","[Training Epoch 4] Batch 291, Loss 0.5190334320068359\n","[Training Epoch 4] Batch 292, Loss 0.5104647278785706\n","[Training Epoch 4] Batch 293, Loss 0.5230854153633118\n","[Training Epoch 4] Batch 294, Loss 0.50384122133255\n","[Training Epoch 4] Batch 295, Loss 0.48575925827026367\n","[Training Epoch 4] Batch 296, Loss 0.49897581338882446\n","[Training Epoch 4] Batch 297, Loss 0.49726569652557373\n","[Training Epoch 4] Batch 298, Loss 0.49765968322753906\n","[Training Epoch 4] Batch 299, Loss 0.5216233134269714\n","[Training Epoch 4] Batch 300, Loss 0.5154211521148682\n","[Training Epoch 4] Batch 301, Loss 0.5116515159606934\n","[Training Epoch 4] Batch 302, Loss 0.48637115955352783\n","[Training Epoch 4] Batch 303, Loss 0.5205131769180298\n","[Training Epoch 4] Batch 304, Loss 0.4821751117706299\n","[Training Epoch 4] Batch 305, Loss 0.49970877170562744\n","[Training Epoch 4] Batch 306, Loss 0.5041648149490356\n","[Training Epoch 4] Batch 307, Loss 0.5188406705856323\n","[Training Epoch 4] Batch 308, Loss 0.5102972388267517\n","[Training Epoch 4] Batch 309, Loss 0.48316216468811035\n","[Training Epoch 4] Batch 310, Loss 0.5277812480926514\n","[Training Epoch 4] Batch 311, Loss 0.4599419832229614\n","[Training Epoch 4] Batch 312, Loss 0.5056996941566467\n","[Training Epoch 4] Batch 313, Loss 0.4955803155899048\n","[Training Epoch 4] Batch 314, Loss 0.5313966274261475\n","[Training Epoch 4] Batch 315, Loss 0.49386632442474365\n","[Training Epoch 4] Batch 316, Loss 0.5007646083831787\n","[Training Epoch 4] Batch 317, Loss 0.4979311227798462\n","[Training Epoch 4] Batch 318, Loss 0.4674922823905945\n","[Training Epoch 4] Batch 319, Loss 0.49143171310424805\n","[Training Epoch 4] Batch 320, Loss 0.5008459091186523\n","[Training Epoch 4] Batch 321, Loss 0.5074872970581055\n","[Training Epoch 4] Batch 322, Loss 0.49387288093566895\n","[Training Epoch 4] Batch 323, Loss 0.4918707609176636\n","[Training Epoch 4] Batch 324, Loss 0.5104401111602783\n","[Training Epoch 4] Batch 325, Loss 0.4776734709739685\n","[Training Epoch 4] Batch 326, Loss 0.4939209520816803\n","[Training Epoch 4] Batch 327, Loss 0.4821125864982605\n","[Training Epoch 4] Batch 328, Loss 0.4858453571796417\n","[Training Epoch 4] Batch 329, Loss 0.4919489920139313\n","[Training Epoch 4] Batch 330, Loss 0.5294269323348999\n","[Training Epoch 4] Batch 331, Loss 0.49813535809516907\n","[Training Epoch 4] Batch 332, Loss 0.5267520546913147\n","[Training Epoch 4] Batch 333, Loss 0.5333143472671509\n","[Training Epoch 4] Batch 334, Loss 0.5123212933540344\n","[Training Epoch 4] Batch 335, Loss 0.5110551118850708\n","[Training Epoch 4] Batch 336, Loss 0.48315590620040894\n","[Training Epoch 4] Batch 337, Loss 0.4792141318321228\n","[Training Epoch 4] Batch 338, Loss 0.5112110376358032\n","[Training Epoch 4] Batch 339, Loss 0.4938606321811676\n","[Training Epoch 4] Batch 340, Loss 0.5112472176551819\n","[Training Epoch 4] Batch 341, Loss 0.4891827404499054\n","[Training Epoch 4] Batch 342, Loss 0.48327627778053284\n","[Training Epoch 4] Batch 343, Loss 0.524773120880127\n","[Training Epoch 4] Batch 344, Loss 0.5247284173965454\n","[Training Epoch 4] Batch 345, Loss 0.5116393566131592\n","[Training Epoch 4] Batch 346, Loss 0.48941749334335327\n","[Training Epoch 4] Batch 347, Loss 0.5071372985839844\n","[Training Epoch 4] Batch 348, Loss 0.4873385727405548\n","[Training Epoch 4] Batch 349, Loss 0.5367343425750732\n","[Training Epoch 4] Batch 350, Loss 0.4763645529747009\n","[Training Epoch 4] Batch 351, Loss 0.498058557510376\n","[Training Epoch 4] Batch 352, Loss 0.472883403301239\n","[Training Epoch 4] Batch 353, Loss 0.5004328489303589\n","[Training Epoch 4] Batch 354, Loss 0.5107266902923584\n","[Training Epoch 4] Batch 355, Loss 0.4962998330593109\n","[Training Epoch 4] Batch 356, Loss 0.4760897159576416\n","[Training Epoch 4] Batch 357, Loss 0.5185940265655518\n","[Training Epoch 4] Batch 358, Loss 0.48966529965400696\n","[Training Epoch 4] Batch 359, Loss 0.4827606678009033\n","[Training Epoch 4] Batch 360, Loss 0.531629204750061\n","[Training Epoch 4] Batch 361, Loss 0.47879642248153687\n","[Training Epoch 4] Batch 362, Loss 0.5252019166946411\n","[Training Epoch 4] Batch 363, Loss 0.5146256685256958\n","[Training Epoch 4] Batch 364, Loss 0.5050300359725952\n","[Training Epoch 4] Batch 365, Loss 0.4977608919143677\n","[Training Epoch 4] Batch 366, Loss 0.5123844146728516\n","[Training Epoch 4] Batch 367, Loss 0.4858054220676422\n","[Training Epoch 4] Batch 368, Loss 0.4924429655075073\n","[Training Epoch 4] Batch 369, Loss 0.5073103904724121\n","[Training Epoch 4] Batch 370, Loss 0.5155536532402039\n","[Training Epoch 4] Batch 371, Loss 0.5109003782272339\n","[Training Epoch 4] Batch 372, Loss 0.49286937713623047\n","[Training Epoch 4] Batch 373, Loss 0.5078705549240112\n","[Training Epoch 4] Batch 374, Loss 0.5025081038475037\n","[Training Epoch 4] Batch 375, Loss 0.5085408687591553\n","[Training Epoch 4] Batch 376, Loss 0.48612821102142334\n","[Training Epoch 4] Batch 377, Loss 0.5038310289382935\n","[Training Epoch 4] Batch 378, Loss 0.501953661441803\n","[Training Epoch 4] Batch 379, Loss 0.48592570424079895\n","[Training Epoch 4] Batch 380, Loss 0.4861412048339844\n","[Training Epoch 4] Batch 381, Loss 0.4978477358818054\n","[Training Epoch 4] Batch 382, Loss 0.5021706819534302\n","[Training Epoch 4] Batch 383, Loss 0.5247138738632202\n","[Training Epoch 4] Batch 384, Loss 0.5057008266448975\n","[Training Epoch 4] Batch 385, Loss 0.5112172365188599\n","[Training Epoch 4] Batch 386, Loss 0.5065727233886719\n","[Training Epoch 4] Batch 387, Loss 0.49882620573043823\n","[Training Epoch 4] Batch 388, Loss 0.48050248622894287\n","[Training Epoch 4] Batch 389, Loss 0.47767511010169983\n","[Training Epoch 4] Batch 390, Loss 0.47363075613975525\n","[Training Epoch 4] Batch 391, Loss 0.4977251887321472\n","[Training Epoch 4] Batch 392, Loss 0.5177930593490601\n","[Training Epoch 4] Batch 393, Loss 0.49067434668540955\n","[Training Epoch 4] Batch 394, Loss 0.5136843323707581\n","[Training Epoch 4] Batch 395, Loss 0.5391490459442139\n","[Training Epoch 4] Batch 396, Loss 0.4964720606803894\n","[Training Epoch 4] Batch 397, Loss 0.5159661173820496\n","[Training Epoch 4] Batch 398, Loss 0.5113092660903931\n","[Training Epoch 4] Batch 399, Loss 0.509615957736969\n","[Training Epoch 4] Batch 400, Loss 0.485755980014801\n","[Training Epoch 4] Batch 401, Loss 0.5398284792900085\n","[Training Epoch 4] Batch 402, Loss 0.5123912692070007\n","[Training Epoch 4] Batch 403, Loss 0.5300538539886475\n","[Training Epoch 4] Batch 404, Loss 0.5291795134544373\n","[Training Epoch 4] Batch 405, Loss 0.5155275464057922\n","[Training Epoch 4] Batch 406, Loss 0.4759165048599243\n","[Training Epoch 4] Batch 407, Loss 0.5221418142318726\n","[Training Epoch 4] Batch 408, Loss 0.49631267786026\n","[Training Epoch 4] Batch 409, Loss 0.485113263130188\n","[Training Epoch 4] Batch 410, Loss 0.4566414952278137\n","[Training Epoch 4] Batch 411, Loss 0.4889844059944153\n","[Training Epoch 4] Batch 412, Loss 0.5342167019844055\n","[Training Epoch 4] Batch 413, Loss 0.4819345474243164\n","[Training Epoch 4] Batch 414, Loss 0.5099504590034485\n","[Training Epoch 4] Batch 415, Loss 0.4813917577266693\n","[Training Epoch 4] Batch 416, Loss 0.501490592956543\n","[Training Epoch 4] Batch 417, Loss 0.5129059553146362\n","[Training Epoch 4] Batch 418, Loss 0.4928378164768219\n","[Training Epoch 4] Batch 419, Loss 0.4771431088447571\n","[Training Epoch 4] Batch 420, Loss 0.5131665468215942\n","[Training Epoch 4] Batch 421, Loss 0.5030428767204285\n","[Training Epoch 4] Batch 422, Loss 0.49429044127464294\n","[Training Epoch 4] Batch 423, Loss 0.492379367351532\n","[Training Epoch 4] Batch 424, Loss 0.5045183897018433\n","[Training Epoch 4] Batch 425, Loss 0.5328633189201355\n","[Training Epoch 4] Batch 426, Loss 0.5132825374603271\n","[Training Epoch 4] Batch 427, Loss 0.5183272361755371\n","[Training Epoch 4] Batch 428, Loss 0.5021548271179199\n","[Training Epoch 4] Batch 429, Loss 0.4883610010147095\n","[Training Epoch 4] Batch 430, Loss 0.510118842124939\n","[Training Epoch 4] Batch 431, Loss 0.5020499229431152\n","[Training Epoch 4] Batch 432, Loss 0.457652747631073\n","[Training Epoch 4] Batch 433, Loss 0.4628625214099884\n","[Training Epoch 4] Batch 434, Loss 0.5081233382225037\n","[Training Epoch 4] Batch 435, Loss 0.4716639518737793\n","[Training Epoch 4] Batch 436, Loss 0.4999070167541504\n","[Training Epoch 4] Batch 437, Loss 0.49431389570236206\n","[Training Epoch 4] Batch 438, Loss 0.5181320309638977\n","[Training Epoch 4] Batch 439, Loss 0.49813711643218994\n","[Training Epoch 4] Batch 440, Loss 0.4826420545578003\n","[Training Epoch 4] Batch 441, Loss 0.5096520781517029\n","[Training Epoch 4] Batch 442, Loss 0.5141564607620239\n","[Training Epoch 4] Batch 443, Loss 0.5141558051109314\n","[Training Epoch 4] Batch 444, Loss 0.4874112606048584\n","[Training Epoch 4] Batch 445, Loss 0.5122880339622498\n","[Training Epoch 4] Batch 446, Loss 0.4398858845233917\n","[Training Epoch 4] Batch 447, Loss 0.4939635992050171\n","[Training Epoch 4] Batch 448, Loss 0.49399247765541077\n","[Training Epoch 4] Batch 449, Loss 0.5063895583152771\n","[Training Epoch 4] Batch 450, Loss 0.49372872710227966\n","[Training Epoch 4] Batch 451, Loss 0.491098552942276\n","[Training Epoch 4] Batch 452, Loss 0.5211645364761353\n","[Training Epoch 4] Batch 453, Loss 0.49653005599975586\n","[Training Epoch 4] Batch 454, Loss 0.49786972999572754\n","[Training Epoch 4] Batch 455, Loss 0.48006200790405273\n","[Training Epoch 4] Batch 456, Loss 0.48696407675743103\n","[Training Epoch 4] Batch 457, Loss 0.48284757137298584\n","[Training Epoch 4] Batch 458, Loss 0.4920840263366699\n","[Training Epoch 4] Batch 459, Loss 0.49892351031303406\n","[Training Epoch 4] Batch 460, Loss 0.5116350650787354\n","[Training Epoch 4] Batch 461, Loss 0.5075975060462952\n","[Training Epoch 4] Batch 462, Loss 0.49507415294647217\n","[Training Epoch 4] Batch 463, Loss 0.5316491723060608\n","[Training Epoch 4] Batch 464, Loss 0.5049248933792114\n","[Training Epoch 4] Batch 465, Loss 0.5219978094100952\n","[Training Epoch 4] Batch 466, Loss 0.5303670763969421\n","[Training Epoch 4] Batch 467, Loss 0.4763164818286896\n","[Training Epoch 4] Batch 468, Loss 0.4841875731945038\n","[Training Epoch 4] Batch 469, Loss 0.4847419857978821\n","[Training Epoch 4] Batch 470, Loss 0.5102895498275757\n","[Training Epoch 4] Batch 471, Loss 0.5169706344604492\n","[Training Epoch 4] Batch 472, Loss 0.5039757490158081\n","[Training Epoch 4] Batch 473, Loss 0.5157700181007385\n","[Training Epoch 4] Batch 474, Loss 0.4936724901199341\n","[Training Epoch 4] Batch 475, Loss 0.5023132562637329\n","[Training Epoch 4] Batch 476, Loss 0.4940513074398041\n","[Training Epoch 4] Batch 477, Loss 0.505831241607666\n","[Training Epoch 4] Batch 478, Loss 0.48855745792388916\n","[Training Epoch 4] Batch 479, Loss 0.5050340294837952\n","[Training Epoch 4] Batch 480, Loss 0.48555418848991394\n","[Training Epoch 4] Batch 481, Loss 0.4826904535293579\n","[Training Epoch 4] Batch 482, Loss 0.47236743569374084\n","[Training Epoch 4] Batch 483, Loss 0.49658504128456116\n","[Training Epoch 4] Batch 484, Loss 0.5351951122283936\n","[Training Epoch 4] Batch 485, Loss 0.5167626142501831\n","[Training Epoch 4] Batch 486, Loss 0.5262700319290161\n","[Training Epoch 4] Batch 487, Loss 0.4962972402572632\n","[Training Epoch 4] Batch 488, Loss 0.4870069622993469\n","[Training Epoch 4] Batch 489, Loss 0.5101243853569031\n","[Training Epoch 4] Batch 490, Loss 0.5184434652328491\n","[Training Epoch 4] Batch 491, Loss 0.5101662278175354\n","[Training Epoch 4] Batch 492, Loss 0.4734030067920685\n","[Training Epoch 4] Batch 493, Loss 0.5205044746398926\n","[Training Epoch 4] Batch 494, Loss 0.5043004155158997\n","[Training Epoch 4] Batch 495, Loss 0.5097439289093018\n","[Training Epoch 4] Batch 496, Loss 0.518618106842041\n","[Training Epoch 4] Batch 497, Loss 0.5519681572914124\n","[Training Epoch 4] Batch 498, Loss 0.47660592198371887\n","[Training Epoch 4] Batch 499, Loss 0.5143203735351562\n","[Training Epoch 4] Batch 500, Loss 0.49332329630851746\n","[Training Epoch 4] Batch 501, Loss 0.4775847792625427\n","[Training Epoch 4] Batch 502, Loss 0.492581844329834\n","[Training Epoch 4] Batch 503, Loss 0.4950714707374573\n","[Training Epoch 4] Batch 504, Loss 0.4711112380027771\n","[Training Epoch 4] Batch 505, Loss 0.49640798568725586\n","[Training Epoch 4] Batch 506, Loss 0.4858022928237915\n","[Training Epoch 4] Batch 507, Loss 0.5152800679206848\n","[Training Epoch 4] Batch 508, Loss 0.4799310564994812\n","[Training Epoch 4] Batch 509, Loss 0.4585340917110443\n","[Training Epoch 4] Batch 510, Loss 0.5100307464599609\n","[Training Epoch 4] Batch 511, Loss 0.4742532968521118\n","[Training Epoch 4] Batch 512, Loss 0.5254100561141968\n","[Training Epoch 4] Batch 513, Loss 0.5205159187316895\n","[Training Epoch 4] Batch 514, Loss 0.4927988052368164\n","[Training Epoch 4] Batch 515, Loss 0.5234105587005615\n","[Training Epoch 4] Batch 516, Loss 0.4900217652320862\n","[Training Epoch 4] Batch 517, Loss 0.5072478652000427\n","[Training Epoch 4] Batch 518, Loss 0.4956074655056\n","[Training Epoch 4] Batch 519, Loss 0.5060757994651794\n","[Training Epoch 4] Batch 520, Loss 0.4792589247226715\n","[Training Epoch 4] Batch 521, Loss 0.5027880668640137\n","[Training Epoch 4] Batch 522, Loss 0.4850202798843384\n","[Training Epoch 4] Batch 523, Loss 0.4793716073036194\n","[Training Epoch 4] Batch 524, Loss 0.5245739221572876\n","[Training Epoch 4] Batch 525, Loss 0.5217625498771667\n","[Training Epoch 4] Batch 526, Loss 0.49826502799987793\n","[Training Epoch 4] Batch 527, Loss 0.48832857608795166\n","[Training Epoch 4] Batch 528, Loss 0.5125371217727661\n","[Training Epoch 4] Batch 529, Loss 0.5265123844146729\n","[Training Epoch 4] Batch 530, Loss 0.5087085962295532\n","[Training Epoch 4] Batch 531, Loss 0.5108463168144226\n","[Training Epoch 4] Batch 532, Loss 0.514320969581604\n","[Training Epoch 4] Batch 533, Loss 0.5035205483436584\n","[Training Epoch 4] Batch 534, Loss 0.484713077545166\n","[Training Epoch 4] Batch 535, Loss 0.5088723301887512\n","[Training Epoch 4] Batch 536, Loss 0.49909156560897827\n","[Training Epoch 4] Batch 537, Loss 0.48388031125068665\n","[Training Epoch 4] Batch 538, Loss 0.5035272836685181\n","[Training Epoch 4] Batch 539, Loss 0.47591638565063477\n","[Training Epoch 4] Batch 540, Loss 0.4989433288574219\n","[Training Epoch 4] Batch 541, Loss 0.5043816566467285\n","[Training Epoch 4] Batch 542, Loss 0.5156774520874023\n","[Training Epoch 4] Batch 543, Loss 0.520199179649353\n","[Training Epoch 4] Batch 544, Loss 0.48323673009872437\n","[Training Epoch 4] Batch 545, Loss 0.5025789737701416\n","[Training Epoch 4] Batch 546, Loss 0.473033607006073\n","[Training Epoch 4] Batch 547, Loss 0.4770490825176239\n","[Training Epoch 4] Batch 548, Loss 0.4791269600391388\n","[Training Epoch 4] Batch 549, Loss 0.4920785427093506\n","[Training Epoch 4] Batch 550, Loss 0.49528050422668457\n","[Training Epoch 4] Batch 551, Loss 0.45640990138053894\n","[Training Epoch 4] Batch 552, Loss 0.5301809310913086\n","[Training Epoch 4] Batch 553, Loss 0.4503716826438904\n","[Training Epoch 4] Batch 554, Loss 0.4954497218132019\n","[Training Epoch 4] Batch 555, Loss 0.5277124643325806\n","[Training Epoch 4] Batch 556, Loss 0.4965709149837494\n","[Training Epoch 4] Batch 557, Loss 0.5073468089103699\n","[Training Epoch 4] Batch 558, Loss 0.4947567880153656\n","[Training Epoch 4] Batch 559, Loss 0.49253377318382263\n","[Training Epoch 4] Batch 560, Loss 0.5031459927558899\n","[Training Epoch 4] Batch 561, Loss 0.5159855484962463\n","[Training Epoch 4] Batch 562, Loss 0.5049620866775513\n","[Training Epoch 4] Batch 563, Loss 0.5337022542953491\n","[Training Epoch 4] Batch 564, Loss 0.5022814273834229\n","[Training Epoch 4] Batch 565, Loss 0.4991687834262848\n","[Training Epoch 4] Batch 566, Loss 0.5319640636444092\n","[Training Epoch 4] Batch 567, Loss 0.4967292845249176\n","[Training Epoch 4] Batch 568, Loss 0.49639278650283813\n","[Training Epoch 4] Batch 569, Loss 0.49527209997177124\n","[Training Epoch 4] Batch 570, Loss 0.4999224543571472\n","[Training Epoch 4] Batch 571, Loss 0.4975966811180115\n","[Training Epoch 4] Batch 572, Loss 0.50806725025177\n","[Training Epoch 4] Batch 573, Loss 0.5065175294876099\n","[Training Epoch 4] Batch 574, Loss 0.510923445224762\n","[Training Epoch 4] Batch 575, Loss 0.5530937910079956\n","[Training Epoch 4] Batch 576, Loss 0.5038043856620789\n","[Training Epoch 4] Batch 577, Loss 0.4965490698814392\n","[Training Epoch 4] Batch 578, Loss 0.4892505407333374\n","[Training Epoch 4] Batch 579, Loss 0.5023806095123291\n","[Training Epoch 4] Batch 580, Loss 0.49917662143707275\n","[Training Epoch 4] Batch 581, Loss 0.5320125818252563\n","[Training Epoch 4] Batch 582, Loss 0.5162573456764221\n","[Training Epoch 4] Batch 583, Loss 0.4953056275844574\n","[Training Epoch 4] Batch 584, Loss 0.5017576217651367\n","[Training Epoch 4] Batch 585, Loss 0.5085294246673584\n","[Training Epoch 4] Batch 586, Loss 0.5180714130401611\n","[Training Epoch 4] Batch 587, Loss 0.48647114634513855\n","[Training Epoch 4] Batch 588, Loss 0.5217196345329285\n","[Training Epoch 4] Batch 589, Loss 0.5072096586227417\n","[Training Epoch 4] Batch 590, Loss 0.48626500368118286\n","[Training Epoch 4] Batch 591, Loss 0.48091667890548706\n","[Training Epoch 4] Batch 592, Loss 0.5134516954421997\n","[Training Epoch 4] Batch 593, Loss 0.4957408905029297\n","[Training Epoch 4] Batch 594, Loss 0.5141493082046509\n","[Training Epoch 4] Batch 595, Loss 0.4845059812068939\n","[Training Epoch 4] Batch 596, Loss 0.47851139307022095\n","[Training Epoch 4] Batch 597, Loss 0.5086867809295654\n","[Training Epoch 4] Batch 598, Loss 0.5066570043563843\n","[Training Epoch 4] Batch 599, Loss 0.49802422523498535\n","[Training Epoch 4] Batch 600, Loss 0.5046362280845642\n","[Training Epoch 4] Batch 601, Loss 0.4905420243740082\n","[Training Epoch 4] Batch 602, Loss 0.4558332562446594\n","[Training Epoch 4] Batch 603, Loss 0.5083168148994446\n","[Training Epoch 4] Batch 604, Loss 0.4995359182357788\n","[Training Epoch 4] Batch 605, Loss 0.47776150703430176\n","[Training Epoch 4] Batch 606, Loss 0.4880388379096985\n","[Training Epoch 4] Batch 607, Loss 0.4984225034713745\n","[Training Epoch 4] Batch 608, Loss 0.5197745561599731\n","[Training Epoch 4] Batch 609, Loss 0.5094737410545349\n","[Training Epoch 4] Batch 610, Loss 0.4723440110683441\n","[Training Epoch 4] Batch 611, Loss 0.5147911310195923\n","[Training Epoch 4] Batch 612, Loss 0.49428391456604004\n","[Training Epoch 4] Batch 613, Loss 0.465006947517395\n","[Training Epoch 4] Batch 614, Loss 0.48432111740112305\n","[Training Epoch 4] Batch 615, Loss 0.5085577964782715\n","[Training Epoch 4] Batch 616, Loss 0.5213406085968018\n","[Training Epoch 4] Batch 617, Loss 0.5073540806770325\n","[Training Epoch 4] Batch 618, Loss 0.4962587356567383\n","[Training Epoch 4] Batch 619, Loss 0.48474228382110596\n","[Training Epoch 4] Batch 620, Loss 0.5030581951141357\n","[Training Epoch 4] Batch 621, Loss 0.47642403841018677\n","[Training Epoch 4] Batch 622, Loss 0.456146240234375\n","[Training Epoch 4] Batch 623, Loss 0.4567301571369171\n","[Training Epoch 4] Batch 624, Loss 0.5170250535011292\n","[Training Epoch 4] Batch 625, Loss 0.499626100063324\n","[Training Epoch 4] Batch 626, Loss 0.5115819573402405\n","[Training Epoch 4] Batch 627, Loss 0.5016837120056152\n","[Training Epoch 4] Batch 628, Loss 0.49814295768737793\n","[Training Epoch 4] Batch 629, Loss 0.523769736289978\n","[Training Epoch 4] Batch 630, Loss 0.5237675309181213\n","[Training Epoch 4] Batch 631, Loss 0.5185560584068298\n","[Training Epoch 4] Batch 632, Loss 0.49677518010139465\n","[Training Epoch 4] Batch 633, Loss 0.5102435350418091\n","[Training Epoch 4] Batch 634, Loss 0.5124598741531372\n","[Training Epoch 4] Batch 635, Loss 0.5044697523117065\n","[Training Epoch 4] Batch 636, Loss 0.4981831908226013\n","[Training Epoch 4] Batch 637, Loss 0.4715588390827179\n","[Training Epoch 4] Batch 638, Loss 0.45889467000961304\n","[Training Epoch 4] Batch 639, Loss 0.5190760493278503\n","[Training Epoch 4] Batch 640, Loss 0.5281627774238586\n","[Training Epoch 4] Batch 641, Loss 0.5031083226203918\n","[Training Epoch 4] Batch 642, Loss 0.5207316875457764\n","[Training Epoch 4] Batch 643, Loss 0.4886890947818756\n","[Training Epoch 4] Batch 644, Loss 0.5029093027114868\n","[Training Epoch 4] Batch 645, Loss 0.5045292377471924\n","[Training Epoch 4] Batch 646, Loss 0.5091380476951599\n","[Training Epoch 4] Batch 647, Loss 0.5100172162055969\n","[Training Epoch 4] Batch 648, Loss 0.48025473952293396\n","[Training Epoch 4] Batch 649, Loss 0.5225357413291931\n","[Training Epoch 4] Batch 650, Loss 0.4786442220211029\n","[Training Epoch 4] Batch 651, Loss 0.5188602805137634\n","[Training Epoch 4] Batch 652, Loss 0.5179858207702637\n","[Training Epoch 4] Batch 653, Loss 0.5374603271484375\n","[Training Epoch 4] Batch 654, Loss 0.5047701597213745\n","[Training Epoch 4] Batch 655, Loss 0.5307115912437439\n","[Training Epoch 4] Batch 656, Loss 0.493585467338562\n","[Training Epoch 4] Batch 657, Loss 0.5162031054496765\n","[Training Epoch 4] Batch 658, Loss 0.5037087202072144\n","[Training Epoch 4] Batch 659, Loss 0.5109131336212158\n","[Training Epoch 4] Batch 660, Loss 0.5109490752220154\n","[Training Epoch 4] Batch 661, Loss 0.4842105209827423\n","[Training Epoch 4] Batch 662, Loss 0.5345191359519958\n","[Training Epoch 4] Batch 663, Loss 0.4780234396457672\n","[Training Epoch 4] Batch 664, Loss 0.5125676393508911\n","[Training Epoch 4] Batch 665, Loss 0.5113275647163391\n","[Training Epoch 4] Batch 666, Loss 0.5079975128173828\n","[Training Epoch 4] Batch 667, Loss 0.48073336482048035\n","[Training Epoch 4] Batch 668, Loss 0.47256147861480713\n","[Training Epoch 4] Batch 669, Loss 0.5345412492752075\n","[Training Epoch 4] Batch 670, Loss 0.4924207329750061\n","[Training Epoch 4] Batch 671, Loss 0.4906860589981079\n","[Training Epoch 4] Batch 672, Loss 0.4939819574356079\n","[Training Epoch 4] Batch 673, Loss 0.5018052458763123\n","[Training Epoch 4] Batch 674, Loss 0.4820237159729004\n","[Training Epoch 4] Batch 675, Loss 0.4794585704803467\n","[Training Epoch 4] Batch 676, Loss 0.47412073612213135\n","[Training Epoch 4] Batch 677, Loss 0.49646759033203125\n","[Training Epoch 4] Batch 678, Loss 0.5111443996429443\n","[Training Epoch 4] Batch 679, Loss 0.5001964569091797\n","[Training Epoch 4] Batch 680, Loss 0.519492506980896\n","[Training Epoch 4] Batch 681, Loss 0.5039169788360596\n","[Training Epoch 4] Batch 682, Loss 0.5077891945838928\n","[Training Epoch 4] Batch 683, Loss 0.49844130873680115\n","[Training Epoch 4] Batch 684, Loss 0.4747835397720337\n","[Training Epoch 4] Batch 685, Loss 0.5008560419082642\n","[Training Epoch 4] Batch 686, Loss 0.5230929851531982\n","[Training Epoch 4] Batch 687, Loss 0.4924583435058594\n","[Training Epoch 4] Batch 688, Loss 0.4852607846260071\n","[Training Epoch 4] Batch 689, Loss 0.5229797959327698\n","[Training Epoch 4] Batch 690, Loss 0.500589907169342\n","[Training Epoch 4] Batch 691, Loss 0.5054198503494263\n","[Training Epoch 4] Batch 692, Loss 0.5201992988586426\n","[Training Epoch 4] Batch 693, Loss 0.4644462466239929\n","[Training Epoch 4] Batch 694, Loss 0.5004711747169495\n","[Training Epoch 4] Batch 695, Loss 0.5108222365379333\n","[Training Epoch 4] Batch 696, Loss 0.4936791658401489\n","[Training Epoch 4] Batch 697, Loss 0.4920211732387543\n","[Training Epoch 4] Batch 698, Loss 0.50921630859375\n","[Training Epoch 4] Batch 699, Loss 0.515364408493042\n","[Training Epoch 4] Batch 700, Loss 0.4724889397621155\n","[Training Epoch 4] Batch 701, Loss 0.4906473755836487\n","[Training Epoch 4] Batch 702, Loss 0.5114924907684326\n","[Training Epoch 4] Batch 703, Loss 0.508591890335083\n","[Training Epoch 4] Batch 704, Loss 0.5083060264587402\n","[Training Epoch 4] Batch 705, Loss 0.49124622344970703\n","[Training Epoch 4] Batch 706, Loss 0.5229110717773438\n","[Training Epoch 4] Batch 707, Loss 0.525967001914978\n","[Training Epoch 4] Batch 708, Loss 0.5238431096076965\n","[Training Epoch 4] Batch 709, Loss 0.5146592855453491\n","[Training Epoch 4] Batch 710, Loss 0.4927331209182739\n","[Training Epoch 4] Batch 711, Loss 0.49100449681282043\n","[Training Epoch 4] Batch 712, Loss 0.49649733304977417\n","[Training Epoch 4] Batch 713, Loss 0.4790308475494385\n","[Training Epoch 4] Batch 714, Loss 0.5237753391265869\n","[Training Epoch 4] Batch 715, Loss 0.510189414024353\n","[Training Epoch 4] Batch 716, Loss 0.5265854001045227\n","[Training Epoch 4] Batch 717, Loss 0.5203678607940674\n","[Training Epoch 4] Batch 718, Loss 0.4838959574699402\n","[Training Epoch 4] Batch 719, Loss 0.534024178981781\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 4] HR = 0.0591, NDCG = 0.0304\n","Epoch 5 starts !\n","--------------------------------------------------------------------------------\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/data.py:150: DeprecationWarning: Sampling from a set deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n","[Training Epoch 5] Batch 0, Loss 0.49414363503456116\n","[Training Epoch 5] Batch 1, Loss 0.46928972005844116\n","[Training Epoch 5] Batch 2, Loss 0.49498000741004944\n","[Training Epoch 5] Batch 3, Loss 0.495055228471756\n","[Training Epoch 5] Batch 4, Loss 0.4968128800392151\n","[Training Epoch 5] Batch 5, Loss 0.499889612197876\n","[Training Epoch 5] Batch 6, Loss 0.4980166554450989\n","[Training Epoch 5] Batch 7, Loss 0.5107599496841431\n","[Training Epoch 5] Batch 8, Loss 0.5131510496139526\n","[Training Epoch 5] Batch 9, Loss 0.4656253755092621\n","[Training Epoch 5] Batch 10, Loss 0.49429988861083984\n","[Training Epoch 5] Batch 11, Loss 0.525783896446228\n","[Training Epoch 5] Batch 12, Loss 0.5096040964126587\n","[Training Epoch 5] Batch 13, Loss 0.5031481981277466\n","[Training Epoch 5] Batch 14, Loss 0.4909793436527252\n","[Training Epoch 5] Batch 15, Loss 0.4985921382904053\n","[Training Epoch 5] Batch 16, Loss 0.4821351170539856\n","[Training Epoch 5] Batch 17, Loss 0.5071507692337036\n","[Training Epoch 5] Batch 18, Loss 0.5123906135559082\n","[Training Epoch 5] Batch 19, Loss 0.4934433400630951\n","[Training Epoch 5] Batch 20, Loss 0.46557146310806274\n","[Training Epoch 5] Batch 21, Loss 0.5114045143127441\n","[Training Epoch 5] Batch 22, Loss 0.498869389295578\n","[Training Epoch 5] Batch 23, Loss 0.5167138576507568\n","[Training Epoch 5] Batch 24, Loss 0.48729026317596436\n","[Training Epoch 5] Batch 25, Loss 0.4882087707519531\n","[Training Epoch 5] Batch 26, Loss 0.5071781873703003\n","[Training Epoch 5] Batch 27, Loss 0.4987860918045044\n","[Training Epoch 5] Batch 28, Loss 0.5004438161849976\n","[Training Epoch 5] Batch 29, Loss 0.49351707100868225\n","[Training Epoch 5] Batch 30, Loss 0.49450215697288513\n","[Training Epoch 5] Batch 31, Loss 0.49081623554229736\n","[Training Epoch 5] Batch 32, Loss 0.48304325342178345\n","[Training Epoch 5] Batch 33, Loss 0.5184065103530884\n","[Training Epoch 5] Batch 34, Loss 0.5024237632751465\n","[Training Epoch 5] Batch 35, Loss 0.4965161085128784\n","[Training Epoch 5] Batch 36, Loss 0.48562341928482056\n","[Training Epoch 5] Batch 37, Loss 0.49875199794769287\n","[Training Epoch 5] Batch 38, Loss 0.48452654480934143\n","[Training Epoch 5] Batch 39, Loss 0.5024389624595642\n","[Training Epoch 5] Batch 40, Loss 0.4728994369506836\n","[Training Epoch 5] Batch 41, Loss 0.4845702052116394\n","[Training Epoch 5] Batch 42, Loss 0.5170793533325195\n","[Training Epoch 5] Batch 43, Loss 0.4844641089439392\n","[Training Epoch 5] Batch 44, Loss 0.4993472993373871\n","[Training Epoch 5] Batch 45, Loss 0.5045492649078369\n","[Training Epoch 5] Batch 46, Loss 0.49721139669418335\n","[Training Epoch 5] Batch 47, Loss 0.46660640835762024\n","[Training Epoch 5] Batch 48, Loss 0.5055450201034546\n","[Training Epoch 5] Batch 49, Loss 0.502400815486908\n","[Training Epoch 5] Batch 50, Loss 0.4864638149738312\n","[Training Epoch 5] Batch 51, Loss 0.4947456121444702\n","[Training Epoch 5] Batch 52, Loss 0.46568694710731506\n","[Training Epoch 5] Batch 53, Loss 0.477061927318573\n","[Training Epoch 5] Batch 54, Loss 0.48459234833717346\n","[Training Epoch 5] Batch 55, Loss 0.4892081618309021\n","[Training Epoch 5] Batch 56, Loss 0.4879007935523987\n","[Training Epoch 5] Batch 57, Loss 0.48832836747169495\n","[Training Epoch 5] Batch 58, Loss 0.5027850866317749\n","[Training Epoch 5] Batch 59, Loss 0.5276581048965454\n","[Training Epoch 5] Batch 60, Loss 0.5054003000259399\n","[Training Epoch 5] Batch 61, Loss 0.5296122431755066\n","[Training Epoch 5] Batch 62, Loss 0.5054345726966858\n","[Training Epoch 5] Batch 63, Loss 0.531765878200531\n","[Training Epoch 5] Batch 64, Loss 0.5395798683166504\n","[Training Epoch 5] Batch 65, Loss 0.47262078523635864\n","[Training Epoch 5] Batch 66, Loss 0.48278817534446716\n","[Training Epoch 5] Batch 67, Loss 0.5098221898078918\n","[Training Epoch 5] Batch 68, Loss 0.4857173562049866\n","[Training Epoch 5] Batch 69, Loss 0.523151159286499\n","[Training Epoch 5] Batch 70, Loss 0.5328245162963867\n","[Training Epoch 5] Batch 71, Loss 0.5028519034385681\n","[Training Epoch 5] Batch 72, Loss 0.4737354815006256\n","[Training Epoch 5] Batch 73, Loss 0.4842800796031952\n","[Training Epoch 5] Batch 74, Loss 0.4934695363044739\n","[Training Epoch 5] Batch 75, Loss 0.49459975957870483\n","[Training Epoch 5] Batch 76, Loss 0.5117551684379578\n","[Training Epoch 5] Batch 77, Loss 0.4853824973106384\n","[Training Epoch 5] Batch 78, Loss 0.5207799673080444\n","[Training Epoch 5] Batch 79, Loss 0.5043243765830994\n","[Training Epoch 5] Batch 80, Loss 0.4988901615142822\n","[Training Epoch 5] Batch 81, Loss 0.5114622116088867\n","[Training Epoch 5] Batch 82, Loss 0.4966893196105957\n","[Training Epoch 5] Batch 83, Loss 0.4604584574699402\n","[Training Epoch 5] Batch 84, Loss 0.515264630317688\n","[Training Epoch 5] Batch 85, Loss 0.5056747794151306\n","[Training Epoch 5] Batch 86, Loss 0.48819342255592346\n","[Training Epoch 5] Batch 87, Loss 0.4730910360813141\n","[Training Epoch 5] Batch 88, Loss 0.5129278302192688\n","[Training Epoch 5] Batch 89, Loss 0.5126727223396301\n","[Training Epoch 5] Batch 90, Loss 0.5408203601837158\n","[Training Epoch 5] Batch 91, Loss 0.4737893342971802\n","[Training Epoch 5] Batch 92, Loss 0.5033486485481262\n","[Training Epoch 5] Batch 93, Loss 0.5128847360610962\n","[Training Epoch 5] Batch 94, Loss 0.494967520236969\n","[Training Epoch 5] Batch 95, Loss 0.5145580768585205\n","[Training Epoch 5] Batch 96, Loss 0.501164436340332\n","[Training Epoch 5] Batch 97, Loss 0.5179428458213806\n","[Training Epoch 5] Batch 98, Loss 0.5140239596366882\n","[Training Epoch 5] Batch 99, Loss 0.48064982891082764\n","[Training Epoch 5] Batch 100, Loss 0.4963962733745575\n","[Training Epoch 5] Batch 101, Loss 0.5021046996116638\n","[Training Epoch 5] Batch 102, Loss 0.49801817536354065\n","[Training Epoch 5] Batch 103, Loss 0.5041969418525696\n","[Training Epoch 5] Batch 104, Loss 0.5396803617477417\n","[Training Epoch 5] Batch 105, Loss 0.5046371221542358\n","[Training Epoch 5] Batch 106, Loss 0.49764251708984375\n","[Training Epoch 5] Batch 107, Loss 0.5103090405464172\n","[Training Epoch 5] Batch 108, Loss 0.5021567940711975\n","[Training Epoch 5] Batch 109, Loss 0.5015811324119568\n","[Training Epoch 5] Batch 110, Loss 0.4885786175727844\n","[Training Epoch 5] Batch 111, Loss 0.4874993860721588\n","[Training Epoch 5] Batch 112, Loss 0.5182843208312988\n","[Training Epoch 5] Batch 113, Loss 0.49653977155685425\n","[Training Epoch 5] Batch 114, Loss 0.4992045760154724\n","[Training Epoch 5] Batch 115, Loss 0.49282920360565186\n","[Training Epoch 5] Batch 116, Loss 0.489959716796875\n","[Training Epoch 5] Batch 117, Loss 0.5227280855178833\n","[Training Epoch 5] Batch 118, Loss 0.5211004018783569\n","[Training Epoch 5] Batch 119, Loss 0.507951021194458\n","[Training Epoch 5] Batch 120, Loss 0.4958615303039551\n","[Training Epoch 5] Batch 121, Loss 0.5039550065994263\n","[Training Epoch 5] Batch 122, Loss 0.5277792811393738\n","[Training Epoch 5] Batch 123, Loss 0.528052031993866\n","[Training Epoch 5] Batch 124, Loss 0.484846293926239\n","[Training Epoch 5] Batch 125, Loss 0.4837767481803894\n","[Training Epoch 5] Batch 126, Loss 0.47237521409988403\n","[Training Epoch 5] Batch 127, Loss 0.4882861077785492\n","[Training Epoch 5] Batch 128, Loss 0.5112327933311462\n","[Training Epoch 5] Batch 129, Loss 0.48598799109458923\n","[Training Epoch 5] Batch 130, Loss 0.5067306756973267\n","[Training Epoch 5] Batch 131, Loss 0.47524404525756836\n","[Training Epoch 5] Batch 132, Loss 0.5215989947319031\n","[Training Epoch 5] Batch 133, Loss 0.501579761505127\n","[Training Epoch 5] Batch 134, Loss 0.501807689666748\n","[Training Epoch 5] Batch 135, Loss 0.48516568541526794\n","[Training Epoch 5] Batch 136, Loss 0.4893876314163208\n","[Training Epoch 5] Batch 137, Loss 0.49581438302993774\n","[Training Epoch 5] Batch 138, Loss 0.4856710433959961\n","[Training Epoch 5] Batch 139, Loss 0.4793001413345337\n","[Training Epoch 5] Batch 140, Loss 0.4819561839103699\n","[Training Epoch 5] Batch 141, Loss 0.5026336908340454\n","[Training Epoch 5] Batch 142, Loss 0.4860915541648865\n","[Training Epoch 5] Batch 143, Loss 0.5040918588638306\n","[Training Epoch 5] Batch 144, Loss 0.488694965839386\n","[Training Epoch 5] Batch 145, Loss 0.5145143270492554\n","[Training Epoch 5] Batch 146, Loss 0.48522964119911194\n","[Training Epoch 5] Batch 147, Loss 0.49664831161499023\n","[Training Epoch 5] Batch 148, Loss 0.5213850736618042\n","[Training Epoch 5] Batch 149, Loss 0.5134009122848511\n","[Training Epoch 5] Batch 150, Loss 0.4988436698913574\n","[Training Epoch 5] Batch 151, Loss 0.5108613967895508\n","[Training Epoch 5] Batch 152, Loss 0.5228742957115173\n","[Training Epoch 5] Batch 153, Loss 0.5238263010978699\n","[Training Epoch 5] Batch 154, Loss 0.4933472275733948\n","[Training Epoch 5] Batch 155, Loss 0.5003950595855713\n","[Training Epoch 5] Batch 156, Loss 0.5139042139053345\n","[Training Epoch 5] Batch 157, Loss 0.47652238607406616\n","[Training Epoch 5] Batch 158, Loss 0.4923172891139984\n","[Training Epoch 5] Batch 159, Loss 0.508515477180481\n","[Training Epoch 5] Batch 160, Loss 0.5156787633895874\n","[Training Epoch 5] Batch 161, Loss 0.4953678846359253\n","[Training Epoch 5] Batch 162, Loss 0.4995015859603882\n","[Training Epoch 5] Batch 163, Loss 0.4918399453163147\n","[Training Epoch 5] Batch 164, Loss 0.49120718240737915\n","[Training Epoch 5] Batch 165, Loss 0.474540650844574\n","[Training Epoch 5] Batch 166, Loss 0.5070385932922363\n","[Training Epoch 5] Batch 167, Loss 0.47592219710350037\n","[Training Epoch 5] Batch 168, Loss 0.48437342047691345\n","[Training Epoch 5] Batch 169, Loss 0.49245309829711914\n","[Training Epoch 5] Batch 170, Loss 0.4982200562953949\n","[Training Epoch 5] Batch 171, Loss 0.48356395959854126\n","[Training Epoch 5] Batch 172, Loss 0.4936567544937134\n","[Training Epoch 5] Batch 173, Loss 0.5070457458496094\n","[Training Epoch 5] Batch 174, Loss 0.4830063581466675\n","[Training Epoch 5] Batch 175, Loss 0.50856614112854\n","[Training Epoch 5] Batch 176, Loss 0.4761354923248291\n","[Training Epoch 5] Batch 177, Loss 0.5049190521240234\n","[Training Epoch 5] Batch 178, Loss 0.4819108247756958\n","[Training Epoch 5] Batch 179, Loss 0.4509482979774475\n","[Training Epoch 5] Batch 180, Loss 0.5042750835418701\n","[Training Epoch 5] Batch 181, Loss 0.4891664683818817\n","[Training Epoch 5] Batch 182, Loss 0.5078781247138977\n","[Training Epoch 5] Batch 183, Loss 0.4833855628967285\n","[Training Epoch 5] Batch 184, Loss 0.4753361642360687\n","[Training Epoch 5] Batch 185, Loss 0.5102333426475525\n","[Training Epoch 5] Batch 186, Loss 0.5360568165779114\n","[Training Epoch 5] Batch 187, Loss 0.49549034237861633\n","[Training Epoch 5] Batch 188, Loss 0.4916371703147888\n","[Training Epoch 5] Batch 189, Loss 0.5091445446014404\n","[Training Epoch 5] Batch 190, Loss 0.5088472366333008\n","[Training Epoch 5] Batch 191, Loss 0.501144289970398\n","[Training Epoch 5] Batch 192, Loss 0.5091444253921509\n","[Training Epoch 5] Batch 193, Loss 0.4784373939037323\n","[Training Epoch 5] Batch 194, Loss 0.5247776508331299\n","[Training Epoch 5] Batch 195, Loss 0.4832611680030823\n","[Training Epoch 5] Batch 196, Loss 0.4904906451702118\n","[Training Epoch 5] Batch 197, Loss 0.4895610213279724\n","[Training Epoch 5] Batch 198, Loss 0.47028690576553345\n","[Training Epoch 5] Batch 199, Loss 0.5100773572921753\n","[Training Epoch 5] Batch 200, Loss 0.47174447774887085\n","[Training Epoch 5] Batch 201, Loss 0.5004639625549316\n","[Training Epoch 5] Batch 202, Loss 0.5033183097839355\n","[Training Epoch 5] Batch 203, Loss 0.49683597683906555\n","[Training Epoch 5] Batch 204, Loss 0.49261897802352905\n","[Training Epoch 5] Batch 205, Loss 0.48059022426605225\n","[Training Epoch 5] Batch 206, Loss 0.49075716733932495\n","[Training Epoch 5] Batch 207, Loss 0.5036647319793701\n","[Training Epoch 5] Batch 208, Loss 0.5008606910705566\n","[Training Epoch 5] Batch 209, Loss 0.49644386768341064\n","[Training Epoch 5] Batch 210, Loss 0.47849923372268677\n","[Training Epoch 5] Batch 211, Loss 0.4996669292449951\n","[Training Epoch 5] Batch 212, Loss 0.49612146615982056\n","[Training Epoch 5] Batch 213, Loss 0.4843764901161194\n","[Training Epoch 5] Batch 214, Loss 0.5179768800735474\n","[Training Epoch 5] Batch 215, Loss 0.4893081784248352\n","[Training Epoch 5] Batch 216, Loss 0.47584620118141174\n","[Training Epoch 5] Batch 217, Loss 0.4848863184452057\n","[Training Epoch 5] Batch 218, Loss 0.5224651098251343\n","[Training Epoch 5] Batch 219, Loss 0.5158271193504333\n","[Training Epoch 5] Batch 220, Loss 0.5101086497306824\n","[Training Epoch 5] Batch 221, Loss 0.5211641788482666\n","[Training Epoch 5] Batch 222, Loss 0.489807665348053\n","[Training Epoch 5] Batch 223, Loss 0.5240069627761841\n","[Training Epoch 5] Batch 224, Loss 0.49531689286231995\n","[Training Epoch 5] Batch 225, Loss 0.5187053680419922\n","[Training Epoch 5] Batch 226, Loss 0.4843578338623047\n","[Training Epoch 5] Batch 227, Loss 0.502686619758606\n","[Training Epoch 5] Batch 228, Loss 0.5178690552711487\n","[Training Epoch 5] Batch 229, Loss 0.495735764503479\n","[Training Epoch 5] Batch 230, Loss 0.5048491358757019\n","[Training Epoch 5] Batch 231, Loss 0.5066497325897217\n","[Training Epoch 5] Batch 232, Loss 0.46967825293540955\n","[Training Epoch 5] Batch 233, Loss 0.5016738176345825\n","[Training Epoch 5] Batch 234, Loss 0.514475405216217\n","[Training Epoch 5] Batch 235, Loss 0.4953014552593231\n","[Training Epoch 5] Batch 236, Loss 0.5098752975463867\n","[Training Epoch 5] Batch 237, Loss 0.522346019744873\n","[Training Epoch 5] Batch 238, Loss 0.5250657200813293\n","[Training Epoch 5] Batch 239, Loss 0.5018399953842163\n","[Training Epoch 5] Batch 240, Loss 0.479076087474823\n","[Training Epoch 5] Batch 241, Loss 0.48203063011169434\n","[Training Epoch 5] Batch 242, Loss 0.4917069971561432\n","[Training Epoch 5] Batch 243, Loss 0.5021316409111023\n","[Training Epoch 5] Batch 244, Loss 0.46354448795318604\n","[Training Epoch 5] Batch 245, Loss 0.5180572271347046\n","[Training Epoch 5] Batch 246, Loss 0.5144014358520508\n","[Training Epoch 5] Batch 247, Loss 0.492239773273468\n","[Training Epoch 5] Batch 248, Loss 0.469967782497406\n","[Training Epoch 5] Batch 249, Loss 0.510805606842041\n","[Training Epoch 5] Batch 250, Loss 0.49894851446151733\n","[Training Epoch 5] Batch 251, Loss 0.5293928384780884\n","[Training Epoch 5] Batch 252, Loss 0.5191952586174011\n","[Training Epoch 5] Batch 253, Loss 0.4906059503555298\n","[Training Epoch 5] Batch 254, Loss 0.47929930686950684\n","[Training Epoch 5] Batch 255, Loss 0.5188794136047363\n","[Training Epoch 5] Batch 256, Loss 0.48599234223365784\n","[Training Epoch 5] Batch 257, Loss 0.5237504243850708\n","[Training Epoch 5] Batch 258, Loss 0.5053433179855347\n","[Training Epoch 5] Batch 259, Loss 0.49877819418907166\n","[Training Epoch 5] Batch 260, Loss 0.45498883724212646\n","[Training Epoch 5] Batch 261, Loss 0.47461169958114624\n","[Training Epoch 5] Batch 262, Loss 0.5119722485542297\n","[Training Epoch 5] Batch 263, Loss 0.4850809574127197\n","[Training Epoch 5] Batch 264, Loss 0.5308775901794434\n","[Training Epoch 5] Batch 265, Loss 0.5044174194335938\n","[Training Epoch 5] Batch 266, Loss 0.5161591172218323\n","[Training Epoch 5] Batch 267, Loss 0.5042052865028381\n","[Training Epoch 5] Batch 268, Loss 0.49129965901374817\n","[Training Epoch 5] Batch 269, Loss 0.5241748094558716\n","[Training Epoch 5] Batch 270, Loss 0.5107936859130859\n","[Training Epoch 5] Batch 271, Loss 0.5228356122970581\n","[Training Epoch 5] Batch 272, Loss 0.5122485160827637\n","[Training Epoch 5] Batch 273, Loss 0.5024742484092712\n","[Training Epoch 5] Batch 274, Loss 0.5051179528236389\n","[Training Epoch 5] Batch 275, Loss 0.5031218528747559\n","[Training Epoch 5] Batch 276, Loss 0.5188138484954834\n","[Training Epoch 5] Batch 277, Loss 0.5455083847045898\n","[Training Epoch 5] Batch 278, Loss 0.48796898126602173\n","[Training Epoch 5] Batch 279, Loss 0.5094355344772339\n","[Training Epoch 5] Batch 280, Loss 0.4624450206756592\n","[Training Epoch 5] Batch 281, Loss 0.5000568628311157\n","[Training Epoch 5] Batch 282, Loss 0.520980954170227\n","[Training Epoch 5] Batch 283, Loss 0.4949077367782593\n","[Training Epoch 5] Batch 284, Loss 0.48216354846954346\n","[Training Epoch 5] Batch 285, Loss 0.5025383234024048\n","[Training Epoch 5] Batch 286, Loss 0.4841192066669464\n","[Training Epoch 5] Batch 287, Loss 0.4935689866542816\n","[Training Epoch 5] Batch 288, Loss 0.5262240171432495\n","[Training Epoch 5] Batch 289, Loss 0.5104414224624634\n","[Training Epoch 5] Batch 290, Loss 0.49329742789268494\n","[Training Epoch 5] Batch 291, Loss 0.4863833785057068\n","[Training Epoch 5] Batch 292, Loss 0.5509309768676758\n","[Training Epoch 5] Batch 293, Loss 0.46417224407196045\n","[Training Epoch 5] Batch 294, Loss 0.5243910551071167\n","[Training Epoch 5] Batch 295, Loss 0.5006662011146545\n","[Training Epoch 5] Batch 296, Loss 0.5434722900390625\n","[Training Epoch 5] Batch 297, Loss 0.46341896057128906\n","[Training Epoch 5] Batch 298, Loss 0.4962168335914612\n","[Training Epoch 5] Batch 299, Loss 0.49323177337646484\n","[Training Epoch 5] Batch 300, Loss 0.4987245798110962\n","[Training Epoch 5] Batch 301, Loss 0.5054977536201477\n","[Training Epoch 5] Batch 302, Loss 0.49860137701034546\n","[Training Epoch 5] Batch 303, Loss 0.494404137134552\n","[Training Epoch 5] Batch 304, Loss 0.5059192180633545\n","[Training Epoch 5] Batch 305, Loss 0.5117910504341125\n","[Training Epoch 5] Batch 306, Loss 0.5096213221549988\n","[Training Epoch 5] Batch 307, Loss 0.506193220615387\n","[Training Epoch 5] Batch 308, Loss 0.5393340587615967\n","[Training Epoch 5] Batch 309, Loss 0.51481032371521\n","[Training Epoch 5] Batch 310, Loss 0.5143985748291016\n","[Training Epoch 5] Batch 311, Loss 0.4896145761013031\n","[Training Epoch 5] Batch 312, Loss 0.5003480315208435\n","[Training Epoch 5] Batch 313, Loss 0.4969411790370941\n","[Training Epoch 5] Batch 314, Loss 0.48824116587638855\n","[Training Epoch 5] Batch 315, Loss 0.5298886299133301\n","[Training Epoch 5] Batch 316, Loss 0.5085619688034058\n","[Training Epoch 5] Batch 317, Loss 0.490678071975708\n","[Training Epoch 5] Batch 318, Loss 0.5092161893844604\n","[Training Epoch 5] Batch 319, Loss 0.5112934708595276\n","[Training Epoch 5] Batch 320, Loss 0.47640490531921387\n","[Training Epoch 5] Batch 321, Loss 0.5305494070053101\n","[Training Epoch 5] Batch 322, Loss 0.49305668473243713\n","[Training Epoch 5] Batch 323, Loss 0.4988649785518646\n","[Training Epoch 5] Batch 324, Loss 0.5187333822250366\n","[Training Epoch 5] Batch 325, Loss 0.4852922558784485\n","[Training Epoch 5] Batch 326, Loss 0.48826679587364197\n","[Training Epoch 5] Batch 327, Loss 0.49891048669815063\n","[Training Epoch 5] Batch 328, Loss 0.5011841654777527\n","[Training Epoch 5] Batch 329, Loss 0.5070916414260864\n","[Training Epoch 5] Batch 330, Loss 0.5070773959159851\n","[Training Epoch 5] Batch 331, Loss 0.471638023853302\n","[Training Epoch 5] Batch 332, Loss 0.4922545254230499\n","[Training Epoch 5] Batch 333, Loss 0.5049768090248108\n","[Training Epoch 5] Batch 334, Loss 0.46868467330932617\n","[Training Epoch 5] Batch 335, Loss 0.4625941514968872\n","[Training Epoch 5] Batch 336, Loss 0.5150133967399597\n","[Training Epoch 5] Batch 337, Loss 0.5262315273284912\n","[Training Epoch 5] Batch 338, Loss 0.48912644386291504\n","[Training Epoch 5] Batch 339, Loss 0.5098812580108643\n","[Training Epoch 5] Batch 340, Loss 0.5223233103752136\n","[Training Epoch 5] Batch 341, Loss 0.47975271940231323\n","[Training Epoch 5] Batch 342, Loss 0.4780092239379883\n","[Training Epoch 5] Batch 343, Loss 0.5046254396438599\n","[Training Epoch 5] Batch 344, Loss 0.5056799054145813\n","[Training Epoch 5] Batch 345, Loss 0.47272399067878723\n","[Training Epoch 5] Batch 346, Loss 0.49701249599456787\n","[Training Epoch 5] Batch 347, Loss 0.48032981157302856\n","[Training Epoch 5] Batch 348, Loss 0.49183645844459534\n","[Training Epoch 5] Batch 349, Loss 0.4843199849128723\n","[Training Epoch 5] Batch 350, Loss 0.49567440152168274\n","[Training Epoch 5] Batch 351, Loss 0.5149976015090942\n","[Training Epoch 5] Batch 352, Loss 0.4956468343734741\n","[Training Epoch 5] Batch 353, Loss 0.4943237602710724\n","[Training Epoch 5] Batch 354, Loss 0.5060750246047974\n","[Training Epoch 5] Batch 355, Loss 0.5286130309104919\n","[Training Epoch 5] Batch 356, Loss 0.4975337088108063\n","[Training Epoch 5] Batch 357, Loss 0.5212480425834656\n","[Training Epoch 5] Batch 358, Loss 0.4853047728538513\n","[Training Epoch 5] Batch 359, Loss 0.5054054856300354\n","[Training Epoch 5] Batch 360, Loss 0.49651089310646057\n","[Training Epoch 5] Batch 361, Loss 0.5150179862976074\n","[Training Epoch 5] Batch 362, Loss 0.49950629472732544\n","[Training Epoch 5] Batch 363, Loss 0.5175817012786865\n","[Training Epoch 5] Batch 364, Loss 0.48877719044685364\n","[Training Epoch 5] Batch 365, Loss 0.504489004611969\n","[Training Epoch 5] Batch 366, Loss 0.5102284550666809\n","[Training Epoch 5] Batch 367, Loss 0.4994083642959595\n","[Training Epoch 5] Batch 368, Loss 0.4970960021018982\n","[Training Epoch 5] Batch 369, Loss 0.4997953176498413\n","[Training Epoch 5] Batch 370, Loss 0.5171231627464294\n","[Training Epoch 5] Batch 371, Loss 0.4870966672897339\n","[Training Epoch 5] Batch 372, Loss 0.49324166774749756\n","[Training Epoch 5] Batch 373, Loss 0.5221078395843506\n","[Training Epoch 5] Batch 374, Loss 0.5276084542274475\n","[Training Epoch 5] Batch 375, Loss 0.5075955390930176\n","[Training Epoch 5] Batch 376, Loss 0.5003458261489868\n","[Training Epoch 5] Batch 377, Loss 0.4938206076622009\n","[Training Epoch 5] Batch 378, Loss 0.48394832015037537\n","[Training Epoch 5] Batch 379, Loss 0.48804664611816406\n","[Training Epoch 5] Batch 380, Loss 0.5125818252563477\n","[Training Epoch 5] Batch 381, Loss 0.4998270273208618\n","[Training Epoch 5] Batch 382, Loss 0.5168495774269104\n","[Training Epoch 5] Batch 383, Loss 0.5238615870475769\n","[Training Epoch 5] Batch 384, Loss 0.5081824660301208\n","[Training Epoch 5] Batch 385, Loss 0.48279622197151184\n","[Training Epoch 5] Batch 386, Loss 0.4903581738471985\n","[Training Epoch 5] Batch 387, Loss 0.47985121607780457\n","[Training Epoch 5] Batch 388, Loss 0.48760008811950684\n","[Training Epoch 5] Batch 389, Loss 0.5292930603027344\n","[Training Epoch 5] Batch 390, Loss 0.49699902534484863\n","[Training Epoch 5] Batch 391, Loss 0.5081776976585388\n","[Training Epoch 5] Batch 392, Loss 0.5065903663635254\n","[Training Epoch 5] Batch 393, Loss 0.5103108882904053\n","[Training Epoch 5] Batch 394, Loss 0.5015592575073242\n","[Training Epoch 5] Batch 395, Loss 0.5071176886558533\n","[Training Epoch 5] Batch 396, Loss 0.5082314014434814\n","[Training Epoch 5] Batch 397, Loss 0.4862053692340851\n","[Training Epoch 5] Batch 398, Loss 0.5178424715995789\n","[Training Epoch 5] Batch 399, Loss 0.5167118310928345\n","[Training Epoch 5] Batch 400, Loss 0.5274196863174438\n","[Training Epoch 5] Batch 401, Loss 0.49238741397857666\n","[Training Epoch 5] Batch 402, Loss 0.503079891204834\n","[Training Epoch 5] Batch 403, Loss 0.49856507778167725\n","[Training Epoch 5] Batch 404, Loss 0.501783549785614\n","[Training Epoch 5] Batch 405, Loss 0.4684292674064636\n","[Training Epoch 5] Batch 406, Loss 0.5032258629798889\n","[Training Epoch 5] Batch 407, Loss 0.4939611554145813\n","[Training Epoch 5] Batch 408, Loss 0.4920254349708557\n","[Training Epoch 5] Batch 409, Loss 0.5038639903068542\n","[Training Epoch 5] Batch 410, Loss 0.526505708694458\n","[Training Epoch 5] Batch 411, Loss 0.49234089255332947\n","[Training Epoch 5] Batch 412, Loss 0.5102526545524597\n","[Training Epoch 5] Batch 413, Loss 0.47103312611579895\n","[Training Epoch 5] Batch 414, Loss 0.49216023087501526\n","[Training Epoch 5] Batch 415, Loss 0.5457119941711426\n","[Training Epoch 5] Batch 416, Loss 0.5110753774642944\n","[Training Epoch 5] Batch 417, Loss 0.5134255290031433\n","[Training Epoch 5] Batch 418, Loss 0.5038024187088013\n","[Training Epoch 5] Batch 419, Loss 0.4886956214904785\n","[Training Epoch 5] Batch 420, Loss 0.5191161632537842\n","[Training Epoch 5] Batch 421, Loss 0.49541953206062317\n","[Training Epoch 5] Batch 422, Loss 0.5118703842163086\n","[Training Epoch 5] Batch 423, Loss 0.5193535089492798\n","[Training Epoch 5] Batch 424, Loss 0.5205432176589966\n","[Training Epoch 5] Batch 425, Loss 0.5133851170539856\n","[Training Epoch 5] Batch 426, Loss 0.5076150894165039\n","[Training Epoch 5] Batch 427, Loss 0.4957778751850128\n","[Training Epoch 5] Batch 428, Loss 0.5360244512557983\n","[Training Epoch 5] Batch 429, Loss 0.47464415431022644\n","[Training Epoch 5] Batch 430, Loss 0.5056412220001221\n","[Training Epoch 5] Batch 431, Loss 0.5102838277816772\n","[Training Epoch 5] Batch 432, Loss 0.4817968010902405\n","[Training Epoch 5] Batch 433, Loss 0.5193965435028076\n","[Training Epoch 5] Batch 434, Loss 0.5142090320587158\n","[Training Epoch 5] Batch 435, Loss 0.5184201002120972\n","[Training Epoch 5] Batch 436, Loss 0.4914644956588745\n","[Training Epoch 5] Batch 437, Loss 0.49268999695777893\n","[Training Epoch 5] Batch 438, Loss 0.4824528098106384\n","[Training Epoch 5] Batch 439, Loss 0.48900333046913147\n","[Training Epoch 5] Batch 440, Loss 0.49365314841270447\n","[Training Epoch 5] Batch 441, Loss 0.5170505046844482\n","[Training Epoch 5] Batch 442, Loss 0.4896911084651947\n","[Training Epoch 5] Batch 443, Loss 0.48475533723831177\n","[Training Epoch 5] Batch 444, Loss 0.4993334114551544\n","[Training Epoch 5] Batch 445, Loss 0.4973341226577759\n","[Training Epoch 5] Batch 446, Loss 0.5016834139823914\n","[Training Epoch 5] Batch 447, Loss 0.48627912998199463\n","[Training Epoch 5] Batch 448, Loss 0.5024166107177734\n","[Training Epoch 5] Batch 449, Loss 0.4966928958892822\n","[Training Epoch 5] Batch 450, Loss 0.4822463095188141\n","[Training Epoch 5] Batch 451, Loss 0.526383101940155\n","[Training Epoch 5] Batch 452, Loss 0.45145300030708313\n","[Training Epoch 5] Batch 453, Loss 0.4971584379673004\n","[Training Epoch 5] Batch 454, Loss 0.505585789680481\n","[Training Epoch 5] Batch 455, Loss 0.4925858676433563\n","[Training Epoch 5] Batch 456, Loss 0.48283299803733826\n","[Training Epoch 5] Batch 457, Loss 0.5265630483627319\n","[Training Epoch 5] Batch 458, Loss 0.49697038531303406\n","[Training Epoch 5] Batch 459, Loss 0.5012520551681519\n","[Training Epoch 5] Batch 460, Loss 0.49043935537338257\n","[Training Epoch 5] Batch 461, Loss 0.4916914105415344\n","[Training Epoch 5] Batch 462, Loss 0.4925568103790283\n","[Training Epoch 5] Batch 463, Loss 0.468186616897583\n","[Training Epoch 5] Batch 464, Loss 0.49378371238708496\n","[Training Epoch 5] Batch 465, Loss 0.5076882839202881\n","[Training Epoch 5] Batch 466, Loss 0.4928259253501892\n","[Training Epoch 5] Batch 467, Loss 0.48829567432403564\n","[Training Epoch 5] Batch 468, Loss 0.4861539602279663\n","[Training Epoch 5] Batch 469, Loss 0.5079127550125122\n","[Training Epoch 5] Batch 470, Loss 0.5140035152435303\n","[Training Epoch 5] Batch 471, Loss 0.5111516714096069\n","[Training Epoch 5] Batch 472, Loss 0.4981284737586975\n","[Training Epoch 5] Batch 473, Loss 0.4863520860671997\n","[Training Epoch 5] Batch 474, Loss 0.48728466033935547\n","[Training Epoch 5] Batch 475, Loss 0.49190613627433777\n","[Training Epoch 5] Batch 476, Loss 0.5247534513473511\n","[Training Epoch 5] Batch 477, Loss 0.49875277280807495\n","[Training Epoch 5] Batch 478, Loss 0.47586631774902344\n","[Training Epoch 5] Batch 479, Loss 0.5268812775611877\n","[Training Epoch 5] Batch 480, Loss 0.4978225827217102\n","[Training Epoch 5] Batch 481, Loss 0.5115966796875\n","[Training Epoch 5] Batch 482, Loss 0.5145286321640015\n","[Training Epoch 5] Batch 483, Loss 0.4972662031650543\n","[Training Epoch 5] Batch 484, Loss 0.5226560831069946\n","[Training Epoch 5] Batch 485, Loss 0.5056746006011963\n","[Training Epoch 5] Batch 486, Loss 0.49438410997390747\n","[Training Epoch 5] Batch 487, Loss 0.48157721757888794\n","[Training Epoch 5] Batch 488, Loss 0.4761921167373657\n","[Training Epoch 5] Batch 489, Loss 0.5337077379226685\n","[Training Epoch 5] Batch 490, Loss 0.49297186732292175\n","[Training Epoch 5] Batch 491, Loss 0.49922096729278564\n","[Training Epoch 5] Batch 492, Loss 0.5136086940765381\n","[Training Epoch 5] Batch 493, Loss 0.5172284841537476\n","[Training Epoch 5] Batch 494, Loss 0.5125015377998352\n","[Training Epoch 5] Batch 495, Loss 0.5263490676879883\n","[Training Epoch 5] Batch 496, Loss 0.4825919568538666\n","[Training Epoch 5] Batch 497, Loss 0.49597370624542236\n","[Training Epoch 5] Batch 498, Loss 0.50880366563797\n","[Training Epoch 5] Batch 499, Loss 0.45391011238098145\n","[Training Epoch 5] Batch 500, Loss 0.5275270938873291\n","[Training Epoch 5] Batch 501, Loss 0.4970983862876892\n","[Training Epoch 5] Batch 502, Loss 0.48009324073791504\n","[Training Epoch 5] Batch 503, Loss 0.5167157053947449\n","[Training Epoch 5] Batch 504, Loss 0.5038648247718811\n","[Training Epoch 5] Batch 505, Loss 0.4760684669017792\n","[Training Epoch 5] Batch 506, Loss 0.5183008313179016\n","[Training Epoch 5] Batch 507, Loss 0.506696879863739\n","[Training Epoch 5] Batch 508, Loss 0.5008299350738525\n","[Training Epoch 5] Batch 509, Loss 0.5079938173294067\n","[Training Epoch 5] Batch 510, Loss 0.5157947540283203\n","[Training Epoch 5] Batch 511, Loss 0.49818867444992065\n","[Training Epoch 5] Batch 512, Loss 0.5170712471008301\n","[Training Epoch 5] Batch 513, Loss 0.4789634048938751\n","[Training Epoch 5] Batch 514, Loss 0.5235324501991272\n","[Training Epoch 5] Batch 515, Loss 0.5118269324302673\n","[Training Epoch 5] Batch 516, Loss 0.4974023103713989\n","[Training Epoch 5] Batch 517, Loss 0.48902156949043274\n","[Training Epoch 5] Batch 518, Loss 0.497997522354126\n","[Training Epoch 5] Batch 519, Loss 0.5088451504707336\n","[Training Epoch 5] Batch 520, Loss 0.4839511215686798\n","[Training Epoch 5] Batch 521, Loss 0.49844053387641907\n","[Training Epoch 5] Batch 522, Loss 0.447721004486084\n","[Training Epoch 5] Batch 523, Loss 0.5023744702339172\n","[Training Epoch 5] Batch 524, Loss 0.5177114009857178\n","[Training Epoch 5] Batch 525, Loss 0.484721302986145\n","[Training Epoch 5] Batch 526, Loss 0.49080583453178406\n","[Training Epoch 5] Batch 527, Loss 0.49201154708862305\n","[Training Epoch 5] Batch 528, Loss 0.5190130472183228\n","[Training Epoch 5] Batch 529, Loss 0.5071567893028259\n","[Training Epoch 5] Batch 530, Loss 0.5443583726882935\n","[Training Epoch 5] Batch 531, Loss 0.5070916414260864\n","[Training Epoch 5] Batch 532, Loss 0.5033893585205078\n","[Training Epoch 5] Batch 533, Loss 0.5071401000022888\n","[Training Epoch 5] Batch 534, Loss 0.5006433725357056\n","[Training Epoch 5] Batch 535, Loss 0.4894716739654541\n","[Training Epoch 5] Batch 536, Loss 0.5084062218666077\n","[Training Epoch 5] Batch 537, Loss 0.49111342430114746\n","[Training Epoch 5] Batch 538, Loss 0.49708467721939087\n","[Training Epoch 5] Batch 539, Loss 0.5294849872589111\n","[Training Epoch 5] Batch 540, Loss 0.4881696105003357\n","[Training Epoch 5] Batch 541, Loss 0.488466739654541\n","[Training Epoch 5] Batch 542, Loss 0.5011900663375854\n","[Training Epoch 5] Batch 543, Loss 0.4955565333366394\n","[Training Epoch 5] Batch 544, Loss 0.5048872232437134\n","[Training Epoch 5] Batch 545, Loss 0.4620770514011383\n","[Training Epoch 5] Batch 546, Loss 0.5202035903930664\n","[Training Epoch 5] Batch 547, Loss 0.4868805408477783\n","[Training Epoch 5] Batch 548, Loss 0.5065233707427979\n","[Training Epoch 5] Batch 549, Loss 0.5309163331985474\n","[Training Epoch 5] Batch 550, Loss 0.5013189315795898\n","[Training Epoch 5] Batch 551, Loss 0.5290277004241943\n","[Training Epoch 5] Batch 552, Loss 0.5366266369819641\n","[Training Epoch 5] Batch 553, Loss 0.49541428685188293\n","[Training Epoch 5] Batch 554, Loss 0.48206159472465515\n","[Training Epoch 5] Batch 555, Loss 0.4876844882965088\n","[Training Epoch 5] Batch 556, Loss 0.4997698962688446\n","[Training Epoch 5] Batch 557, Loss 0.4732462167739868\n","[Training Epoch 5] Batch 558, Loss 0.49126631021499634\n","[Training Epoch 5] Batch 559, Loss 0.49075940251350403\n","[Training Epoch 5] Batch 560, Loss 0.5486879348754883\n","[Training Epoch 5] Batch 561, Loss 0.4678599536418915\n","[Training Epoch 5] Batch 562, Loss 0.49134561419487\n","[Training Epoch 5] Batch 563, Loss 0.4950208067893982\n","[Training Epoch 5] Batch 564, Loss 0.48874717950820923\n","[Training Epoch 5] Batch 565, Loss 0.5110714435577393\n","[Training Epoch 5] Batch 566, Loss 0.5056118965148926\n","[Training Epoch 5] Batch 567, Loss 0.5133702754974365\n","[Training Epoch 5] Batch 568, Loss 0.539049506187439\n","[Training Epoch 5] Batch 569, Loss 0.49080604314804077\n","[Training Epoch 5] Batch 570, Loss 0.5065877437591553\n","[Training Epoch 5] Batch 571, Loss 0.5043950080871582\n","[Training Epoch 5] Batch 572, Loss 0.512550950050354\n","[Training Epoch 5] Batch 573, Loss 0.49149489402770996\n","[Training Epoch 5] Batch 574, Loss 0.4826216995716095\n","[Training Epoch 5] Batch 575, Loss 0.4820398986339569\n","[Training Epoch 5] Batch 576, Loss 0.5189245343208313\n","[Training Epoch 5] Batch 577, Loss 0.5381003618240356\n","[Training Epoch 5] Batch 578, Loss 0.4983411431312561\n","[Training Epoch 5] Batch 579, Loss 0.5237421989440918\n","[Training Epoch 5] Batch 580, Loss 0.507054328918457\n","[Training Epoch 5] Batch 581, Loss 0.4621003270149231\n","[Training Epoch 5] Batch 582, Loss 0.4919646084308624\n","[Training Epoch 5] Batch 583, Loss 0.4918830692768097\n","[Training Epoch 5] Batch 584, Loss 0.49985888600349426\n","[Training Epoch 5] Batch 585, Loss 0.49590402841567993\n","[Training Epoch 5] Batch 586, Loss 0.5030321478843689\n","[Training Epoch 5] Batch 587, Loss 0.501264750957489\n","[Training Epoch 5] Batch 588, Loss 0.5095247030258179\n","[Training Epoch 5] Batch 589, Loss 0.4949641823768616\n","[Training Epoch 5] Batch 590, Loss 0.4930759072303772\n","[Training Epoch 5] Batch 591, Loss 0.47249871492385864\n","[Training Epoch 5] Batch 592, Loss 0.4991510808467865\n","[Training Epoch 5] Batch 593, Loss 0.5083909630775452\n","[Training Epoch 5] Batch 594, Loss 0.4997629225254059\n","[Training Epoch 5] Batch 595, Loss 0.5257811546325684\n","[Training Epoch 5] Batch 596, Loss 0.4483807682991028\n","[Training Epoch 5] Batch 597, Loss 0.524484395980835\n","[Training Epoch 5] Batch 598, Loss 0.48011675477027893\n","[Training Epoch 5] Batch 599, Loss 0.5022047758102417\n","[Training Epoch 5] Batch 600, Loss 0.49120765924453735\n","[Training Epoch 5] Batch 601, Loss 0.5008004903793335\n","[Training Epoch 5] Batch 602, Loss 0.48178809881210327\n","[Training Epoch 5] Batch 603, Loss 0.5090891718864441\n","[Training Epoch 5] Batch 604, Loss 0.5222451686859131\n","[Training Epoch 5] Batch 605, Loss 0.5099399089813232\n","[Training Epoch 5] Batch 606, Loss 0.47591742873191833\n","[Training Epoch 5] Batch 607, Loss 0.5035876035690308\n","[Training Epoch 5] Batch 608, Loss 0.515782356262207\n","[Training Epoch 5] Batch 609, Loss 0.5027275681495667\n","[Training Epoch 5] Batch 610, Loss 0.5080944299697876\n","[Training Epoch 5] Batch 611, Loss 0.4830244183540344\n","[Training Epoch 5] Batch 612, Loss 0.48946261405944824\n","[Training Epoch 5] Batch 613, Loss 0.4827880263328552\n","[Training Epoch 5] Batch 614, Loss 0.47072169184684753\n","[Training Epoch 5] Batch 615, Loss 0.4654311537742615\n","[Training Epoch 5] Batch 616, Loss 0.4835394024848938\n","[Training Epoch 5] Batch 617, Loss 0.5126351118087769\n","[Training Epoch 5] Batch 618, Loss 0.5284872055053711\n","[Training Epoch 5] Batch 619, Loss 0.5265361070632935\n","[Training Epoch 5] Batch 620, Loss 0.5287835001945496\n","[Training Epoch 5] Batch 621, Loss 0.48073601722717285\n","[Training Epoch 5] Batch 622, Loss 0.5183496475219727\n","[Training Epoch 5] Batch 623, Loss 0.4956369400024414\n","[Training Epoch 5] Batch 624, Loss 0.4994373321533203\n","[Training Epoch 5] Batch 625, Loss 0.48638081550598145\n","[Training Epoch 5] Batch 626, Loss 0.5048093795776367\n","[Training Epoch 5] Batch 627, Loss 0.503064751625061\n","[Training Epoch 5] Batch 628, Loss 0.49188923835754395\n","[Training Epoch 5] Batch 629, Loss 0.5041261911392212\n","[Training Epoch 5] Batch 630, Loss 0.4966141879558563\n","[Training Epoch 5] Batch 631, Loss 0.4904315769672394\n","[Training Epoch 5] Batch 632, Loss 0.49756884574890137\n","[Training Epoch 5] Batch 633, Loss 0.46732866764068604\n","[Training Epoch 5] Batch 634, Loss 0.519141435623169\n","[Training Epoch 5] Batch 635, Loss 0.49485525488853455\n","[Training Epoch 5] Batch 636, Loss 0.5085487961769104\n","[Training Epoch 5] Batch 637, Loss 0.5029272437095642\n","[Training Epoch 5] Batch 638, Loss 0.5096558928489685\n","[Training Epoch 5] Batch 639, Loss 0.5436220169067383\n","[Training Epoch 5] Batch 640, Loss 0.5149445533752441\n","[Training Epoch 5] Batch 641, Loss 0.48065149784088135\n","[Training Epoch 5] Batch 642, Loss 0.5216153860092163\n","[Training Epoch 5] Batch 643, Loss 0.5083916187286377\n","[Training Epoch 5] Batch 644, Loss 0.5264604687690735\n","[Training Epoch 5] Batch 645, Loss 0.5113639831542969\n","[Training Epoch 5] Batch 646, Loss 0.5021497011184692\n","[Training Epoch 5] Batch 647, Loss 0.5261162519454956\n","[Training Epoch 5] Batch 648, Loss 0.500349223613739\n","[Training Epoch 5] Batch 649, Loss 0.4666402339935303\n","[Training Epoch 5] Batch 650, Loss 0.4822333753108978\n","[Training Epoch 5] Batch 651, Loss 0.49578574299812317\n","[Training Epoch 5] Batch 652, Loss 0.5138172507286072\n","[Training Epoch 5] Batch 653, Loss 0.5023557543754578\n","[Training Epoch 5] Batch 654, Loss 0.4892655611038208\n","[Training Epoch 5] Batch 655, Loss 0.5116504430770874\n","[Training Epoch 5] Batch 656, Loss 0.5028343796730042\n","[Training Epoch 5] Batch 657, Loss 0.47616371512413025\n","[Training Epoch 5] Batch 658, Loss 0.5121191740036011\n","[Training Epoch 5] Batch 659, Loss 0.4925469160079956\n","[Training Epoch 5] Batch 660, Loss 0.4879411458969116\n","[Training Epoch 5] Batch 661, Loss 0.49010616540908813\n","[Training Epoch 5] Batch 662, Loss 0.5163560509681702\n","[Training Epoch 5] Batch 663, Loss 0.4989567697048187\n","[Training Epoch 5] Batch 664, Loss 0.5001468658447266\n","[Training Epoch 5] Batch 665, Loss 0.4941989779472351\n","[Training Epoch 5] Batch 666, Loss 0.48537299036979675\n","[Training Epoch 5] Batch 667, Loss 0.495333194732666\n","[Training Epoch 5] Batch 668, Loss 0.5049724578857422\n","[Training Epoch 5] Batch 669, Loss 0.48085013031959534\n","[Training Epoch 5] Batch 670, Loss 0.48827093839645386\n","[Training Epoch 5] Batch 671, Loss 0.488337904214859\n","[Training Epoch 5] Batch 672, Loss 0.5198867321014404\n","[Training Epoch 5] Batch 673, Loss 0.5046027898788452\n","[Training Epoch 5] Batch 674, Loss 0.5248399972915649\n","[Training Epoch 5] Batch 675, Loss 0.49542587995529175\n","[Training Epoch 5] Batch 676, Loss 0.5225464105606079\n","[Training Epoch 5] Batch 677, Loss 0.48297345638275146\n","[Training Epoch 5] Batch 678, Loss 0.5014064311981201\n","[Training Epoch 5] Batch 679, Loss 0.4814065396785736\n","[Training Epoch 5] Batch 680, Loss 0.5166931748390198\n","[Training Epoch 5] Batch 681, Loss 0.48985975980758667\n","[Training Epoch 5] Batch 682, Loss 0.5174198150634766\n","[Training Epoch 5] Batch 683, Loss 0.494728147983551\n","[Training Epoch 5] Batch 684, Loss 0.5137021541595459\n","[Training Epoch 5] Batch 685, Loss 0.49939143657684326\n","[Training Epoch 5] Batch 686, Loss 0.4944213628768921\n","[Training Epoch 5] Batch 687, Loss 0.4923657178878784\n","[Training Epoch 5] Batch 688, Loss 0.5197123289108276\n","[Training Epoch 5] Batch 689, Loss 0.5183074474334717\n","[Training Epoch 5] Batch 690, Loss 0.5130035877227783\n","[Training Epoch 5] Batch 691, Loss 0.5039217472076416\n","[Training Epoch 5] Batch 692, Loss 0.4905627965927124\n","[Training Epoch 5] Batch 693, Loss 0.48377400636672974\n","[Training Epoch 5] Batch 694, Loss 0.4909557104110718\n","[Training Epoch 5] Batch 695, Loss 0.5231856107711792\n","[Training Epoch 5] Batch 696, Loss 0.4905737042427063\n","[Training Epoch 5] Batch 697, Loss 0.47729992866516113\n","[Training Epoch 5] Batch 698, Loss 0.47385528683662415\n","[Training Epoch 5] Batch 699, Loss 0.5053558349609375\n","[Training Epoch 5] Batch 700, Loss 0.4784364700317383\n","[Training Epoch 5] Batch 701, Loss 0.5107713937759399\n","[Training Epoch 5] Batch 702, Loss 0.48456287384033203\n","[Training Epoch 5] Batch 703, Loss 0.5260082483291626\n","[Training Epoch 5] Batch 704, Loss 0.48790884017944336\n","[Training Epoch 5] Batch 705, Loss 0.5215182304382324\n","[Training Epoch 5] Batch 706, Loss 0.46333831548690796\n","[Training Epoch 5] Batch 707, Loss 0.5157599449157715\n","[Training Epoch 5] Batch 708, Loss 0.501387357711792\n","[Training Epoch 5] Batch 709, Loss 0.5131930708885193\n","[Training Epoch 5] Batch 710, Loss 0.506881833076477\n","[Training Epoch 5] Batch 711, Loss 0.5119180679321289\n","[Training Epoch 5] Batch 712, Loss 0.4756549000740051\n","[Training Epoch 5] Batch 713, Loss 0.5242599844932556\n","[Training Epoch 5] Batch 714, Loss 0.48607802391052246\n","[Training Epoch 5] Batch 715, Loss 0.509955108165741\n","[Training Epoch 5] Batch 716, Loss 0.5160727500915527\n","[Training Epoch 5] Batch 717, Loss 0.49131399393081665\n","[Training Epoch 5] Batch 718, Loss 0.5075076818466187\n","[Training Epoch 5] Batch 719, Loss 0.5053438544273376\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 5] HR = 0.0595, NDCG = 0.0315\n"]}],"source":["!pyenv local 3.10\n","!python ./Torch-NCF/train.py --data_dir '../data/sample_reviews_train.json' --model 'gmf'"]},{"cell_type":"markdown","metadata":{"id":"0I_AVfe5jmbs"},"source":["# Training MLP (Optional)\n","Set the pretrained MF path in the mlp_config in `train.py` if you set pretrain = true.\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"EpiYaPOekXXq"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading Data....\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/data.py:96: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  ratings['rating'][ratings['rating'] > 0] = 1.0\n","Generating Negative Items...\n","      userId                                   interacted_items\n","0          0  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","1          1  {26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 3...\n","2          2  {69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 8...\n","3          3  {84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 9...\n","4          4  {128, 129, 130, 2, 131, 132, 6, 133, 134, 135,...\n","...      ...                                                ...\n","5908    5908  {22016, 10629, 8073, 10634, 17168, 20245, 2639...\n","5909    5909  {16387, 4733, 3433, 4061, 1727, 16498, 5236, 2...\n","5910    5910  {12806, 5263, 5522, 14355, 5527, 5529, 10143, ...\n","5911    5911  {2946, 3075, 4866, 3973, 3334, 8711, 3975, 179...\n","5912    5912  {12290, 12419, 11907, 1542, 10891, 5773, 2189,...\n","\n","[5913 rows x 2 columns]\n","--------------------------------------------------\n","      userId  ...                                     negative_items\n","0          0  ...  {26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 3...\n","1          1  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","2          2  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","3          3  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","4          4  ...  {0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 1...\n","...      ...  ...                                                ...\n","5908    5908  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","5909    5909  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","5910    5910  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","5911    5911  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","5912    5912  ...  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n","\n","[5913 rows x 3 columns]\n","/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/data.py:124: DeprecationWarning: Sampling from a set deprecated\n","since Python 3.9 and will be removed in a subsequent version.\n","  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, min(99, len(x)))) # min(99, len(x))\n","--------------------------------------------------\n","      userId  ...                                   negative_samples\n","0          0  ...  [12649, 24862, 13807, 1352, 8510, 16779, 15948...\n","1          1  ...  [12655, 10431, 18905, 7976, 9556, 6068, 6248, ...\n","2          2  ...  [22275, 761, 17848, 13961, 20350, 3340, 8532, ...\n","3          3  ...  [20930, 14728, 12451, 23320, 22122, 18661, 136...\n","4          4  ...  [19931, 21496, 16225, 23364, 21129, 15051, 209...\n","...      ...  ...                                                ...\n","5908    5908  ...  [18499, 23201, 10265, 11938, 5271, 7370, 19911...\n","5909    5909  ...  [14211, 19206, 15809, 17299, 25447, 4754, 488,...\n","5910    5910  ...  [17217, 26324, 17176, 15930, 13053, 18959, 479...\n","5911    5911  ...  [17661, 13161, 12156, 22112, 1512, 22952, 2233...\n","5912    5912  ...  [9769, 17273, 1308, 23218, 10673, 4761, 16943,...\n","\n","[5913 rows x 4 columns]\n","[tensor([5357, 5357, 5357,  ...,  146, 3136, 3797]), tensor([ 5834, 14615, 16502,  ...,  4068,  6292,  7705]), tensor([5357, 5357, 5357,  ..., 3797, 3797, 3797]), tensor([ 5610, 15291,  4500,  ...,  1508, 13695,  8520])]\n","MLP(\n","  (embedding_user): Embedding(5913, 8)\n","  (embedding_item): Embedding(28028, 8)\n","  (fc_layers): ModuleList(\n","    (0): Linear(in_features=16, out_features=64, bias=True)\n","    (1): Linear(in_features=64, out_features=32, bias=True)\n","    (2): Linear(in_features=32, out_features=16, bias=True)\n","    (3): Linear(in_features=16, out_features=8, bias=True)\n","  )\n","  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n","  (logistic): Sigmoid()\n",")\n","Traceback (most recent call last):\n","  File \"/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/./Torch-NCF/train.py\", line 131, in <module>\n","    main()\n","  File \"/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/./Torch-NCF/train.py\", line 109, in main\n","    engine = MLPEngine(config)\n","  File \"/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/mlp.py\", line 63, in __init__\n","    self.model.load_pretrain_weights()\n","  File \"/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/mlp.py\", line 47, in load_pretrain_weights\n","    resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n","  File \"/mnt/c/Users/medmed/OneDrive - Georgia Institute of Technology/Fall 2023/CS 6220/RestaurantRecommendationSys/NeuralCF/Torch-NCF/utils.py\", line 15, in resume_checkpoint\n","    state_dict = torch.load(model_dir)    \n","  File \"/home/med/.local/lib/python3.10/site-packages/torch/serialization.py\", line 791, in load\n","    with _open_file_like(f, 'rb') as opened_file:\n","  File \"/home/med/.local/lib/python3.10/site-packages/torch/serialization.py\", line 271, in _open_file_like\n","    return _open_file(name_or_buffer, mode)\n","  File \"/home/med/.local/lib/python3.10/site-packages/torch/serialization.py\", line 252, in __init__\n","    super().__init__(open(name, mode))\n","FileNotFoundError: [Errno 2] No such file or directory: 'checkpoints/gmf_factor8neg4-implict_Epoch5_HR0.0844_NDCG0.0414.model'\n"]}],"source":["!python ./Torch-NCF/train.py --data_dir '../data/sample_reviews_train.json' --model 'mlp'"]},{"cell_type":"markdown","metadata":{"id":"uD9PQGP_hgcJ"},"source":["# Train NeuralMF\n","Edit the pretrain setting of neumf_config in `train.py` to determine whether you want to use the pretrained MLP and GMF to train the NeuralMF or not.\n","\n","For small predictive factors, running NeuMF without pre-training can achieve better performance than GMF and MLP. For large predictive factors, pre-training NeuMF can yield better performance."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13335483,"status":"ok","timestamp":1701674160101,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"},"user_tz":300},"id":"yZdIPJd-h3Ye","outputId":"7373207b-42de-4cf0-942f-fd49d58eff6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","[Training Epoch 96] Batch 849, Loss 0.17721793055534363\n","[Training Epoch 96] Batch 850, Loss 0.1520378291606903\n","[Training Epoch 96] Batch 851, Loss 0.1492203027009964\n","[Training Epoch 96] Batch 852, Loss 0.17099374532699585\n","[Training Epoch 96] Batch 853, Loss 0.16935113072395325\n","[Training Epoch 96] Batch 854, Loss 0.14862091839313507\n","[Training Epoch 96] Batch 855, Loss 0.17448438704013824\n","[Training Epoch 96] Batch 856, Loss 0.159054696559906\n","[Training Epoch 96] Batch 857, Loss 0.17863057553768158\n","[Training Epoch 96] Batch 858, Loss 0.1656428575515747\n","[Training Epoch 96] Batch 859, Loss 0.18480516970157623\n","[Training Epoch 96] Batch 860, Loss 0.17102020978927612\n","[Training Epoch 96] Batch 861, Loss 0.17859633266925812\n","[Training Epoch 96] Batch 862, Loss 0.16704785823822021\n","[Training Epoch 96] Batch 863, Loss 0.16532832384109497\n","[Training Epoch 96] Batch 864, Loss 0.1440219283103943\n","[Training Epoch 96] Batch 865, Loss 0.17113807797431946\n","[Training Epoch 96] Batch 866, Loss 0.17769986391067505\n","[Training Epoch 96] Batch 867, Loss 0.14984285831451416\n","[Training Epoch 96] Batch 868, Loss 0.16784194111824036\n","[Training Epoch 96] Batch 869, Loss 0.18145905435085297\n","[Training Epoch 96] Batch 870, Loss 0.1543004810810089\n","[Training Epoch 96] Batch 871, Loss 0.14158281683921814\n","[Training Epoch 96] Batch 872, Loss 0.17589949071407318\n","[Training Epoch 96] Batch 873, Loss 0.16805842518806458\n","[Training Epoch 96] Batch 874, Loss 0.1582821011543274\n","[Training Epoch 96] Batch 875, Loss 0.16069099307060242\n","[Training Epoch 96] Batch 876, Loss 0.1607246994972229\n","[Training Epoch 96] Batch 877, Loss 0.13781340420246124\n","[Training Epoch 96] Batch 878, Loss 0.16186591982841492\n","[Training Epoch 96] Batch 879, Loss 0.1628274917602539\n","[Training Epoch 96] Batch 880, Loss 0.17122882604599\n","[Training Epoch 96] Batch 881, Loss 0.14557971060276031\n","[Training Epoch 96] Batch 882, Loss 0.16708609461784363\n","[Training Epoch 96] Batch 883, Loss 0.17173466086387634\n","[Training Epoch 96] Batch 884, Loss 0.16142304241657257\n","[Training Epoch 96] Batch 885, Loss 0.15814219415187836\n","[Training Epoch 96] Batch 886, Loss 0.16584381461143494\n","[Training Epoch 96] Batch 887, Loss 0.16334347426891327\n","[Training Epoch 96] Batch 888, Loss 0.18459656834602356\n","[Training Epoch 96] Batch 889, Loss 0.15399327874183655\n","[Training Epoch 96] Batch 890, Loss 0.15656472742557526\n","[Training Epoch 96] Batch 891, Loss 0.16386336088180542\n","[Training Epoch 96] Batch 892, Loss 0.17851215600967407\n","[Training Epoch 96] Batch 893, Loss 0.15955376625061035\n","[Training Epoch 96] Batch 894, Loss 0.16469037532806396\n","[Training Epoch 96] Batch 895, Loss 0.16579419374465942\n","[Training Epoch 96] Batch 896, Loss 0.18870213627815247\n","[Training Epoch 96] Batch 897, Loss 0.16758623719215393\n","[Training Epoch 96] Batch 898, Loss 0.17950661480426788\n","[Training Epoch 96] Batch 899, Loss 0.17891183495521545\n","[Training Epoch 96] Batch 900, Loss 0.18840256333351135\n","[Training Epoch 96] Batch 901, Loss 0.15701551735401154\n","[Training Epoch 96] Batch 902, Loss 0.17648565769195557\n","[Training Epoch 96] Batch 903, Loss 0.15262436866760254\n","[Training Epoch 96] Batch 904, Loss 0.16402560472488403\n","[Training Epoch 96] Batch 905, Loss 0.17861902713775635\n","[Training Epoch 96] Batch 906, Loss 0.16437239944934845\n","[Training Epoch 96] Batch 907, Loss 0.1631144881248474\n","[Training Epoch 96] Batch 908, Loss 0.17069873213768005\n","[Training Epoch 96] Batch 909, Loss 0.18190641701221466\n","[Training Epoch 96] Batch 910, Loss 0.16904032230377197\n","[Training Epoch 96] Batch 911, Loss 0.15025445818901062\n","[Training Epoch 96] Batch 912, Loss 0.17095434665679932\n","[Training Epoch 96] Batch 913, Loss 0.14592455327510834\n","[Training Epoch 96] Batch 914, Loss 0.18467697501182556\n","[Training Epoch 96] Batch 915, Loss 0.14820338785648346\n","[Training Epoch 96] Batch 916, Loss 0.19332656264305115\n","[Training Epoch 96] Batch 917, Loss 0.15254700183868408\n","[Training Epoch 96] Batch 918, Loss 0.14979144930839539\n","[Training Epoch 96] Batch 919, Loss 0.15709400177001953\n","[Training Epoch 96] Batch 920, Loss 0.1774480640888214\n","[Training Epoch 96] Batch 921, Loss 0.17843514680862427\n","[Training Epoch 96] Batch 922, Loss 0.16894572973251343\n","[Training Epoch 96] Batch 923, Loss 0.15970732271671295\n","[Training Epoch 96] Batch 924, Loss 0.17936238646507263\n","[Training Epoch 96] Batch 925, Loss 0.17285975813865662\n","[Training Epoch 96] Batch 926, Loss 0.17085742950439453\n","[Training Epoch 96] Batch 927, Loss 0.18933528661727905\n","[Training Epoch 96] Batch 928, Loss 0.1668788492679596\n","[Training Epoch 96] Batch 929, Loss 0.15952587127685547\n","[Training Epoch 96] Batch 930, Loss 0.15468356013298035\n","[Training Epoch 96] Batch 931, Loss 0.17244692146778107\n","[Training Epoch 96] Batch 932, Loss 0.18001581728458405\n","[Training Epoch 96] Batch 933, Loss 0.1818915456533432\n","[Training Epoch 96] Batch 934, Loss 0.15917828679084778\n","[Training Epoch 96] Batch 935, Loss 0.15776753425598145\n","[Training Epoch 96] Batch 936, Loss 0.160715714097023\n","[Training Epoch 96] Batch 937, Loss 0.17830735445022583\n","[Training Epoch 96] Batch 938, Loss 0.16017723083496094\n","[Training Epoch 96] Batch 939, Loss 0.1792520433664322\n","[Training Epoch 96] Batch 940, Loss 0.1690262258052826\n","[Training Epoch 96] Batch 941, Loss 0.16157349944114685\n","[Training Epoch 96] Batch 942, Loss 0.17100292444229126\n","[Training Epoch 96] Batch 943, Loss 0.13210715353488922\n","[Training Epoch 96] Batch 944, Loss 0.1808542162179947\n","[Training Epoch 96] Batch 945, Loss 0.17261254787445068\n","[Training Epoch 96] Batch 946, Loss 0.15468081831932068\n","[Training Epoch 96] Batch 947, Loss 0.14838919043540955\n","[Training Epoch 96] Batch 948, Loss 0.1724529266357422\n","[Training Epoch 96] Batch 949, Loss 0.19800131022930145\n","[Training Epoch 96] Batch 950, Loss 0.17252066731452942\n","[Training Epoch 96] Batch 951, Loss 0.18698926270008087\n","[Training Epoch 96] Batch 952, Loss 0.1572636067867279\n","[Training Epoch 96] Batch 953, Loss 0.17926841974258423\n","[Training Epoch 96] Batch 954, Loss 0.15604782104492188\n","[Training Epoch 96] Batch 955, Loss 0.17252887785434723\n","[Training Epoch 96] Batch 956, Loss 0.16863542795181274\n","[Training Epoch 96] Batch 957, Loss 0.15593138337135315\n","[Training Epoch 96] Batch 958, Loss 0.16948074102401733\n","[Training Epoch 96] Batch 959, Loss 0.15567728877067566\n","[Training Epoch 96] Batch 960, Loss 0.16312755644321442\n","[Training Epoch 96] Batch 961, Loss 0.17620548605918884\n","[Training Epoch 96] Batch 962, Loss 0.17709404230117798\n","[Training Epoch 96] Batch 963, Loss 0.176919087767601\n","[Training Epoch 96] Batch 964, Loss 0.17099085450172424\n","[Training Epoch 96] Batch 965, Loss 0.17237767577171326\n","[Training Epoch 96] Batch 966, Loss 0.16047857701778412\n","[Training Epoch 96] Batch 967, Loss 0.18438495695590973\n","[Training Epoch 96] Batch 968, Loss 0.16549527645111084\n","[Training Epoch 96] Batch 969, Loss 0.14948248863220215\n","[Training Epoch 96] Batch 970, Loss 0.17135494947433472\n","[Training Epoch 96] Batch 971, Loss 0.15932303667068481\n","[Training Epoch 96] Batch 972, Loss 0.1671287715435028\n","[Training Epoch 96] Batch 973, Loss 0.15085628628730774\n","[Training Epoch 96] Batch 974, Loss 0.1605505794286728\n","[Training Epoch 96] Batch 975, Loss 0.1487797200679779\n","[Training Epoch 96] Batch 976, Loss 0.19924813508987427\n","[Training Epoch 96] Batch 977, Loss 0.17344966530799866\n","[Training Epoch 96] Batch 978, Loss 0.1764475554227829\n","[Training Epoch 96] Batch 979, Loss 0.1831497997045517\n","[Training Epoch 96] Batch 980, Loss 0.17057886719703674\n","[Training Epoch 96] Batch 981, Loss 0.17434161901474\n","[Training Epoch 96] Batch 982, Loss 0.19428610801696777\n","[Training Epoch 96] Batch 983, Loss 0.14826539158821106\n","[Training Epoch 96] Batch 984, Loss 0.16036468744277954\n","[Training Epoch 96] Batch 985, Loss 0.14233291149139404\n","[Training Epoch 96] Batch 986, Loss 0.1591877043247223\n","[Training Epoch 96] Batch 987, Loss 0.1582852005958557\n","[Training Epoch 96] Batch 988, Loss 0.15710489451885223\n","[Training Epoch 96] Batch 989, Loss 0.18675664067268372\n","[Training Epoch 96] Batch 990, Loss 0.15100333094596863\n","[Training Epoch 96] Batch 991, Loss 0.15580344200134277\n","[Training Epoch 96] Batch 992, Loss 0.15641453862190247\n","[Training Epoch 96] Batch 993, Loss 0.16508053243160248\n","[Training Epoch 96] Batch 994, Loss 0.16141220927238464\n","[Training Epoch 96] Batch 995, Loss 0.16208118200302124\n","[Training Epoch 96] Batch 996, Loss 0.16970127820968628\n","[Training Epoch 96] Batch 997, Loss 0.17660175263881683\n","[Training Epoch 96] Batch 998, Loss 0.1610005795955658\n","[Training Epoch 96] Batch 999, Loss 0.17625759541988373\n","[Training Epoch 96] Batch 1000, Loss 0.173714280128479\n","[Training Epoch 96] Batch 1001, Loss 0.18722566962242126\n","[Training Epoch 96] Batch 1002, Loss 0.15793800354003906\n","[Training Epoch 96] Batch 1003, Loss 0.184682697057724\n","[Training Epoch 96] Batch 1004, Loss 0.1400032788515091\n","[Training Epoch 96] Batch 1005, Loss 0.17675334215164185\n","[Training Epoch 96] Batch 1006, Loss 0.15998540818691254\n","[Training Epoch 96] Batch 1007, Loss 0.16601696610450745\n","[Training Epoch 96] Batch 1008, Loss 0.1654260754585266\n","[Training Epoch 96] Batch 1009, Loss 0.15578442811965942\n","[Training Epoch 96] Batch 1010, Loss 0.18249982595443726\n","[Training Epoch 96] Batch 1011, Loss 0.18339066207408905\n","[Training Epoch 96] Batch 1012, Loss 0.16071847081184387\n","[Training Epoch 96] Batch 1013, Loss 0.17215773463249207\n","[Training Epoch 96] Batch 1014, Loss 0.18753299117088318\n","[Training Epoch 96] Batch 1015, Loss 0.18513059616088867\n","[Training Epoch 96] Batch 1016, Loss 0.164443701505661\n","[Training Epoch 96] Batch 1017, Loss 0.1662805676460266\n","[Training Epoch 96] Batch 1018, Loss 0.1551423966884613\n","[Training Epoch 96] Batch 1019, Loss 0.16170546412467957\n","[Training Epoch 96] Batch 1020, Loss 0.19639776647090912\n","[Training Epoch 96] Batch 1021, Loss 0.17605847120285034\n","[Training Epoch 96] Batch 1022, Loss 0.1742987036705017\n","[Training Epoch 96] Batch 1023, Loss 0.17103001475334167\n","[Training Epoch 96] Batch 1024, Loss 0.1737157106399536\n","[Training Epoch 96] Batch 1025, Loss 0.1633436530828476\n","[Training Epoch 96] Batch 1026, Loss 0.1542031168937683\n","[Training Epoch 96] Batch 1027, Loss 0.16730283200740814\n","[Training Epoch 96] Batch 1028, Loss 0.19795820116996765\n","[Training Epoch 96] Batch 1029, Loss 0.15587690472602844\n","[Training Epoch 96] Batch 1030, Loss 0.1831972450017929\n","[Training Epoch 96] Batch 1031, Loss 0.15625283122062683\n","[Training Epoch 96] Batch 1032, Loss 0.16095122694969177\n","[Training Epoch 96] Batch 1033, Loss 0.15672622621059418\n","[Training Epoch 96] Batch 1034, Loss 0.1687747985124588\n","[Training Epoch 96] Batch 1035, Loss 0.15682768821716309\n","[Training Epoch 96] Batch 1036, Loss 0.1727275848388672\n","[Training Epoch 96] Batch 1037, Loss 0.1636701226234436\n","[Training Epoch 96] Batch 1038, Loss 0.17786796391010284\n","[Training Epoch 96] Batch 1039, Loss 0.17438241839408875\n","[Training Epoch 96] Batch 1040, Loss 0.1691070944070816\n","[Training Epoch 96] Batch 1041, Loss 0.1652396321296692\n","[Training Epoch 96] Batch 1042, Loss 0.16007772088050842\n","[Training Epoch 96] Batch 1043, Loss 0.17787133157253265\n","[Training Epoch 96] Batch 1044, Loss 0.1660718023777008\n","[Training Epoch 96] Batch 1045, Loss 0.15222187340259552\n","[Training Epoch 96] Batch 1046, Loss 0.16867883503437042\n","[Training Epoch 96] Batch 1047, Loss 0.16355638206005096\n","[Training Epoch 96] Batch 1048, Loss 0.17098510265350342\n","[Training Epoch 96] Batch 1049, Loss 0.1386590301990509\n","[Training Epoch 96] Batch 1050, Loss 0.17281970381736755\n","[Training Epoch 96] Batch 1051, Loss 0.15244479477405548\n","[Training Epoch 96] Batch 1052, Loss 0.16504323482513428\n","[Training Epoch 96] Batch 1053, Loss 0.20075181126594543\n","[Training Epoch 96] Batch 1054, Loss 0.15724164247512817\n","[Training Epoch 96] Batch 1055, Loss 0.17819960415363312\n","[Training Epoch 96] Batch 1056, Loss 0.16323888301849365\n","[Training Epoch 96] Batch 1057, Loss 0.1524074226617813\n","[Training Epoch 96] Batch 1058, Loss 0.17749246954917908\n","[Training Epoch 96] Batch 1059, Loss 0.19809004664421082\n","[Training Epoch 96] Batch 1060, Loss 0.15726915001869202\n","[Training Epoch 96] Batch 1061, Loss 0.19077467918395996\n","[Training Epoch 96] Batch 1062, Loss 0.1729549765586853\n","[Training Epoch 96] Batch 1063, Loss 0.16321848332881927\n","[Training Epoch 96] Batch 1064, Loss 0.16555073857307434\n","[Training Epoch 96] Batch 1065, Loss 0.17742453515529633\n","[Training Epoch 96] Batch 1066, Loss 0.15021619200706482\n","[Training Epoch 96] Batch 1067, Loss 0.1951591968536377\n","[Training Epoch 96] Batch 1068, Loss 0.18450438976287842\n","[Training Epoch 96] Batch 1069, Loss 0.1812448501586914\n","[Training Epoch 96] Batch 1070, Loss 0.17240643501281738\n","[Training Epoch 96] Batch 1071, Loss 0.15694333612918854\n","[Training Epoch 96] Batch 1072, Loss 0.16729788482189178\n","[Training Epoch 96] Batch 1073, Loss 0.16135868430137634\n","[Training Epoch 96] Batch 1074, Loss 0.17121759057044983\n","[Training Epoch 96] Batch 1075, Loss 0.16413751244544983\n","[Training Epoch 96] Batch 1076, Loss 0.18188124895095825\n","[Training Epoch 96] Batch 1077, Loss 0.16319787502288818\n","[Training Epoch 96] Batch 1078, Loss 0.16447509825229645\n","[Training Epoch 96] Batch 1079, Loss 0.18612340092658997\n","[Training Epoch 96] Batch 1080, Loss 0.1501350998878479\n","[Training Epoch 96] Batch 1081, Loss 0.17196664214134216\n","[Training Epoch 96] Batch 1082, Loss 0.1659427285194397\n","[Training Epoch 96] Batch 1083, Loss 0.14876168966293335\n","[Training Epoch 96] Batch 1084, Loss 0.18044432997703552\n","[Training Epoch 96] Batch 1085, Loss 0.1669507920742035\n","[Training Epoch 96] Batch 1086, Loss 0.1803465634584427\n","[Training Epoch 96] Batch 1087, Loss 0.17777912318706512\n","[Training Epoch 96] Batch 1088, Loss 0.1778009533882141\n","[Training Epoch 96] Batch 1089, Loss 0.14077028632164001\n","[Training Epoch 96] Batch 1090, Loss 0.14710432291030884\n","[Training Epoch 96] Batch 1091, Loss 0.15746468305587769\n","[Training Epoch 96] Batch 1092, Loss 0.15815699100494385\n","[Training Epoch 96] Batch 1093, Loss 0.13688451051712036\n","[Training Epoch 96] Batch 1094, Loss 0.17471259832382202\n","[Training Epoch 96] Batch 1095, Loss 0.1668015420436859\n","[Training Epoch 96] Batch 1096, Loss 0.16889069974422455\n","[Training Epoch 96] Batch 1097, Loss 0.18021167814731598\n","[Training Epoch 96] Batch 1098, Loss 0.16715100407600403\n","[Training Epoch 96] Batch 1099, Loss 0.1411687135696411\n","[Training Epoch 96] Batch 1100, Loss 0.166916161775589\n","[Training Epoch 96] Batch 1101, Loss 0.16148558259010315\n","[Training Epoch 96] Batch 1102, Loss 0.157065287232399\n","[Training Epoch 96] Batch 1103, Loss 0.17985063791275024\n","[Training Epoch 96] Batch 1104, Loss 0.1638108193874359\n","[Training Epoch 96] Batch 1105, Loss 0.1818598210811615\n","[Training Epoch 96] Batch 1106, Loss 0.16409024596214294\n","[Training Epoch 96] Batch 1107, Loss 0.16292208433151245\n","[Training Epoch 96] Batch 1108, Loss 0.1692878156900406\n","[Training Epoch 96] Batch 1109, Loss 0.17269280552864075\n","[Training Epoch 96] Batch 1110, Loss 0.14800907671451569\n","[Training Epoch 96] Batch 1111, Loss 0.1697443574666977\n","[Training Epoch 96] Batch 1112, Loss 0.1749683916568756\n","[Training Epoch 96] Batch 1113, Loss 0.17096053063869476\n","[Training Epoch 96] Batch 1114, Loss 0.15819120407104492\n","[Training Epoch 96] Batch 1115, Loss 0.1455201804637909\n","[Training Epoch 96] Batch 1116, Loss 0.15128237009048462\n","[Training Epoch 96] Batch 1117, Loss 0.15732428431510925\n","[Training Epoch 96] Batch 1118, Loss 0.17225810885429382\n","[Training Epoch 96] Batch 1119, Loss 0.1505342721939087\n","[Training Epoch 96] Batch 1120, Loss 0.17469531297683716\n","[Training Epoch 96] Batch 1121, Loss 0.17462824285030365\n","[Training Epoch 96] Batch 1122, Loss 0.18475443124771118\n","[Training Epoch 96] Batch 1123, Loss 0.17588621377944946\n","[Training Epoch 96] Batch 1124, Loss 0.17835693061351776\n","[Training Epoch 96] Batch 1125, Loss 0.16600489616394043\n","[Training Epoch 96] Batch 1126, Loss 0.16063131392002106\n","[Training Epoch 96] Batch 1127, Loss 0.163497656583786\n","[Training Epoch 96] Batch 1128, Loss 0.18850800395011902\n","[Training Epoch 96] Batch 1129, Loss 0.16617867350578308\n","[Training Epoch 96] Batch 1130, Loss 0.18328066170215607\n","[Training Epoch 96] Batch 1131, Loss 0.16718360781669617\n","[Training Epoch 96] Batch 1132, Loss 0.15929776430130005\n","[Training Epoch 96] Batch 1133, Loss 0.1553361713886261\n","[Training Epoch 96] Batch 1134, Loss 0.1765305995941162\n","[Training Epoch 96] Batch 1135, Loss 0.17455366253852844\n","[Training Epoch 96] Batch 1136, Loss 0.1456022560596466\n","[Training Epoch 96] Batch 1137, Loss 0.19260311126708984\n","[Training Epoch 96] Batch 1138, Loss 0.17325571179389954\n","[Training Epoch 96] Batch 1139, Loss 0.1670740842819214\n","[Training Epoch 96] Batch 1140, Loss 0.15417657792568207\n","[Training Epoch 96] Batch 1141, Loss 0.16823332011699677\n","[Training Epoch 96] Batch 1142, Loss 0.18311840295791626\n","[Training Epoch 96] Batch 1143, Loss 0.16258665919303894\n","[Training Epoch 96] Batch 1144, Loss 0.15803013741970062\n","[Training Epoch 96] Batch 1145, Loss 0.19047462940216064\n","[Training Epoch 96] Batch 1146, Loss 0.16148605942726135\n","[Training Epoch 96] Batch 1147, Loss 0.16599240899085999\n","[Training Epoch 96] Batch 1148, Loss 0.13990165293216705\n","[Training Epoch 96] Batch 1149, Loss 0.1880936175584793\n","[Training Epoch 96] Batch 1150, Loss 0.16994619369506836\n","[Training Epoch 96] Batch 1151, Loss 0.18171583116054535\n","[Training Epoch 96] Batch 1152, Loss 0.15366992354393005\n","[Training Epoch 96] Batch 1153, Loss 0.15909427404403687\n","[Training Epoch 96] Batch 1154, Loss 0.17394807934761047\n","[Training Epoch 96] Batch 1155, Loss 0.18016791343688965\n","[Training Epoch 96] Batch 1156, Loss 0.15360310673713684\n","[Training Epoch 96] Batch 1157, Loss 0.1610168218612671\n","[Training Epoch 96] Batch 1158, Loss 0.1595534384250641\n","[Training Epoch 96] Batch 1159, Loss 0.15881094336509705\n","[Training Epoch 96] Batch 1160, Loss 0.19049577414989471\n","/content/drive/MyDrive/Neural-CF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 96] HR = 0.7239, NDCG = 0.3526\n","Epoch 97 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 97] Batch 0, Loss 0.17376776039600372\n","[Training Epoch 97] Batch 1, Loss 0.1639563888311386\n","[Training Epoch 97] Batch 2, Loss 0.1672441065311432\n","[Training Epoch 97] Batch 3, Loss 0.16988417506217957\n","[Training Epoch 97] Batch 4, Loss 0.15349027514457703\n","[Training Epoch 97] Batch 5, Loss 0.13566218316555023\n","[Training Epoch 97] Batch 6, Loss 0.15960021317005157\n","[Training Epoch 97] Batch 7, Loss 0.15467429161071777\n","[Training Epoch 97] Batch 8, Loss 0.13827180862426758\n","[Training Epoch 97] Batch 9, Loss 0.16767451167106628\n","[Training Epoch 97] Batch 10, Loss 0.15372827649116516\n","[Training Epoch 97] Batch 11, Loss 0.16054008901119232\n","[Training Epoch 97] Batch 12, Loss 0.1606912910938263\n","[Training Epoch 97] Batch 13, Loss 0.18054458498954773\n","[Training Epoch 97] Batch 14, Loss 0.16820485889911652\n","[Training Epoch 97] Batch 15, Loss 0.16051511466503143\n","[Training Epoch 97] Batch 16, Loss 0.1538490355014801\n","[Training Epoch 97] Batch 17, Loss 0.17394757270812988\n","[Training Epoch 97] Batch 18, Loss 0.1644878387451172\n","[Training Epoch 97] Batch 19, Loss 0.17297634482383728\n","[Training Epoch 97] Batch 20, Loss 0.1937948614358902\n","[Training Epoch 97] Batch 21, Loss 0.16989091038703918\n","[Training Epoch 97] Batch 22, Loss 0.1645810455083847\n","[Training Epoch 97] Batch 23, Loss 0.15535058081150055\n","[Training Epoch 97] Batch 24, Loss 0.16457107663154602\n","[Training Epoch 97] Batch 25, Loss 0.13669589161872864\n","[Training Epoch 97] Batch 26, Loss 0.14736701548099518\n","[Training Epoch 97] Batch 27, Loss 0.18634045124053955\n","[Training Epoch 97] Batch 28, Loss 0.15410462021827698\n","[Training Epoch 97] Batch 29, Loss 0.15948370099067688\n","[Training Epoch 97] Batch 30, Loss 0.16977258026599884\n","[Training Epoch 97] Batch 31, Loss 0.1733979880809784\n","[Training Epoch 97] Batch 32, Loss 0.15187808871269226\n","[Training Epoch 97] Batch 33, Loss 0.1793258786201477\n","[Training Epoch 97] Batch 34, Loss 0.16193848848342896\n","[Training Epoch 97] Batch 35, Loss 0.17741358280181885\n","[Training Epoch 97] Batch 36, Loss 0.16482673585414886\n","[Training Epoch 97] Batch 37, Loss 0.18907782435417175\n","[Training Epoch 97] Batch 38, Loss 0.16839337348937988\n","[Training Epoch 97] Batch 39, Loss 0.14923900365829468\n","[Training Epoch 97] Batch 40, Loss 0.15499678254127502\n","[Training Epoch 97] Batch 41, Loss 0.17590394616127014\n","[Training Epoch 97] Batch 42, Loss 0.16357260942459106\n","[Training Epoch 97] Batch 43, Loss 0.14432314038276672\n","[Training Epoch 97] Batch 44, Loss 0.16845981776714325\n","[Training Epoch 97] Batch 45, Loss 0.15002700686454773\n","[Training Epoch 97] Batch 46, Loss 0.17799800634384155\n","[Training Epoch 97] Batch 47, Loss 0.16581588983535767\n","[Training Epoch 97] Batch 48, Loss 0.16012126207351685\n","[Training Epoch 97] Batch 49, Loss 0.15021640062332153\n","[Training Epoch 97] Batch 50, Loss 0.1832391619682312\n","[Training Epoch 97] Batch 51, Loss 0.16981536149978638\n","[Training Epoch 97] Batch 52, Loss 0.16044797003269196\n","[Training Epoch 97] Batch 53, Loss 0.1740555763244629\n","[Training Epoch 97] Batch 54, Loss 0.20307867228984833\n","[Training Epoch 97] Batch 55, Loss 0.1587510108947754\n","[Training Epoch 97] Batch 56, Loss 0.15183550119400024\n","[Training Epoch 97] Batch 57, Loss 0.17029830813407898\n","[Training Epoch 97] Batch 58, Loss 0.1699584275484085\n","[Training Epoch 97] Batch 59, Loss 0.17188525199890137\n","[Training Epoch 97] Batch 60, Loss 0.17876122891902924\n","[Training Epoch 97] Batch 61, Loss 0.1598292887210846\n","[Training Epoch 97] Batch 62, Loss 0.18340849876403809\n","[Training Epoch 97] Batch 63, Loss 0.15117613971233368\n","[Training Epoch 97] Batch 64, Loss 0.17525842785835266\n","[Training Epoch 97] Batch 65, Loss 0.1648373007774353\n","[Training Epoch 97] Batch 66, Loss 0.14818769693374634\n","[Training Epoch 97] Batch 67, Loss 0.1814277321100235\n","[Training Epoch 97] Batch 68, Loss 0.17258252203464508\n","[Training Epoch 97] Batch 69, Loss 0.15579378604888916\n","[Training Epoch 97] Batch 70, Loss 0.16742739081382751\n","[Training Epoch 97] Batch 71, Loss 0.18091368675231934\n","[Training Epoch 97] Batch 72, Loss 0.161918044090271\n","[Training Epoch 97] Batch 73, Loss 0.16798339784145355\n","[Training Epoch 97] Batch 74, Loss 0.17322197556495667\n","[Training Epoch 97] Batch 75, Loss 0.15805158019065857\n","[Training Epoch 97] Batch 76, Loss 0.1578434705734253\n","[Training Epoch 97] Batch 77, Loss 0.16132670640945435\n","[Training Epoch 97] Batch 78, Loss 0.16400426626205444\n","[Training Epoch 97] Batch 79, Loss 0.14605823159217834\n","[Training Epoch 97] Batch 80, Loss 0.15510843694210052\n","[Training Epoch 97] Batch 81, Loss 0.1692085862159729\n","[Training Epoch 97] Batch 82, Loss 0.18324674665927887\n","[Training Epoch 97] Batch 83, Loss 0.16441486775875092\n","[Training Epoch 97] Batch 84, Loss 0.15109993517398834\n","[Training Epoch 97] Batch 85, Loss 0.17494416236877441\n","[Training Epoch 97] Batch 86, Loss 0.1686304360628128\n","[Training Epoch 97] Batch 87, Loss 0.14243072271347046\n","[Training Epoch 97] Batch 88, Loss 0.19722110033035278\n","[Training Epoch 97] Batch 89, Loss 0.15507546067237854\n","[Training Epoch 97] Batch 90, Loss 0.17574554681777954\n","[Training Epoch 97] Batch 91, Loss 0.16647064685821533\n","[Training Epoch 97] Batch 92, Loss 0.17549800872802734\n","[Training Epoch 97] Batch 93, Loss 0.1733834445476532\n","[Training Epoch 97] Batch 94, Loss 0.1706375777721405\n","[Training Epoch 97] Batch 95, Loss 0.16002508997917175\n","[Training Epoch 97] Batch 96, Loss 0.16010728478431702\n","[Training Epoch 97] Batch 97, Loss 0.14694753289222717\n","[Training Epoch 97] Batch 98, Loss 0.1707870066165924\n","[Training Epoch 97] Batch 99, Loss 0.1651625782251358\n","[Training Epoch 97] Batch 100, Loss 0.15899145603179932\n","[Training Epoch 97] Batch 101, Loss 0.1486177146434784\n","[Training Epoch 97] Batch 102, Loss 0.18003961443901062\n","[Training Epoch 97] Batch 103, Loss 0.15152263641357422\n","[Training Epoch 97] Batch 104, Loss 0.15534543991088867\n","[Training Epoch 97] Batch 105, Loss 0.18218758702278137\n","[Training Epoch 97] Batch 106, Loss 0.1705807000398636\n","[Training Epoch 97] Batch 107, Loss 0.1656264066696167\n","[Training Epoch 97] Batch 108, Loss 0.14194989204406738\n","[Training Epoch 97] Batch 109, Loss 0.15592466294765472\n","[Training Epoch 97] Batch 110, Loss 0.1660810112953186\n","[Training Epoch 97] Batch 111, Loss 0.15485411882400513\n","[Training Epoch 97] Batch 112, Loss 0.1640288531780243\n","[Training Epoch 97] Batch 113, Loss 0.15618854761123657\n","[Training Epoch 97] Batch 114, Loss 0.1566420942544937\n","[Training Epoch 97] Batch 115, Loss 0.16494490206241608\n","[Training Epoch 97] Batch 116, Loss 0.15949541330337524\n","[Training Epoch 97] Batch 117, Loss 0.16348350048065186\n","[Training Epoch 97] Batch 118, Loss 0.1605234295129776\n","[Training Epoch 97] Batch 119, Loss 0.15878050029277802\n","[Training Epoch 97] Batch 120, Loss 0.1723281294107437\n","[Training Epoch 97] Batch 121, Loss 0.14903858304023743\n","[Training Epoch 97] Batch 122, Loss 0.18463531136512756\n","[Training Epoch 97] Batch 123, Loss 0.15804776549339294\n","[Training Epoch 97] Batch 124, Loss 0.15906620025634766\n","[Training Epoch 97] Batch 125, Loss 0.14969009160995483\n","[Training Epoch 97] Batch 126, Loss 0.16258220374584198\n","[Training Epoch 97] Batch 127, Loss 0.183819979429245\n","[Training Epoch 97] Batch 128, Loss 0.14654900133609772\n","[Training Epoch 97] Batch 129, Loss 0.1645098626613617\n","[Training Epoch 97] Batch 130, Loss 0.16783574223518372\n","[Training Epoch 97] Batch 131, Loss 0.18525409698486328\n","[Training Epoch 97] Batch 132, Loss 0.15632684528827667\n","[Training Epoch 97] Batch 133, Loss 0.17937204241752625\n","[Training Epoch 97] Batch 134, Loss 0.1660257875919342\n","[Training Epoch 97] Batch 135, Loss 0.18423300981521606\n","[Training Epoch 97] Batch 136, Loss 0.15841200947761536\n","[Training Epoch 97] Batch 137, Loss 0.15429429709911346\n","[Training Epoch 97] Batch 138, Loss 0.16781483590602875\n","[Training Epoch 97] Batch 139, Loss 0.17004331946372986\n","[Training Epoch 97] Batch 140, Loss 0.19042927026748657\n","[Training Epoch 97] Batch 141, Loss 0.16387103497982025\n","[Training Epoch 97] Batch 142, Loss 0.1611875593662262\n","[Training Epoch 97] Batch 143, Loss 0.15361540019512177\n","[Training Epoch 97] Batch 144, Loss 0.1962105631828308\n","[Training Epoch 97] Batch 145, Loss 0.17258906364440918\n","[Training Epoch 97] Batch 146, Loss 0.18871811032295227\n","[Training Epoch 97] Batch 147, Loss 0.1610850989818573\n","[Training Epoch 97] Batch 148, Loss 0.15540942549705505\n","[Training Epoch 97] Batch 149, Loss 0.15895074605941772\n","[Training Epoch 97] Batch 150, Loss 0.15915881097316742\n","[Training Epoch 97] Batch 151, Loss 0.16833628714084625\n","[Training Epoch 97] Batch 152, Loss 0.1801360845565796\n","[Training Epoch 97] Batch 153, Loss 0.16125771403312683\n","[Training Epoch 97] Batch 154, Loss 0.15044841170310974\n","[Training Epoch 97] Batch 155, Loss 0.16815029084682465\n","[Training Epoch 97] Batch 156, Loss 0.15268398821353912\n","[Training Epoch 97] Batch 157, Loss 0.18127593398094177\n","[Training Epoch 97] Batch 158, Loss 0.14813116192817688\n","[Training Epoch 97] Batch 159, Loss 0.14659279584884644\n","[Training Epoch 97] Batch 160, Loss 0.16404536366462708\n","[Training Epoch 97] Batch 161, Loss 0.15919311344623566\n","[Training Epoch 97] Batch 162, Loss 0.15666981041431427\n","[Training Epoch 97] Batch 163, Loss 0.15873003005981445\n","[Training Epoch 97] Batch 164, Loss 0.1743353009223938\n","[Training Epoch 97] Batch 165, Loss 0.1718147099018097\n","[Training Epoch 97] Batch 166, Loss 0.16360893845558167\n","[Training Epoch 97] Batch 167, Loss 0.18521448969841003\n","[Training Epoch 97] Batch 168, Loss 0.16660335659980774\n","[Training Epoch 97] Batch 169, Loss 0.14818209409713745\n","[Training Epoch 97] Batch 170, Loss 0.16427135467529297\n","[Training Epoch 97] Batch 171, Loss 0.1629086434841156\n","[Training Epoch 97] Batch 172, Loss 0.17191967368125916\n","[Training Epoch 97] Batch 173, Loss 0.17744866013526917\n","[Training Epoch 97] Batch 174, Loss 0.16100232303142548\n","[Training Epoch 97] Batch 175, Loss 0.18240942060947418\n","[Training Epoch 97] Batch 176, Loss 0.15057113766670227\n","[Training Epoch 97] Batch 177, Loss 0.16084879636764526\n","[Training Epoch 97] Batch 178, Loss 0.14901268482208252\n","[Training Epoch 97] Batch 179, Loss 0.1586923897266388\n","[Training Epoch 97] Batch 180, Loss 0.14833514392375946\n","[Training Epoch 97] Batch 181, Loss 0.146140456199646\n","[Training Epoch 97] Batch 182, Loss 0.17271949350833893\n","[Training Epoch 97] Batch 183, Loss 0.18626806139945984\n","[Training Epoch 97] Batch 184, Loss 0.16504314541816711\n","[Training Epoch 97] Batch 185, Loss 0.1588759422302246\n","[Training Epoch 97] Batch 186, Loss 0.16434995830059052\n","[Training Epoch 97] Batch 187, Loss 0.17850133776664734\n","[Training Epoch 97] Batch 188, Loss 0.16224358975887299\n","[Training Epoch 97] Batch 189, Loss 0.1786404550075531\n","[Training Epoch 97] Batch 190, Loss 0.1582927107810974\n","[Training Epoch 97] Batch 191, Loss 0.15092474222183228\n","[Training Epoch 97] Batch 192, Loss 0.18042482435703278\n","[Training Epoch 97] Batch 193, Loss 0.16237568855285645\n","[Training Epoch 97] Batch 194, Loss 0.16979444026947021\n","[Training Epoch 97] Batch 195, Loss 0.15603265166282654\n","[Training Epoch 97] Batch 196, Loss 0.14968091249465942\n","[Training Epoch 97] Batch 197, Loss 0.14843204617500305\n","[Training Epoch 97] Batch 198, Loss 0.155008003115654\n","[Training Epoch 97] Batch 199, Loss 0.1793525516986847\n","[Training Epoch 97] Batch 200, Loss 0.161177396774292\n","[Training Epoch 97] Batch 201, Loss 0.17152494192123413\n","[Training Epoch 97] Batch 202, Loss 0.1913212537765503\n","[Training Epoch 97] Batch 203, Loss 0.16472923755645752\n","[Training Epoch 97] Batch 204, Loss 0.15837395191192627\n","[Training Epoch 97] Batch 205, Loss 0.16248400509357452\n","[Training Epoch 97] Batch 206, Loss 0.16720923781394958\n","[Training Epoch 97] Batch 207, Loss 0.16371314227581024\n","[Training Epoch 97] Batch 208, Loss 0.1386943757534027\n","[Training Epoch 97] Batch 209, Loss 0.17558574676513672\n","[Training Epoch 97] Batch 210, Loss 0.15398183465003967\n","[Training Epoch 97] Batch 211, Loss 0.1570170670747757\n","[Training Epoch 97] Batch 212, Loss 0.18027816712856293\n","[Training Epoch 97] Batch 213, Loss 0.1609123945236206\n","[Training Epoch 97] Batch 214, Loss 0.18136413395404816\n","[Training Epoch 97] Batch 215, Loss 0.16614505648612976\n","[Training Epoch 97] Batch 216, Loss 0.16876666247844696\n","[Training Epoch 97] Batch 217, Loss 0.17054471373558044\n","[Training Epoch 97] Batch 218, Loss 0.1671399623155594\n","[Training Epoch 97] Batch 219, Loss 0.18636980652809143\n","[Training Epoch 97] Batch 220, Loss 0.15715648233890533\n","[Training Epoch 97] Batch 221, Loss 0.17547430098056793\n","[Training Epoch 97] Batch 222, Loss 0.17746958136558533\n","[Training Epoch 97] Batch 223, Loss 0.16174906492233276\n","[Training Epoch 97] Batch 224, Loss 0.18771713972091675\n","[Training Epoch 97] Batch 225, Loss 0.15461672842502594\n","[Training Epoch 97] Batch 226, Loss 0.13933297991752625\n","[Training Epoch 97] Batch 227, Loss 0.16102635860443115\n","[Training Epoch 97] Batch 228, Loss 0.15786361694335938\n","[Training Epoch 97] Batch 229, Loss 0.16559824347496033\n","[Training Epoch 97] Batch 230, Loss 0.16557152569293976\n","[Training Epoch 97] Batch 231, Loss 0.14306700229644775\n","[Training Epoch 97] Batch 232, Loss 0.1768454611301422\n","[Training Epoch 97] Batch 233, Loss 0.17226704955101013\n","[Training Epoch 97] Batch 234, Loss 0.14879795908927917\n","[Training Epoch 97] Batch 235, Loss 0.14424782991409302\n","[Training Epoch 97] Batch 236, Loss 0.16379985213279724\n","[Training Epoch 97] Batch 237, Loss 0.16135530173778534\n","[Training Epoch 97] Batch 238, Loss 0.17564374208450317\n","[Training Epoch 97] Batch 239, Loss 0.15646880865097046\n","[Training Epoch 97] Batch 240, Loss 0.16187788546085358\n","[Training Epoch 97] Batch 241, Loss 0.1654888093471527\n","[Training Epoch 97] Batch 242, Loss 0.16809867322444916\n","[Training Epoch 97] Batch 243, Loss 0.1621231734752655\n","[Training Epoch 97] Batch 244, Loss 0.15540862083435059\n","[Training Epoch 97] Batch 245, Loss 0.16574442386627197\n","[Training Epoch 97] Batch 246, Loss 0.15687808394432068\n","[Training Epoch 97] Batch 247, Loss 0.13154195249080658\n","[Training Epoch 97] Batch 248, Loss 0.16890732944011688\n","[Training Epoch 97] Batch 249, Loss 0.15216055512428284\n","[Training Epoch 97] Batch 250, Loss 0.15584266185760498\n","[Training Epoch 97] Batch 251, Loss 0.17167693376541138\n","[Training Epoch 97] Batch 252, Loss 0.16657796502113342\n","[Training Epoch 97] Batch 253, Loss 0.16525834798812866\n","[Training Epoch 97] Batch 254, Loss 0.15338082611560822\n","[Training Epoch 97] Batch 255, Loss 0.17713181674480438\n","[Training Epoch 97] Batch 256, Loss 0.17752699553966522\n","[Training Epoch 97] Batch 257, Loss 0.14206919074058533\n","[Training Epoch 97] Batch 258, Loss 0.15296843647956848\n","[Training Epoch 97] Batch 259, Loss 0.1686263233423233\n","[Training Epoch 97] Batch 260, Loss 0.16392028331756592\n","[Training Epoch 97] Batch 261, Loss 0.15612518787384033\n","[Training Epoch 97] Batch 262, Loss 0.14671799540519714\n","[Training Epoch 97] Batch 263, Loss 0.19689813256263733\n","[Training Epoch 97] Batch 264, Loss 0.15855064988136292\n","[Training Epoch 97] Batch 265, Loss 0.18029415607452393\n","[Training Epoch 97] Batch 266, Loss 0.1850421130657196\n","[Training Epoch 97] Batch 267, Loss 0.15732277929782867\n","[Training Epoch 97] Batch 268, Loss 0.15438199043273926\n","[Training Epoch 97] Batch 269, Loss 0.1735924780368805\n","[Training Epoch 97] Batch 270, Loss 0.1779007464647293\n","[Training Epoch 97] Batch 271, Loss 0.17852696776390076\n","[Training Epoch 97] Batch 272, Loss 0.17485523223876953\n","[Training Epoch 97] Batch 273, Loss 0.16531282663345337\n","[Training Epoch 97] Batch 274, Loss 0.16157928109169006\n","[Training Epoch 97] Batch 275, Loss 0.15567490458488464\n","[Training Epoch 97] Batch 276, Loss 0.14739388227462769\n","[Training Epoch 97] Batch 277, Loss 0.1584823578596115\n","[Training Epoch 97] Batch 278, Loss 0.15995821356773376\n","[Training Epoch 97] Batch 279, Loss 0.17638403177261353\n","[Training Epoch 97] Batch 280, Loss 0.15553078055381775\n","[Training Epoch 97] Batch 281, Loss 0.16592252254486084\n","[Training Epoch 97] Batch 282, Loss 0.15594357252120972\n","[Training Epoch 97] Batch 283, Loss 0.1543799638748169\n","[Training Epoch 97] Batch 284, Loss 0.17049941420555115\n","[Training Epoch 97] Batch 285, Loss 0.1668681800365448\n","[Training Epoch 97] Batch 286, Loss 0.15939846634864807\n","[Training Epoch 97] Batch 287, Loss 0.16279219090938568\n","[Training Epoch 97] Batch 288, Loss 0.16762030124664307\n","[Training Epoch 97] Batch 289, Loss 0.1559804379940033\n","[Training Epoch 97] Batch 290, Loss 0.1587175726890564\n","[Training Epoch 97] Batch 291, Loss 0.1658436357975006\n","[Training Epoch 97] Batch 292, Loss 0.14800304174423218\n","[Training Epoch 97] Batch 293, Loss 0.17727868258953094\n","[Training Epoch 97] Batch 294, Loss 0.17800836265087128\n","[Training Epoch 97] Batch 295, Loss 0.15834984183311462\n","[Training Epoch 97] Batch 296, Loss 0.16521231830120087\n","[Training Epoch 97] Batch 297, Loss 0.18716272711753845\n","[Training Epoch 97] Batch 298, Loss 0.14359010756015778\n","[Training Epoch 97] Batch 299, Loss 0.151458740234375\n","[Training Epoch 97] Batch 300, Loss 0.16444779932498932\n","[Training Epoch 97] Batch 301, Loss 0.18054330348968506\n","[Training Epoch 97] Batch 302, Loss 0.16600868105888367\n","[Training Epoch 97] Batch 303, Loss 0.15523172914981842\n","[Training Epoch 97] Batch 304, Loss 0.1616896539926529\n","[Training Epoch 97] Batch 305, Loss 0.148557648062706\n","[Training Epoch 97] Batch 306, Loss 0.16639205813407898\n","[Training Epoch 97] Batch 307, Loss 0.17631840705871582\n","[Training Epoch 97] Batch 308, Loss 0.16521355509757996\n","[Training Epoch 97] Batch 309, Loss 0.16535954177379608\n","[Training Epoch 97] Batch 310, Loss 0.1451127678155899\n","[Training Epoch 97] Batch 311, Loss 0.16881601512432098\n","[Training Epoch 97] Batch 312, Loss 0.16820983588695526\n","[Training Epoch 97] Batch 313, Loss 0.16975948214530945\n","[Training Epoch 97] Batch 314, Loss 0.1723657250404358\n","[Training Epoch 97] Batch 315, Loss 0.1513230800628662\n","[Training Epoch 97] Batch 316, Loss 0.16728517413139343\n","[Training Epoch 97] Batch 317, Loss 0.1520250141620636\n","[Training Epoch 97] Batch 318, Loss 0.14584526419639587\n","[Training Epoch 97] Batch 319, Loss 0.16240698099136353\n","[Training Epoch 97] Batch 320, Loss 0.18336398899555206\n","[Training Epoch 97] Batch 321, Loss 0.19499751925468445\n","[Training Epoch 97] Batch 322, Loss 0.1740216463804245\n","[Training Epoch 97] Batch 323, Loss 0.17015117406845093\n","[Training Epoch 97] Batch 324, Loss 0.19440853595733643\n","[Training Epoch 97] Batch 325, Loss 0.17620691657066345\n","[Training Epoch 97] Batch 326, Loss 0.1824205368757248\n","[Training Epoch 97] Batch 327, Loss 0.1908862590789795\n","[Training Epoch 97] Batch 328, Loss 0.17282068729400635\n","[Training Epoch 97] Batch 329, Loss 0.1842895746231079\n","[Training Epoch 97] Batch 330, Loss 0.1559983193874359\n","[Training Epoch 97] Batch 331, Loss 0.1673443764448166\n","[Training Epoch 97] Batch 332, Loss 0.166754812002182\n","[Training Epoch 97] Batch 333, Loss 0.14964070916175842\n","[Training Epoch 97] Batch 334, Loss 0.17094582319259644\n","[Training Epoch 97] Batch 335, Loss 0.16761399805545807\n","[Training Epoch 97] Batch 336, Loss 0.15033620595932007\n","[Training Epoch 97] Batch 337, Loss 0.18576322495937347\n","[Training Epoch 97] Batch 338, Loss 0.1746116280555725\n","[Training Epoch 97] Batch 339, Loss 0.15978199243545532\n","[Training Epoch 97] Batch 340, Loss 0.15107037127017975\n","[Training Epoch 97] Batch 341, Loss 0.1762484908103943\n","[Training Epoch 97] Batch 342, Loss 0.17279517650604248\n","[Training Epoch 97] Batch 343, Loss 0.18353427946567535\n","[Training Epoch 97] Batch 344, Loss 0.15187916159629822\n","[Training Epoch 97] Batch 345, Loss 0.16392216086387634\n","[Training Epoch 97] Batch 346, Loss 0.16801278293132782\n","[Training Epoch 97] Batch 347, Loss 0.1580512374639511\n","[Training Epoch 97] Batch 348, Loss 0.14999622106552124\n","[Training Epoch 97] Batch 349, Loss 0.16038888692855835\n","[Training Epoch 97] Batch 350, Loss 0.14983437955379486\n","[Training Epoch 97] Batch 351, Loss 0.19480761885643005\n","[Training Epoch 97] Batch 352, Loss 0.16074028611183167\n","[Training Epoch 97] Batch 353, Loss 0.1665867269039154\n","[Training Epoch 97] Batch 354, Loss 0.17667421698570251\n","[Training Epoch 97] Batch 355, Loss 0.18061906099319458\n","[Training Epoch 97] Batch 356, Loss 0.20191189646720886\n","[Training Epoch 97] Batch 357, Loss 0.17702646553516388\n","[Training Epoch 97] Batch 358, Loss 0.16836968064308167\n","[Training Epoch 97] Batch 359, Loss 0.17147454619407654\n","[Training Epoch 97] Batch 360, Loss 0.18834178149700165\n","[Training Epoch 97] Batch 361, Loss 0.17850323021411896\n","[Training Epoch 97] Batch 362, Loss 0.1761593520641327\n","[Training Epoch 97] Batch 363, Loss 0.16628126800060272\n","[Training Epoch 97] Batch 364, Loss 0.2007465660572052\n","[Training Epoch 97] Batch 365, Loss 0.19468191266059875\n","[Training Epoch 97] Batch 366, Loss 0.16164997220039368\n","[Training Epoch 97] Batch 367, Loss 0.14543604850769043\n","[Training Epoch 97] Batch 368, Loss 0.16459766030311584\n","[Training Epoch 97] Batch 369, Loss 0.1682271510362625\n","[Training Epoch 97] Batch 370, Loss 0.16210928559303284\n","[Training Epoch 97] Batch 371, Loss 0.1606864631175995\n","[Training Epoch 97] Batch 372, Loss 0.19264809787273407\n","[Training Epoch 97] Batch 373, Loss 0.17655456066131592\n","[Training Epoch 97] Batch 374, Loss 0.15300504863262177\n","[Training Epoch 97] Batch 375, Loss 0.16401880979537964\n","[Training Epoch 97] Batch 376, Loss 0.18463005125522614\n","[Training Epoch 97] Batch 377, Loss 0.19179776310920715\n","[Training Epoch 97] Batch 378, Loss 0.1447172462940216\n","[Training Epoch 97] Batch 379, Loss 0.16944070160388947\n","[Training Epoch 97] Batch 380, Loss 0.16556334495544434\n","[Training Epoch 97] Batch 381, Loss 0.1233241930603981\n","[Training Epoch 97] Batch 382, Loss 0.16006408631801605\n","[Training Epoch 97] Batch 383, Loss 0.16345345973968506\n","[Training Epoch 97] Batch 384, Loss 0.167168527841568\n","[Training Epoch 97] Batch 385, Loss 0.18036185204982758\n","[Training Epoch 97] Batch 386, Loss 0.19054733216762543\n","[Training Epoch 97] Batch 387, Loss 0.19795699417591095\n","[Training Epoch 97] Batch 388, Loss 0.1936418116092682\n","[Training Epoch 97] Batch 389, Loss 0.17152869701385498\n","[Training Epoch 97] Batch 390, Loss 0.17737996578216553\n","[Training Epoch 97] Batch 391, Loss 0.15312498807907104\n","[Training Epoch 97] Batch 392, Loss 0.18714427947998047\n","[Training Epoch 97] Batch 393, Loss 0.18626384437084198\n","[Training Epoch 97] Batch 394, Loss 0.16427667438983917\n","[Training Epoch 97] Batch 395, Loss 0.17052461206912994\n","[Training Epoch 97] Batch 396, Loss 0.17534293234348297\n","[Training Epoch 97] Batch 397, Loss 0.1436419039964676\n","[Training Epoch 97] Batch 398, Loss 0.18225522339344025\n","[Training Epoch 97] Batch 399, Loss 0.16079795360565186\n","[Training Epoch 97] Batch 400, Loss 0.171592116355896\n","[Training Epoch 97] Batch 401, Loss 0.15194115042686462\n","[Training Epoch 97] Batch 402, Loss 0.14717629551887512\n","[Training Epoch 97] Batch 403, Loss 0.1407792568206787\n","[Training Epoch 97] Batch 404, Loss 0.1648881733417511\n","[Training Epoch 97] Batch 405, Loss 0.1678115576505661\n","[Training Epoch 97] Batch 406, Loss 0.16396717727184296\n","[Training Epoch 97] Batch 407, Loss 0.15433159470558167\n","[Training Epoch 97] Batch 408, Loss 0.14567668735980988\n","[Training Epoch 97] Batch 409, Loss 0.18875637650489807\n","[Training Epoch 97] Batch 410, Loss 0.17153149843215942\n","[Training Epoch 97] Batch 411, Loss 0.17671461403369904\n","[Training Epoch 97] Batch 412, Loss 0.17133599519729614\n","[Training Epoch 97] Batch 413, Loss 0.16718612611293793\n","[Training Epoch 97] Batch 414, Loss 0.15003511309623718\n","[Training Epoch 97] Batch 415, Loss 0.16059955954551697\n","[Training Epoch 97] Batch 416, Loss 0.163238525390625\n","[Training Epoch 97] Batch 417, Loss 0.16141101717948914\n","[Training Epoch 97] Batch 418, Loss 0.18225038051605225\n","[Training Epoch 97] Batch 419, Loss 0.1753324568271637\n","[Training Epoch 97] Batch 420, Loss 0.19784557819366455\n","[Training Epoch 97] Batch 421, Loss 0.15826161205768585\n","[Training Epoch 97] Batch 422, Loss 0.15523023903369904\n","[Training Epoch 97] Batch 423, Loss 0.16244256496429443\n","[Training Epoch 97] Batch 424, Loss 0.16174089908599854\n","[Training Epoch 97] Batch 425, Loss 0.15283381938934326\n","[Training Epoch 97] Batch 426, Loss 0.16048698127269745\n","[Training Epoch 97] Batch 427, Loss 0.1663207709789276\n","[Training Epoch 97] Batch 428, Loss 0.15185143053531647\n","[Training Epoch 97] Batch 429, Loss 0.15366356074810028\n","[Training Epoch 97] Batch 430, Loss 0.15525931119918823\n","[Training Epoch 97] Batch 431, Loss 0.1793377846479416\n","[Training Epoch 97] Batch 432, Loss 0.16870194673538208\n","[Training Epoch 97] Batch 433, Loss 0.17493242025375366\n","[Training Epoch 97] Batch 434, Loss 0.1586465835571289\n","[Training Epoch 97] Batch 435, Loss 0.1539636254310608\n","[Training Epoch 97] Batch 436, Loss 0.17812412977218628\n","[Training Epoch 97] Batch 437, Loss 0.1663796752691269\n","[Training Epoch 97] Batch 438, Loss 0.16900433599948883\n","[Training Epoch 97] Batch 439, Loss 0.18128448724746704\n","[Training Epoch 97] Batch 440, Loss 0.15920758247375488\n","[Training Epoch 97] Batch 441, Loss 0.151656836271286\n","[Training Epoch 97] Batch 442, Loss 0.16442710161209106\n","[Training Epoch 97] Batch 443, Loss 0.16041724383831024\n","[Training Epoch 97] Batch 444, Loss 0.1647375524044037\n","[Training Epoch 97] Batch 445, Loss 0.17102587223052979\n","[Training Epoch 97] Batch 446, Loss 0.17306648194789886\n","[Training Epoch 97] Batch 447, Loss 0.17620286345481873\n","[Training Epoch 97] Batch 448, Loss 0.16270771622657776\n","[Training Epoch 97] Batch 449, Loss 0.16077573597431183\n","[Training Epoch 97] Batch 450, Loss 0.15071803331375122\n","[Training Epoch 97] Batch 451, Loss 0.17217576503753662\n","[Training Epoch 97] Batch 452, Loss 0.14767736196517944\n","[Training Epoch 97] Batch 453, Loss 0.1566040962934494\n","[Training Epoch 97] Batch 454, Loss 0.1596677303314209\n","[Training Epoch 97] Batch 455, Loss 0.17003388702869415\n","[Training Epoch 97] Batch 456, Loss 0.13922108709812164\n","[Training Epoch 97] Batch 457, Loss 0.15709444880485535\n","[Training Epoch 97] Batch 458, Loss 0.16478121280670166\n","[Training Epoch 97] Batch 459, Loss 0.16943615674972534\n","[Training Epoch 97] Batch 460, Loss 0.16448843479156494\n","[Training Epoch 97] Batch 461, Loss 0.16252024471759796\n","[Training Epoch 97] Batch 462, Loss 0.1716797947883606\n","[Training Epoch 97] Batch 463, Loss 0.17682839930057526\n","[Training Epoch 97] Batch 464, Loss 0.15537573397159576\n","[Training Epoch 97] Batch 465, Loss 0.14872881770133972\n","[Training Epoch 97] Batch 466, Loss 0.14134371280670166\n","[Training Epoch 97] Batch 467, Loss 0.1603596806526184\n","[Training Epoch 97] Batch 468, Loss 0.1588633954524994\n","[Training Epoch 97] Batch 469, Loss 0.154355987906456\n","[Training Epoch 97] Batch 470, Loss 0.17893174290657043\n","[Training Epoch 97] Batch 471, Loss 0.17019854485988617\n","[Training Epoch 97] Batch 472, Loss 0.15961726009845734\n","[Training Epoch 97] Batch 473, Loss 0.15695038437843323\n","[Training Epoch 97] Batch 474, Loss 0.16317863762378693\n","[Training Epoch 97] Batch 475, Loss 0.16177260875701904\n","[Training Epoch 97] Batch 476, Loss 0.18072982132434845\n","[Training Epoch 97] Batch 477, Loss 0.18584442138671875\n","[Training Epoch 97] Batch 478, Loss 0.16229966282844543\n","[Training Epoch 97] Batch 479, Loss 0.171475350856781\n","[Training Epoch 97] Batch 480, Loss 0.1953817903995514\n","[Training Epoch 97] Batch 481, Loss 0.18143291771411896\n","[Training Epoch 97] Batch 482, Loss 0.15547208487987518\n","[Training Epoch 97] Batch 483, Loss 0.1558195799589157\n","[Training Epoch 97] Batch 484, Loss 0.17031285166740417\n","[Training Epoch 97] Batch 485, Loss 0.17483188211917877\n","[Training Epoch 97] Batch 486, Loss 0.17534634470939636\n","[Training Epoch 97] Batch 487, Loss 0.18761000037193298\n","[Training Epoch 97] Batch 488, Loss 0.1612410843372345\n","[Training Epoch 97] Batch 489, Loss 0.16215309500694275\n","[Training Epoch 97] Batch 490, Loss 0.1706932634115219\n","[Training Epoch 97] Batch 491, Loss 0.16840903460979462\n","[Training Epoch 97] Batch 492, Loss 0.16075223684310913\n","[Training Epoch 97] Batch 493, Loss 0.1584109216928482\n","[Training Epoch 97] Batch 494, Loss 0.15576797723770142\n","[Training Epoch 97] Batch 495, Loss 0.17671671509742737\n","[Training Epoch 97] Batch 496, Loss 0.15481089055538177\n","[Training Epoch 97] Batch 497, Loss 0.1662588119506836\n","[Training Epoch 97] Batch 498, Loss 0.15571564435958862\n","[Training Epoch 97] Batch 499, Loss 0.17315679788589478\n","[Training Epoch 97] Batch 500, Loss 0.16611844301223755\n","[Training Epoch 97] Batch 501, Loss 0.16580553352832794\n","[Training Epoch 97] Batch 502, Loss 0.19809988141059875\n","[Training Epoch 97] Batch 503, Loss 0.16340778768062592\n","[Training Epoch 97] Batch 504, Loss 0.16460634768009186\n","[Training Epoch 97] Batch 505, Loss 0.19347169995307922\n","[Training Epoch 97] Batch 506, Loss 0.17285406589508057\n","[Training Epoch 97] Batch 507, Loss 0.1407400518655777\n","[Training Epoch 97] Batch 508, Loss 0.1621665060520172\n","[Training Epoch 97] Batch 509, Loss 0.17641359567642212\n","[Training Epoch 97] Batch 510, Loss 0.15278193354606628\n","[Training Epoch 97] Batch 511, Loss 0.15652665495872498\n","[Training Epoch 97] Batch 512, Loss 0.15817758440971375\n","[Training Epoch 97] Batch 513, Loss 0.16274073719978333\n","[Training Epoch 97] Batch 514, Loss 0.15180757641792297\n","[Training Epoch 97] Batch 515, Loss 0.1732637882232666\n","[Training Epoch 97] Batch 516, Loss 0.17201605439186096\n","[Training Epoch 97] Batch 517, Loss 0.15117421746253967\n","[Training Epoch 97] Batch 518, Loss 0.16690847277641296\n","[Training Epoch 97] Batch 519, Loss 0.1587303876876831\n","[Training Epoch 97] Batch 520, Loss 0.1676778793334961\n","[Training Epoch 97] Batch 521, Loss 0.17446720600128174\n","[Training Epoch 97] Batch 522, Loss 0.1783127337694168\n","[Training Epoch 97] Batch 523, Loss 0.1599845290184021\n","[Training Epoch 97] Batch 524, Loss 0.18015944957733154\n","[Training Epoch 97] Batch 525, Loss 0.1808822751045227\n","[Training Epoch 97] Batch 526, Loss 0.14729395508766174\n","[Training Epoch 97] Batch 527, Loss 0.1690652221441269\n","[Training Epoch 97] Batch 528, Loss 0.1660492718219757\n","[Training Epoch 97] Batch 529, Loss 0.1666756570339203\n","[Training Epoch 97] Batch 530, Loss 0.16822302341461182\n","[Training Epoch 97] Batch 531, Loss 0.1586390733718872\n","[Training Epoch 97] Batch 532, Loss 0.18805062770843506\n","[Training Epoch 97] Batch 533, Loss 0.16047778725624084\n","[Training Epoch 97] Batch 534, Loss 0.15428471565246582\n","[Training Epoch 97] Batch 535, Loss 0.17784032225608826\n","[Training Epoch 97] Batch 536, Loss 0.1674048900604248\n","[Training Epoch 97] Batch 537, Loss 0.15197968482971191\n","[Training Epoch 97] Batch 538, Loss 0.17272749543190002\n","[Training Epoch 97] Batch 539, Loss 0.17345090210437775\n","[Training Epoch 97] Batch 540, Loss 0.20759078860282898\n","[Training Epoch 97] Batch 541, Loss 0.18121400475502014\n","[Training Epoch 97] Batch 542, Loss 0.16345185041427612\n","[Training Epoch 97] Batch 543, Loss 0.15163299441337585\n","[Training Epoch 97] Batch 544, Loss 0.15491428971290588\n","[Training Epoch 97] Batch 545, Loss 0.18566513061523438\n","[Training Epoch 97] Batch 546, Loss 0.17051446437835693\n","[Training Epoch 97] Batch 547, Loss 0.14863918721675873\n","[Training Epoch 97] Batch 548, Loss 0.15281802415847778\n","[Training Epoch 97] Batch 549, Loss 0.16779866814613342\n","[Training Epoch 97] Batch 550, Loss 0.1851862668991089\n","[Training Epoch 97] Batch 551, Loss 0.1639849692583084\n","[Training Epoch 97] Batch 552, Loss 0.16915729641914368\n","[Training Epoch 97] Batch 553, Loss 0.1635255068540573\n","[Training Epoch 97] Batch 554, Loss 0.170346200466156\n","[Training Epoch 97] Batch 555, Loss 0.1701059341430664\n","[Training Epoch 97] Batch 556, Loss 0.15869130194187164\n","[Training Epoch 97] Batch 557, Loss 0.180508553981781\n","[Training Epoch 97] Batch 558, Loss 0.14057745039463043\n","[Training Epoch 97] Batch 559, Loss 0.1759071946144104\n","[Training Epoch 97] Batch 560, Loss 0.1746101677417755\n","[Training Epoch 97] Batch 561, Loss 0.15337878465652466\n","[Training Epoch 97] Batch 562, Loss 0.16784504055976868\n","[Training Epoch 97] Batch 563, Loss 0.16408896446228027\n","[Training Epoch 97] Batch 564, Loss 0.19768238067626953\n","[Training Epoch 97] Batch 565, Loss 0.15468907356262207\n","[Training Epoch 97] Batch 566, Loss 0.17788109183311462\n","[Training Epoch 97] Batch 567, Loss 0.1699269562959671\n","[Training Epoch 97] Batch 568, Loss 0.17882205545902252\n","[Training Epoch 97] Batch 569, Loss 0.1721566915512085\n","[Training Epoch 97] Batch 570, Loss 0.19688555598258972\n","[Training Epoch 97] Batch 571, Loss 0.15948837995529175\n","[Training Epoch 97] Batch 572, Loss 0.18130679428577423\n","[Training Epoch 97] Batch 573, Loss 0.16170871257781982\n","[Training Epoch 97] Batch 574, Loss 0.18389341235160828\n","[Training Epoch 97] Batch 575, Loss 0.1653939187526703\n","[Training Epoch 97] Batch 576, Loss 0.15979337692260742\n","[Training Epoch 97] Batch 577, Loss 0.18048357963562012\n","[Training Epoch 97] Batch 578, Loss 0.17694124579429626\n","[Training Epoch 97] Batch 579, Loss 0.15349330008029938\n","[Training Epoch 97] Batch 580, Loss 0.15353122353553772\n","[Training Epoch 97] Batch 581, Loss 0.1532248854637146\n","[Training Epoch 97] Batch 582, Loss 0.16549274325370789\n","[Training Epoch 97] Batch 583, Loss 0.1656590849161148\n","[Training Epoch 97] Batch 584, Loss 0.18314574658870697\n","[Training Epoch 97] Batch 585, Loss 0.1618463695049286\n","[Training Epoch 97] Batch 586, Loss 0.1744377762079239\n","[Training Epoch 97] Batch 587, Loss 0.16865649819374084\n","[Training Epoch 97] Batch 588, Loss 0.1717376708984375\n","[Training Epoch 97] Batch 589, Loss 0.15586578845977783\n","[Training Epoch 97] Batch 590, Loss 0.15148474276065826\n","[Training Epoch 97] Batch 591, Loss 0.1587999314069748\n","[Training Epoch 97] Batch 592, Loss 0.17161136865615845\n","[Training Epoch 97] Batch 593, Loss 0.15792536735534668\n","[Training Epoch 97] Batch 594, Loss 0.15472015738487244\n","[Training Epoch 97] Batch 595, Loss 0.16423892974853516\n","[Training Epoch 97] Batch 596, Loss 0.169530987739563\n","[Training Epoch 97] Batch 597, Loss 0.17441704869270325\n","[Training Epoch 97] Batch 598, Loss 0.1593484878540039\n","[Training Epoch 97] Batch 599, Loss 0.1661812663078308\n","[Training Epoch 97] Batch 600, Loss 0.15518294274806976\n","[Training Epoch 97] Batch 601, Loss 0.18261346220970154\n","[Training Epoch 97] Batch 602, Loss 0.13702699542045593\n","[Training Epoch 97] Batch 603, Loss 0.1719975769519806\n","[Training Epoch 97] Batch 604, Loss 0.15104036033153534\n","[Training Epoch 97] Batch 605, Loss 0.17019537091255188\n","[Training Epoch 97] Batch 606, Loss 0.1649966835975647\n","[Training Epoch 97] Batch 607, Loss 0.17778263986110687\n","[Training Epoch 97] Batch 608, Loss 0.1484295278787613\n","[Training Epoch 97] Batch 609, Loss 0.18524153530597687\n","[Training Epoch 97] Batch 610, Loss 0.16729016602039337\n","[Training Epoch 97] Batch 611, Loss 0.14866572618484497\n","[Training Epoch 97] Batch 612, Loss 0.16596649587154388\n","[Training Epoch 97] Batch 613, Loss 0.17479866743087769\n","[Training Epoch 97] Batch 614, Loss 0.15696877241134644\n","[Training Epoch 97] Batch 615, Loss 0.18200188875198364\n","[Training Epoch 97] Batch 616, Loss 0.16490459442138672\n","[Training Epoch 97] Batch 617, Loss 0.13957303762435913\n","[Training Epoch 97] Batch 618, Loss 0.1691938042640686\n","[Training Epoch 97] Batch 619, Loss 0.15685272216796875\n","[Training Epoch 97] Batch 620, Loss 0.17098723351955414\n","[Training Epoch 97] Batch 621, Loss 0.15004348754882812\n","[Training Epoch 97] Batch 622, Loss 0.18085893988609314\n","[Training Epoch 97] Batch 623, Loss 0.18431499600410461\n","[Training Epoch 97] Batch 624, Loss 0.1776270866394043\n","[Training Epoch 97] Batch 625, Loss 0.1532129943370819\n","[Training Epoch 97] Batch 626, Loss 0.1865001916885376\n","[Training Epoch 97] Batch 627, Loss 0.14541751146316528\n","[Training Epoch 97] Batch 628, Loss 0.2015683650970459\n","[Training Epoch 97] Batch 629, Loss 0.16352833807468414\n","[Training Epoch 97] Batch 630, Loss 0.1652202010154724\n","[Training Epoch 97] Batch 631, Loss 0.16295510530471802\n","[Training Epoch 97] Batch 632, Loss 0.1615258753299713\n","[Training Epoch 97] Batch 633, Loss 0.18465930223464966\n","[Training Epoch 97] Batch 634, Loss 0.17941221594810486\n","[Training Epoch 97] Batch 635, Loss 0.16385018825531006\n","[Training Epoch 97] Batch 636, Loss 0.18326105177402496\n","[Training Epoch 97] Batch 637, Loss 0.16450542211532593\n","[Training Epoch 97] Batch 638, Loss 0.18041664361953735\n","[Training Epoch 97] Batch 639, Loss 0.1729407012462616\n","[Training Epoch 97] Batch 640, Loss 0.17822515964508057\n","[Training Epoch 97] Batch 641, Loss 0.16499242186546326\n","[Training Epoch 97] Batch 642, Loss 0.15623311698436737\n","[Training Epoch 97] Batch 643, Loss 0.18053679168224335\n","[Training Epoch 97] Batch 644, Loss 0.15545222163200378\n","[Training Epoch 97] Batch 645, Loss 0.1579953283071518\n","[Training Epoch 97] Batch 646, Loss 0.16503971815109253\n","[Training Epoch 97] Batch 647, Loss 0.17829062044620514\n","[Training Epoch 97] Batch 648, Loss 0.1684824526309967\n","[Training Epoch 97] Batch 649, Loss 0.16920995712280273\n","[Training Epoch 97] Batch 650, Loss 0.18393874168395996\n","[Training Epoch 97] Batch 651, Loss 0.15414756536483765\n","[Training Epoch 97] Batch 652, Loss 0.15975400805473328\n","[Training Epoch 97] Batch 653, Loss 0.1537863314151764\n","[Training Epoch 97] Batch 654, Loss 0.1483362913131714\n","[Training Epoch 97] Batch 655, Loss 0.14819341897964478\n","[Training Epoch 97] Batch 656, Loss 0.1678386926651001\n","[Training Epoch 97] Batch 657, Loss 0.1360148936510086\n","[Training Epoch 97] Batch 658, Loss 0.146900936961174\n","[Training Epoch 97] Batch 659, Loss 0.16252188384532928\n","[Training Epoch 97] Batch 660, Loss 0.1804870069026947\n","[Training Epoch 97] Batch 661, Loss 0.15210171043872833\n","[Training Epoch 97] Batch 662, Loss 0.16798534989356995\n","[Training Epoch 97] Batch 663, Loss 0.17905782163143158\n","[Training Epoch 97] Batch 664, Loss 0.15563198924064636\n","[Training Epoch 97] Batch 665, Loss 0.15042364597320557\n","[Training Epoch 97] Batch 666, Loss 0.17215263843536377\n","[Training Epoch 97] Batch 667, Loss 0.1876370757818222\n","[Training Epoch 97] Batch 668, Loss 0.16450995206832886\n","[Training Epoch 97] Batch 669, Loss 0.18365028500556946\n","[Training Epoch 97] Batch 670, Loss 0.1888219714164734\n","[Training Epoch 97] Batch 671, Loss 0.19012722373008728\n","[Training Epoch 97] Batch 672, Loss 0.16873623430728912\n","[Training Epoch 97] Batch 673, Loss 0.17912256717681885\n","[Training Epoch 97] Batch 674, Loss 0.16762901842594147\n","[Training Epoch 97] Batch 675, Loss 0.17379741370677948\n","[Training Epoch 97] Batch 676, Loss 0.1651161015033722\n","[Training Epoch 97] Batch 677, Loss 0.17863790690898895\n","[Training Epoch 97] Batch 678, Loss 0.18977396190166473\n","[Training Epoch 97] Batch 679, Loss 0.18437165021896362\n","[Training Epoch 97] Batch 680, Loss 0.16319677233695984\n","[Training Epoch 97] Batch 681, Loss 0.17082057893276215\n","[Training Epoch 97] Batch 682, Loss 0.18314090371131897\n","[Training Epoch 97] Batch 683, Loss 0.1635381132364273\n","[Training Epoch 97] Batch 684, Loss 0.1789064109325409\n","[Training Epoch 97] Batch 685, Loss 0.16661623120307922\n","[Training Epoch 97] Batch 686, Loss 0.17949515581130981\n","[Training Epoch 97] Batch 687, Loss 0.17685875296592712\n","[Training Epoch 97] Batch 688, Loss 0.16121461987495422\n","[Training Epoch 97] Batch 689, Loss 0.16480249166488647\n","[Training Epoch 97] Batch 690, Loss 0.19089001417160034\n","[Training Epoch 97] Batch 691, Loss 0.17360061407089233\n","[Training Epoch 97] Batch 692, Loss 0.16368257999420166\n","[Training Epoch 97] Batch 693, Loss 0.18511945009231567\n","[Training Epoch 97] Batch 694, Loss 0.154616117477417\n","[Training Epoch 97] Batch 695, Loss 0.1650269776582718\n","[Training Epoch 97] Batch 696, Loss 0.16626906394958496\n","[Training Epoch 97] Batch 697, Loss 0.1606261134147644\n","[Training Epoch 97] Batch 698, Loss 0.15980908274650574\n","[Training Epoch 97] Batch 699, Loss 0.14431756734848022\n","[Training Epoch 97] Batch 700, Loss 0.17301753163337708\n","[Training Epoch 97] Batch 701, Loss 0.1819230318069458\n","[Training Epoch 97] Batch 702, Loss 0.16395291686058044\n","[Training Epoch 97] Batch 703, Loss 0.1560308039188385\n","[Training Epoch 97] Batch 704, Loss 0.15448221564292908\n","[Training Epoch 97] Batch 705, Loss 0.16174155473709106\n","[Training Epoch 97] Batch 706, Loss 0.1725756824016571\n","[Training Epoch 97] Batch 707, Loss 0.13762758672237396\n","[Training Epoch 97] Batch 708, Loss 0.15074442327022552\n","[Training Epoch 97] Batch 709, Loss 0.17857226729393005\n","[Training Epoch 97] Batch 710, Loss 0.14593622088432312\n","[Training Epoch 97] Batch 711, Loss 0.1590091437101364\n","[Training Epoch 97] Batch 712, Loss 0.18131643533706665\n","[Training Epoch 97] Batch 713, Loss 0.17997300624847412\n","[Training Epoch 97] Batch 714, Loss 0.17608843743801117\n","[Training Epoch 97] Batch 715, Loss 0.15690520405769348\n","[Training Epoch 97] Batch 716, Loss 0.1663881242275238\n","[Training Epoch 97] Batch 717, Loss 0.17436179518699646\n","[Training Epoch 97] Batch 718, Loss 0.18977104127407074\n","[Training Epoch 97] Batch 719, Loss 0.1722916066646576\n","[Training Epoch 97] Batch 720, Loss 0.14915072917938232\n","[Training Epoch 97] Batch 721, Loss 0.17949894070625305\n","[Training Epoch 97] Batch 722, Loss 0.1756400167942047\n","[Training Epoch 97] Batch 723, Loss 0.160701721906662\n","[Training Epoch 97] Batch 724, Loss 0.15459099411964417\n","[Training Epoch 97] Batch 725, Loss 0.16331791877746582\n","[Training Epoch 97] Batch 726, Loss 0.17460592091083527\n","[Training Epoch 97] Batch 727, Loss 0.1692718267440796\n","[Training Epoch 97] Batch 728, Loss 0.21602939069271088\n","[Training Epoch 97] Batch 729, Loss 0.1687554121017456\n","[Training Epoch 97] Batch 730, Loss 0.17174649238586426\n","[Training Epoch 97] Batch 731, Loss 0.18437954783439636\n","[Training Epoch 97] Batch 732, Loss 0.18127372860908508\n","[Training Epoch 97] Batch 733, Loss 0.16167159378528595\n","[Training Epoch 97] Batch 734, Loss 0.1816178858280182\n","[Training Epoch 97] Batch 735, Loss 0.17030684649944305\n","[Training Epoch 97] Batch 736, Loss 0.16630017757415771\n","[Training Epoch 97] Batch 737, Loss 0.1961519867181778\n","[Training Epoch 97] Batch 738, Loss 0.15180978178977966\n","[Training Epoch 97] Batch 739, Loss 0.1802809089422226\n","[Training Epoch 97] Batch 740, Loss 0.14722350239753723\n","[Training Epoch 97] Batch 741, Loss 0.16192729771137238\n","[Training Epoch 97] Batch 742, Loss 0.16520962119102478\n","[Training Epoch 97] Batch 743, Loss 0.17893069982528687\n","[Training Epoch 97] Batch 744, Loss 0.17992237210273743\n","[Training Epoch 97] Batch 745, Loss 0.1620694249868393\n","[Training Epoch 97] Batch 746, Loss 0.19705459475517273\n","[Training Epoch 97] Batch 747, Loss 0.16434860229492188\n","[Training Epoch 97] Batch 748, Loss 0.18268151581287384\n","[Training Epoch 97] Batch 749, Loss 0.1678306609392166\n","[Training Epoch 97] Batch 750, Loss 0.15906980633735657\n","[Training Epoch 97] Batch 751, Loss 0.1547858566045761\n","[Training Epoch 97] Batch 752, Loss 0.16805052757263184\n","[Training Epoch 97] Batch 753, Loss 0.1638350635766983\n","[Training Epoch 97] Batch 754, Loss 0.20365683734416962\n","[Training Epoch 97] Batch 755, Loss 0.176110178232193\n","[Training Epoch 97] Batch 756, Loss 0.19494113326072693\n","[Training Epoch 97] Batch 757, Loss 0.1437438428401947\n","[Training Epoch 97] Batch 758, Loss 0.16050396859645844\n","[Training Epoch 97] Batch 759, Loss 0.1525995433330536\n","[Training Epoch 97] Batch 760, Loss 0.1694115400314331\n","[Training Epoch 97] Batch 761, Loss 0.19328083097934723\n","[Training Epoch 97] Batch 762, Loss 0.1818333864212036\n","[Training Epoch 97] Batch 763, Loss 0.1658097803592682\n","[Training Epoch 97] Batch 764, Loss 0.18463796377182007\n","[Training Epoch 97] Batch 765, Loss 0.15441396832466125\n","[Training Epoch 97] Batch 766, Loss 0.15019598603248596\n","[Training Epoch 97] Batch 767, Loss 0.16570663452148438\n","[Training Epoch 97] Batch 768, Loss 0.18515463173389435\n","[Training Epoch 97] Batch 769, Loss 0.16855332255363464\n","[Training Epoch 97] Batch 770, Loss 0.18464377522468567\n","[Training Epoch 97] Batch 771, Loss 0.16817013919353485\n","[Training Epoch 97] Batch 772, Loss 0.16417986154556274\n","[Training Epoch 97] Batch 773, Loss 0.19260331988334656\n","[Training Epoch 97] Batch 774, Loss 0.16935160756111145\n","[Training Epoch 97] Batch 775, Loss 0.15195871889591217\n","[Training Epoch 97] Batch 776, Loss 0.15074731409549713\n","[Training Epoch 97] Batch 777, Loss 0.14991992712020874\n","[Training Epoch 97] Batch 778, Loss 0.17734426259994507\n","[Training Epoch 97] Batch 779, Loss 0.15027174353599548\n","[Training Epoch 97] Batch 780, Loss 0.16558128595352173\n","[Training Epoch 97] Batch 781, Loss 0.168494313955307\n","[Training Epoch 97] Batch 782, Loss 0.1446584165096283\n","[Training Epoch 97] Batch 783, Loss 0.16829082369804382\n","[Training Epoch 97] Batch 784, Loss 0.13979923725128174\n","[Training Epoch 97] Batch 785, Loss 0.15518587827682495\n","[Training Epoch 97] Batch 786, Loss 0.14857470989227295\n","[Training Epoch 97] Batch 787, Loss 0.17201539874076843\n","[Training Epoch 97] Batch 788, Loss 0.16945430636405945\n","[Training Epoch 97] Batch 789, Loss 0.1767634004354477\n","[Training Epoch 97] Batch 790, Loss 0.16764122247695923\n","[Training Epoch 97] Batch 791, Loss 0.1832846999168396\n","[Training Epoch 97] Batch 792, Loss 0.15759170055389404\n","[Training Epoch 97] Batch 793, Loss 0.1833980530500412\n","[Training Epoch 97] Batch 794, Loss 0.16961246728897095\n","[Training Epoch 97] Batch 795, Loss 0.1648743450641632\n","[Training Epoch 97] Batch 796, Loss 0.17916861176490784\n","[Training Epoch 97] Batch 797, Loss 0.14601963758468628\n","[Training Epoch 97] Batch 798, Loss 0.17970240116119385\n","[Training Epoch 97] Batch 799, Loss 0.1568354070186615\n","[Training Epoch 97] Batch 800, Loss 0.18575620651245117\n","[Training Epoch 97] Batch 801, Loss 0.15595242381095886\n","[Training Epoch 97] Batch 802, Loss 0.17643076181411743\n","[Training Epoch 97] Batch 803, Loss 0.1467759609222412\n","[Training Epoch 97] Batch 804, Loss 0.15540361404418945\n","[Training Epoch 97] Batch 805, Loss 0.1721550077199936\n","[Training Epoch 97] Batch 806, Loss 0.16501939296722412\n","[Training Epoch 97] Batch 807, Loss 0.15593679249286652\n","[Training Epoch 97] Batch 808, Loss 0.1791626662015915\n","[Training Epoch 97] Batch 809, Loss 0.16848644614219666\n","[Training Epoch 97] Batch 810, Loss 0.1516895592212677\n","[Training Epoch 97] Batch 811, Loss 0.15827304124832153\n","[Training Epoch 97] Batch 812, Loss 0.16932089626789093\n","[Training Epoch 97] Batch 813, Loss 0.15057030320167542\n","[Training Epoch 97] Batch 814, Loss 0.16320163011550903\n","[Training Epoch 97] Batch 815, Loss 0.17663171887397766\n","[Training Epoch 97] Batch 816, Loss 0.14358502626419067\n","[Training Epoch 97] Batch 817, Loss 0.16892726719379425\n","[Training Epoch 97] Batch 818, Loss 0.1966753900051117\n","[Training Epoch 97] Batch 819, Loss 0.15380644798278809\n","[Training Epoch 97] Batch 820, Loss 0.17274148762226105\n","[Training Epoch 97] Batch 821, Loss 0.18105977773666382\n","[Training Epoch 97] Batch 822, Loss 0.17323385179042816\n","[Training Epoch 97] Batch 823, Loss 0.16707046329975128\n","[Training Epoch 97] Batch 824, Loss 0.15476860105991364\n","[Training Epoch 97] Batch 825, Loss 0.15974608063697815\n","[Training Epoch 97] Batch 826, Loss 0.1710270345211029\n","[Training Epoch 97] Batch 827, Loss 0.17361928522586823\n","[Training Epoch 97] Batch 828, Loss 0.15338271856307983\n","[Training Epoch 97] Batch 829, Loss 0.17902740836143494\n","[Training Epoch 97] Batch 830, Loss 0.19257065653800964\n","[Training Epoch 97] Batch 831, Loss 0.1717069298028946\n","[Training Epoch 97] Batch 832, Loss 0.15624964237213135\n","[Training Epoch 97] Batch 833, Loss 0.18538793921470642\n","[Training Epoch 97] Batch 834, Loss 0.17756476998329163\n","[Training Epoch 97] Batch 835, Loss 0.17903190851211548\n","[Training Epoch 97] Batch 836, Loss 0.1496734470129013\n","[Training Epoch 97] Batch 837, Loss 0.1568184196949005\n","[Training Epoch 97] Batch 838, Loss 0.19739291071891785\n","[Training Epoch 97] Batch 839, Loss 0.1573161482810974\n","[Training Epoch 97] Batch 840, Loss 0.16339567303657532\n","[Training Epoch 97] Batch 841, Loss 0.1688101887702942\n","[Training Epoch 97] Batch 842, Loss 0.17144057154655457\n","[Training Epoch 97] Batch 843, Loss 0.17815391719341278\n","[Training Epoch 97] Batch 844, Loss 0.15164776146411896\n","[Training Epoch 97] Batch 845, Loss 0.19286514818668365\n","[Training Epoch 97] Batch 846, Loss 0.1580151468515396\n","[Training Epoch 97] Batch 847, Loss 0.15606869757175446\n","[Training Epoch 97] Batch 848, Loss 0.1659286618232727\n","[Training Epoch 97] Batch 849, Loss 0.16551516950130463\n","[Training Epoch 97] Batch 850, Loss 0.1397669017314911\n","[Training Epoch 97] Batch 851, Loss 0.16682898998260498\n","[Training Epoch 97] Batch 852, Loss 0.17688046395778656\n","[Training Epoch 97] Batch 853, Loss 0.1506451666355133\n","[Training Epoch 97] Batch 854, Loss 0.1757390946149826\n","[Training Epoch 97] Batch 855, Loss 0.19257752597332\n","[Training Epoch 97] Batch 856, Loss 0.1667861044406891\n","[Training Epoch 97] Batch 857, Loss 0.14908179640769958\n","[Training Epoch 97] Batch 858, Loss 0.17059093713760376\n","[Training Epoch 97] Batch 859, Loss 0.1767682433128357\n","[Training Epoch 97] Batch 860, Loss 0.15609312057495117\n","[Training Epoch 97] Batch 861, Loss 0.15862154960632324\n","[Training Epoch 97] Batch 862, Loss 0.17802031338214874\n","[Training Epoch 97] Batch 863, Loss 0.16677740216255188\n","[Training Epoch 97] Batch 864, Loss 0.16214671730995178\n","[Training Epoch 97] Batch 865, Loss 0.18430519104003906\n","[Training Epoch 97] Batch 866, Loss 0.1611059606075287\n","[Training Epoch 97] Batch 867, Loss 0.18050074577331543\n","[Training Epoch 97] Batch 868, Loss 0.17165717482566833\n","[Training Epoch 97] Batch 869, Loss 0.17412680387496948\n","[Training Epoch 97] Batch 870, Loss 0.15452232956886292\n","[Training Epoch 97] Batch 871, Loss 0.16664241254329681\n","[Training Epoch 97] Batch 872, Loss 0.18564580380916595\n","[Training Epoch 97] Batch 873, Loss 0.16395418345928192\n","[Training Epoch 97] Batch 874, Loss 0.16702140867710114\n","[Training Epoch 97] Batch 875, Loss 0.1529015600681305\n","[Training Epoch 97] Batch 876, Loss 0.17257288098335266\n","[Training Epoch 97] Batch 877, Loss 0.17367717623710632\n","[Training Epoch 97] Batch 878, Loss 0.16396883130073547\n","[Training Epoch 97] Batch 879, Loss 0.1643979847431183\n","[Training Epoch 97] Batch 880, Loss 0.16010811924934387\n","[Training Epoch 97] Batch 881, Loss 0.17107951641082764\n","[Training Epoch 97] Batch 882, Loss 0.15525181591510773\n","[Training Epoch 97] Batch 883, Loss 0.15989571809768677\n","[Training Epoch 97] Batch 884, Loss 0.1611410230398178\n","[Training Epoch 97] Batch 885, Loss 0.15756799280643463\n","[Training Epoch 97] Batch 886, Loss 0.17284388840198517\n","[Training Epoch 97] Batch 887, Loss 0.16758158802986145\n","[Training Epoch 97] Batch 888, Loss 0.18541350960731506\n","[Training Epoch 97] Batch 889, Loss 0.1569862961769104\n","[Training Epoch 97] Batch 890, Loss 0.17868071794509888\n","[Training Epoch 97] Batch 891, Loss 0.1867588311433792\n","[Training Epoch 97] Batch 892, Loss 0.1652383953332901\n","[Training Epoch 97] Batch 893, Loss 0.1740918755531311\n","[Training Epoch 97] Batch 894, Loss 0.16980502009391785\n","[Training Epoch 97] Batch 895, Loss 0.16092310845851898\n","[Training Epoch 97] Batch 896, Loss 0.1845587193965912\n","[Training Epoch 97] Batch 897, Loss 0.16063721477985382\n","[Training Epoch 97] Batch 898, Loss 0.17830108106136322\n","[Training Epoch 97] Batch 899, Loss 0.15791769325733185\n","[Training Epoch 97] Batch 900, Loss 0.15933449566364288\n","[Training Epoch 97] Batch 901, Loss 0.188364177942276\n","[Training Epoch 97] Batch 902, Loss 0.1511271893978119\n","[Training Epoch 97] Batch 903, Loss 0.16640284657478333\n","[Training Epoch 97] Batch 904, Loss 0.1579534113407135\n","[Training Epoch 97] Batch 905, Loss 0.14164799451828003\n","[Training Epoch 97] Batch 906, Loss 0.1475517451763153\n","[Training Epoch 97] Batch 907, Loss 0.1803530603647232\n","[Training Epoch 97] Batch 908, Loss 0.16424903273582458\n","[Training Epoch 97] Batch 909, Loss 0.17622347176074982\n","[Training Epoch 97] Batch 910, Loss 0.16745656728744507\n","[Training Epoch 97] Batch 911, Loss 0.166727215051651\n","[Training Epoch 97] Batch 912, Loss 0.1978393793106079\n","[Training Epoch 97] Batch 913, Loss 0.1559326946735382\n","[Training Epoch 97] Batch 914, Loss 0.1785963773727417\n","[Training Epoch 97] Batch 915, Loss 0.15561196208000183\n","[Training Epoch 97] Batch 916, Loss 0.16834694147109985\n","[Training Epoch 97] Batch 917, Loss 0.16291628777980804\n","[Training Epoch 97] Batch 918, Loss 0.154511958360672\n","[Training Epoch 97] Batch 919, Loss 0.18602371215820312\n","[Training Epoch 97] Batch 920, Loss 0.1697475016117096\n","[Training Epoch 97] Batch 921, Loss 0.1607145369052887\n","[Training Epoch 97] Batch 922, Loss 0.19951269030570984\n","[Training Epoch 97] Batch 923, Loss 0.14892476797103882\n","[Training Epoch 97] Batch 924, Loss 0.1756964921951294\n","[Training Epoch 97] Batch 925, Loss 0.17356811463832855\n","[Training Epoch 97] Batch 926, Loss 0.15019658207893372\n","[Training Epoch 97] Batch 927, Loss 0.17000582814216614\n","[Training Epoch 97] Batch 928, Loss 0.17594924569129944\n","[Training Epoch 97] Batch 929, Loss 0.18235473334789276\n","[Training Epoch 97] Batch 930, Loss 0.13468706607818604\n","[Training Epoch 97] Batch 931, Loss 0.16322176158428192\n","[Training Epoch 97] Batch 932, Loss 0.1903245598077774\n","[Training Epoch 97] Batch 933, Loss 0.17348447442054749\n","[Training Epoch 97] Batch 934, Loss 0.1877025067806244\n","[Training Epoch 97] Batch 935, Loss 0.17632991075515747\n","[Training Epoch 97] Batch 936, Loss 0.16181600093841553\n","[Training Epoch 97] Batch 937, Loss 0.16215062141418457\n","[Training Epoch 97] Batch 938, Loss 0.1660362333059311\n","[Training Epoch 97] Batch 939, Loss 0.14456933736801147\n","[Training Epoch 97] Batch 940, Loss 0.15922124683856964\n","[Training Epoch 97] Batch 941, Loss 0.18799346685409546\n","[Training Epoch 97] Batch 942, Loss 0.17110276222229004\n","[Training Epoch 97] Batch 943, Loss 0.17445413768291473\n","[Training Epoch 97] Batch 944, Loss 0.16265973448753357\n","[Training Epoch 97] Batch 945, Loss 0.16629210114479065\n","[Training Epoch 97] Batch 946, Loss 0.16513165831565857\n","[Training Epoch 97] Batch 947, Loss 0.18074339628219604\n","[Training Epoch 97] Batch 948, Loss 0.14405114948749542\n","[Training Epoch 97] Batch 949, Loss 0.17799249291419983\n","[Training Epoch 97] Batch 950, Loss 0.18823131918907166\n","[Training Epoch 97] Batch 951, Loss 0.1693495512008667\n","[Training Epoch 97] Batch 952, Loss 0.15639829635620117\n","[Training Epoch 97] Batch 953, Loss 0.15897412598133087\n","[Training Epoch 97] Batch 954, Loss 0.17103733122348785\n","[Training Epoch 97] Batch 955, Loss 0.17310908436775208\n","[Training Epoch 97] Batch 956, Loss 0.17734970152378082\n","[Training Epoch 97] Batch 957, Loss 0.18787497282028198\n","[Training Epoch 97] Batch 958, Loss 0.18016810715198517\n","[Training Epoch 97] Batch 959, Loss 0.17715241014957428\n","[Training Epoch 97] Batch 960, Loss 0.1795913130044937\n","[Training Epoch 97] Batch 961, Loss 0.178909033536911\n","[Training Epoch 97] Batch 962, Loss 0.17915599048137665\n","[Training Epoch 97] Batch 963, Loss 0.16352571547031403\n","[Training Epoch 97] Batch 964, Loss 0.1755438894033432\n","[Training Epoch 97] Batch 965, Loss 0.1536453664302826\n","[Training Epoch 97] Batch 966, Loss 0.15678037703037262\n","[Training Epoch 97] Batch 967, Loss 0.16713254153728485\n","[Training Epoch 97] Batch 968, Loss 0.17046211659908295\n","[Training Epoch 97] Batch 969, Loss 0.15604932606220245\n","[Training Epoch 97] Batch 970, Loss 0.17291224002838135\n","[Training Epoch 97] Batch 971, Loss 0.18126431107521057\n","[Training Epoch 97] Batch 972, Loss 0.15353062748908997\n","[Training Epoch 97] Batch 973, Loss 0.16884565353393555\n","[Training Epoch 97] Batch 974, Loss 0.167252317070961\n","[Training Epoch 97] Batch 975, Loss 0.1560555100440979\n","[Training Epoch 97] Batch 976, Loss 0.16408497095108032\n","[Training Epoch 97] Batch 977, Loss 0.1625276803970337\n","[Training Epoch 97] Batch 978, Loss 0.14401602745056152\n","[Training Epoch 97] Batch 979, Loss 0.1633421629667282\n","[Training Epoch 97] Batch 980, Loss 0.1690737009048462\n","[Training Epoch 97] Batch 981, Loss 0.17444884777069092\n","[Training Epoch 97] Batch 982, Loss 0.15516747534275055\n","[Training Epoch 97] Batch 983, Loss 0.161551833152771\n","[Training Epoch 97] Batch 984, Loss 0.17104177176952362\n","[Training Epoch 97] Batch 985, Loss 0.16514338552951813\n","[Training Epoch 97] Batch 986, Loss 0.17088118195533752\n","[Training Epoch 97] Batch 987, Loss 0.19301192462444305\n","[Training Epoch 97] Batch 988, Loss 0.16962958872318268\n","[Training Epoch 97] Batch 989, Loss 0.17677466571331024\n","[Training Epoch 97] Batch 990, Loss 0.17370575666427612\n","[Training Epoch 97] Batch 991, Loss 0.14617443084716797\n","[Training Epoch 97] Batch 992, Loss 0.18010814487934113\n","[Training Epoch 97] Batch 993, Loss 0.16125960648059845\n","[Training Epoch 97] Batch 994, Loss 0.18596263229846954\n","[Training Epoch 97] Batch 995, Loss 0.18045419454574585\n","[Training Epoch 97] Batch 996, Loss 0.1652131825685501\n","[Training Epoch 97] Batch 997, Loss 0.15507052838802338\n","[Training Epoch 97] Batch 998, Loss 0.1609361618757248\n","[Training Epoch 97] Batch 999, Loss 0.1884767711162567\n","[Training Epoch 97] Batch 1000, Loss 0.1657155454158783\n","[Training Epoch 97] Batch 1001, Loss 0.14875143766403198\n","[Training Epoch 97] Batch 1002, Loss 0.16338691115379333\n","[Training Epoch 97] Batch 1003, Loss 0.17967942357063293\n","[Training Epoch 97] Batch 1004, Loss 0.1581060141324997\n","[Training Epoch 97] Batch 1005, Loss 0.16912953555583954\n","[Training Epoch 97] Batch 1006, Loss 0.16428527235984802\n","[Training Epoch 97] Batch 1007, Loss 0.17354154586791992\n","[Training Epoch 97] Batch 1008, Loss 0.15157291293144226\n","[Training Epoch 97] Batch 1009, Loss 0.16918808221817017\n","[Training Epoch 97] Batch 1010, Loss 0.1698407679796219\n","[Training Epoch 97] Batch 1011, Loss 0.15682578086853027\n","[Training Epoch 97] Batch 1012, Loss 0.1936771422624588\n","[Training Epoch 97] Batch 1013, Loss 0.17060250043869019\n","[Training Epoch 97] Batch 1014, Loss 0.17514079809188843\n","[Training Epoch 97] Batch 1015, Loss 0.16563579440116882\n","[Training Epoch 97] Batch 1016, Loss 0.16336172819137573\n","[Training Epoch 97] Batch 1017, Loss 0.1527707427740097\n","[Training Epoch 97] Batch 1018, Loss 0.1685415655374527\n","[Training Epoch 97] Batch 1019, Loss 0.18034037947654724\n","[Training Epoch 97] Batch 1020, Loss 0.16100499033927917\n","[Training Epoch 97] Batch 1021, Loss 0.1698826551437378\n","[Training Epoch 97] Batch 1022, Loss 0.15255728363990784\n","[Training Epoch 97] Batch 1023, Loss 0.17442363500595093\n","[Training Epoch 97] Batch 1024, Loss 0.1675901710987091\n","[Training Epoch 97] Batch 1025, Loss 0.15461760759353638\n","[Training Epoch 97] Batch 1026, Loss 0.15891537070274353\n","[Training Epoch 97] Batch 1027, Loss 0.1538410484790802\n","[Training Epoch 97] Batch 1028, Loss 0.1937616914510727\n","[Training Epoch 97] Batch 1029, Loss 0.18407750129699707\n","[Training Epoch 97] Batch 1030, Loss 0.17800471186637878\n","[Training Epoch 97] Batch 1031, Loss 0.1881292164325714\n","[Training Epoch 97] Batch 1032, Loss 0.16142801940441132\n","[Training Epoch 97] Batch 1033, Loss 0.18103629350662231\n","[Training Epoch 97] Batch 1034, Loss 0.15616929531097412\n","[Training Epoch 97] Batch 1035, Loss 0.14894013106822968\n","[Training Epoch 97] Batch 1036, Loss 0.18596869707107544\n","[Training Epoch 97] Batch 1037, Loss 0.15421418845653534\n","[Training Epoch 97] Batch 1038, Loss 0.13957944512367249\n","[Training Epoch 97] Batch 1039, Loss 0.16207869350910187\n","[Training Epoch 97] Batch 1040, Loss 0.16579873859882355\n","[Training Epoch 97] Batch 1041, Loss 0.1805466115474701\n","[Training Epoch 97] Batch 1042, Loss 0.1592821627855301\n","[Training Epoch 97] Batch 1043, Loss 0.15158912539482117\n","[Training Epoch 97] Batch 1044, Loss 0.18051297962665558\n","[Training Epoch 97] Batch 1045, Loss 0.16383731365203857\n","[Training Epoch 97] Batch 1046, Loss 0.17917269468307495\n","[Training Epoch 97] Batch 1047, Loss 0.18337038159370422\n","[Training Epoch 97] Batch 1048, Loss 0.1608121544122696\n","[Training Epoch 97] Batch 1049, Loss 0.1781274676322937\n","[Training Epoch 97] Batch 1050, Loss 0.17991897463798523\n","[Training Epoch 97] Batch 1051, Loss 0.15952643752098083\n","[Training Epoch 97] Batch 1052, Loss 0.16663262248039246\n","[Training Epoch 97] Batch 1053, Loss 0.16941124200820923\n","[Training Epoch 97] Batch 1054, Loss 0.1686355173587799\n","[Training Epoch 97] Batch 1055, Loss 0.17143456637859344\n","[Training Epoch 97] Batch 1056, Loss 0.16031740605831146\n","[Training Epoch 97] Batch 1057, Loss 0.16324125230312347\n","[Training Epoch 97] Batch 1058, Loss 0.15924179553985596\n","[Training Epoch 97] Batch 1059, Loss 0.15928509831428528\n","[Training Epoch 97] Batch 1060, Loss 0.16876846551895142\n","[Training Epoch 97] Batch 1061, Loss 0.17460781335830688\n","[Training Epoch 97] Batch 1062, Loss 0.16110405325889587\n","[Training Epoch 97] Batch 1063, Loss 0.1835513710975647\n","[Training Epoch 97] Batch 1064, Loss 0.15933462977409363\n","[Training Epoch 97] Batch 1065, Loss 0.16746479272842407\n","[Training Epoch 97] Batch 1066, Loss 0.1399916112422943\n","[Training Epoch 97] Batch 1067, Loss 0.16680733859539032\n","[Training Epoch 97] Batch 1068, Loss 0.16543132066726685\n","[Training Epoch 97] Batch 1069, Loss 0.16578343510627747\n","[Training Epoch 97] Batch 1070, Loss 0.16143375635147095\n","[Training Epoch 97] Batch 1071, Loss 0.18119600415229797\n","[Training Epoch 97] Batch 1072, Loss 0.15081048011779785\n","[Training Epoch 97] Batch 1073, Loss 0.1646972894668579\n","[Training Epoch 97] Batch 1074, Loss 0.15417030453681946\n","[Training Epoch 97] Batch 1075, Loss 0.15958932042121887\n","[Training Epoch 97] Batch 1076, Loss 0.17096321284770966\n","[Training Epoch 97] Batch 1077, Loss 0.18113195896148682\n","[Training Epoch 97] Batch 1078, Loss 0.16991640627384186\n","[Training Epoch 97] Batch 1079, Loss 0.17954690754413605\n","[Training Epoch 97] Batch 1080, Loss 0.19213782250881195\n","[Training Epoch 97] Batch 1081, Loss 0.1911429464817047\n","[Training Epoch 97] Batch 1082, Loss 0.14231137931346893\n","[Training Epoch 97] Batch 1083, Loss 0.16306072473526\n","[Training Epoch 97] Batch 1084, Loss 0.15795603394508362\n","[Training Epoch 97] Batch 1085, Loss 0.17400886118412018\n","[Training Epoch 97] Batch 1086, Loss 0.16230763494968414\n","[Training Epoch 97] Batch 1087, Loss 0.1577208936214447\n","[Training Epoch 97] Batch 1088, Loss 0.18009474873542786\n","[Training Epoch 97] Batch 1089, Loss 0.1507708728313446\n","[Training Epoch 97] Batch 1090, Loss 0.15766997635364532\n","[Training Epoch 97] Batch 1091, Loss 0.15959830582141876\n","[Training Epoch 97] Batch 1092, Loss 0.16535532474517822\n","[Training Epoch 97] Batch 1093, Loss 0.18688631057739258\n","[Training Epoch 97] Batch 1094, Loss 0.15487802028656006\n","[Training Epoch 97] Batch 1095, Loss 0.18224263191223145\n","[Training Epoch 97] Batch 1096, Loss 0.14691193401813507\n","[Training Epoch 97] Batch 1097, Loss 0.1766451895236969\n","[Training Epoch 97] Batch 1098, Loss 0.18555638194084167\n","[Training Epoch 97] Batch 1099, Loss 0.19497042894363403\n","[Training Epoch 97] Batch 1100, Loss 0.1864582747220993\n","[Training Epoch 97] Batch 1101, Loss 0.15066668391227722\n","[Training Epoch 97] Batch 1102, Loss 0.17270313203334808\n","[Training Epoch 97] Batch 1103, Loss 0.17566794157028198\n","[Training Epoch 97] Batch 1104, Loss 0.17822301387786865\n","[Training Epoch 97] Batch 1105, Loss 0.15660029649734497\n","[Training Epoch 97] Batch 1106, Loss 0.18664096295833588\n","[Training Epoch 97] Batch 1107, Loss 0.1583639681339264\n","[Training Epoch 97] Batch 1108, Loss 0.16894491016864777\n","[Training Epoch 97] Batch 1109, Loss 0.1759532392024994\n","[Training Epoch 97] Batch 1110, Loss 0.1696811020374298\n","[Training Epoch 97] Batch 1111, Loss 0.17056143283843994\n","[Training Epoch 97] Batch 1112, Loss 0.17588438093662262\n","[Training Epoch 97] Batch 1113, Loss 0.16036579012870789\n","[Training Epoch 97] Batch 1114, Loss 0.1932123750448227\n","[Training Epoch 97] Batch 1115, Loss 0.19192707538604736\n","[Training Epoch 97] Batch 1116, Loss 0.15210746228694916\n","[Training Epoch 97] Batch 1117, Loss 0.18274806439876556\n","[Training Epoch 97] Batch 1118, Loss 0.15819332003593445\n","[Training Epoch 97] Batch 1119, Loss 0.16922196745872498\n","[Training Epoch 97] Batch 1120, Loss 0.1699504405260086\n","[Training Epoch 97] Batch 1121, Loss 0.1646912693977356\n","[Training Epoch 97] Batch 1122, Loss 0.15980318188667297\n","[Training Epoch 97] Batch 1123, Loss 0.15414509177207947\n","[Training Epoch 97] Batch 1124, Loss 0.16825619339942932\n","[Training Epoch 97] Batch 1125, Loss 0.1867346167564392\n","[Training Epoch 97] Batch 1126, Loss 0.15302366018295288\n","[Training Epoch 97] Batch 1127, Loss 0.17116570472717285\n","[Training Epoch 97] Batch 1128, Loss 0.19233456254005432\n","[Training Epoch 97] Batch 1129, Loss 0.1517542451620102\n","[Training Epoch 97] Batch 1130, Loss 0.17462995648384094\n","[Training Epoch 97] Batch 1131, Loss 0.1700812131166458\n","[Training Epoch 97] Batch 1132, Loss 0.18827605247497559\n","[Training Epoch 97] Batch 1133, Loss 0.13720956444740295\n","[Training Epoch 97] Batch 1134, Loss 0.1788366734981537\n","[Training Epoch 97] Batch 1135, Loss 0.17286303639411926\n","[Training Epoch 97] Batch 1136, Loss 0.13250991702079773\n","[Training Epoch 97] Batch 1137, Loss 0.1720668524503708\n","[Training Epoch 97] Batch 1138, Loss 0.18449942767620087\n","[Training Epoch 97] Batch 1139, Loss 0.14757667481899261\n","[Training Epoch 97] Batch 1140, Loss 0.1562758833169937\n","[Training Epoch 97] Batch 1141, Loss 0.19007104635238647\n","[Training Epoch 97] Batch 1142, Loss 0.1510181725025177\n","[Training Epoch 97] Batch 1143, Loss 0.17681416869163513\n","[Training Epoch 97] Batch 1144, Loss 0.15775248408317566\n","[Training Epoch 97] Batch 1145, Loss 0.16083839535713196\n","[Training Epoch 97] Batch 1146, Loss 0.15754744410514832\n","[Training Epoch 97] Batch 1147, Loss 0.1596326380968094\n","[Training Epoch 97] Batch 1148, Loss 0.16237416863441467\n","[Training Epoch 97] Batch 1149, Loss 0.189569354057312\n","[Training Epoch 97] Batch 1150, Loss 0.1971529722213745\n","[Training Epoch 97] Batch 1151, Loss 0.1526603102684021\n","[Training Epoch 97] Batch 1152, Loss 0.18563377857208252\n","[Training Epoch 97] Batch 1153, Loss 0.1690501868724823\n","[Training Epoch 97] Batch 1154, Loss 0.17363016307353973\n","[Training Epoch 97] Batch 1155, Loss 0.18418893218040466\n","[Training Epoch 97] Batch 1156, Loss 0.17350801825523376\n","[Training Epoch 97] Batch 1157, Loss 0.1763988733291626\n","[Training Epoch 97] Batch 1158, Loss 0.16247352957725525\n","[Training Epoch 97] Batch 1159, Loss 0.16176503896713257\n","[Training Epoch 97] Batch 1160, Loss 0.18586978316307068\n","/content/drive/MyDrive/Neural-CF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 97] HR = 0.7148, NDCG = 0.3519\n","Epoch 98 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 98] Batch 0, Loss 0.15800005197525024\n","[Training Epoch 98] Batch 1, Loss 0.1667591631412506\n","[Training Epoch 98] Batch 2, Loss 0.17483077943325043\n","[Training Epoch 98] Batch 3, Loss 0.16978752613067627\n","[Training Epoch 98] Batch 4, Loss 0.149916410446167\n","[Training Epoch 98] Batch 5, Loss 0.1907360702753067\n","[Training Epoch 98] Batch 6, Loss 0.15010415017604828\n","[Training Epoch 98] Batch 7, Loss 0.1647387146949768\n","[Training Epoch 98] Batch 8, Loss 0.17886489629745483\n","[Training Epoch 98] Batch 9, Loss 0.15710872411727905\n","[Training Epoch 98] Batch 10, Loss 0.15738874673843384\n","[Training Epoch 98] Batch 11, Loss 0.15319719910621643\n","[Training Epoch 98] Batch 12, Loss 0.17631025612354279\n","[Training Epoch 98] Batch 13, Loss 0.1706300526857376\n","[Training Epoch 98] Batch 14, Loss 0.1502615213394165\n","[Training Epoch 98] Batch 15, Loss 0.16155676543712616\n","[Training Epoch 98] Batch 16, Loss 0.14898106455802917\n","[Training Epoch 98] Batch 17, Loss 0.1464734673500061\n","[Training Epoch 98] Batch 18, Loss 0.15569685399532318\n","[Training Epoch 98] Batch 19, Loss 0.17810802161693573\n","[Training Epoch 98] Batch 20, Loss 0.15716277062892914\n","[Training Epoch 98] Batch 21, Loss 0.17866842448711395\n","[Training Epoch 98] Batch 22, Loss 0.16836443543434143\n","[Training Epoch 98] Batch 23, Loss 0.14642482995986938\n","[Training Epoch 98] Batch 24, Loss 0.16315771639347076\n","[Training Epoch 98] Batch 25, Loss 0.15892350673675537\n","[Training Epoch 98] Batch 26, Loss 0.14942941069602966\n","[Training Epoch 98] Batch 27, Loss 0.17095065116882324\n","[Training Epoch 98] Batch 28, Loss 0.17247316241264343\n","[Training Epoch 98] Batch 29, Loss 0.15394294261932373\n","[Training Epoch 98] Batch 30, Loss 0.17980584502220154\n","[Training Epoch 98] Batch 31, Loss 0.17029426991939545\n","[Training Epoch 98] Batch 32, Loss 0.1638863980770111\n","[Training Epoch 98] Batch 33, Loss 0.1825307011604309\n","[Training Epoch 98] Batch 34, Loss 0.16940554976463318\n","[Training Epoch 98] Batch 35, Loss 0.16686564683914185\n","[Training Epoch 98] Batch 36, Loss 0.17148062586784363\n","[Training Epoch 98] Batch 37, Loss 0.1441698968410492\n","[Training Epoch 98] Batch 38, Loss 0.16588979959487915\n","[Training Epoch 98] Batch 39, Loss 0.13667158782482147\n","[Training Epoch 98] Batch 40, Loss 0.15318474173545837\n","[Training Epoch 98] Batch 41, Loss 0.18609094619750977\n","[Training Epoch 98] Batch 42, Loss 0.18365484476089478\n","[Training Epoch 98] Batch 43, Loss 0.14128080010414124\n","[Training Epoch 98] Batch 44, Loss 0.1729775369167328\n","[Training Epoch 98] Batch 45, Loss 0.15075960755348206\n","[Training Epoch 98] Batch 46, Loss 0.17425313591957092\n","[Training Epoch 98] Batch 47, Loss 0.15874189138412476\n","[Training Epoch 98] Batch 48, Loss 0.14898942410945892\n","[Training Epoch 98] Batch 49, Loss 0.1789626032114029\n","[Training Epoch 98] Batch 50, Loss 0.18301190435886383\n","[Training Epoch 98] Batch 51, Loss 0.16362188756465912\n","[Training Epoch 98] Batch 52, Loss 0.1370924711227417\n","[Training Epoch 98] Batch 53, Loss 0.17150342464447021\n","[Training Epoch 98] Batch 54, Loss 0.1631617248058319\n","[Training Epoch 98] Batch 55, Loss 0.17297030985355377\n","[Training Epoch 98] Batch 56, Loss 0.16019290685653687\n","[Training Epoch 98] Batch 57, Loss 0.16222526133060455\n","[Training Epoch 98] Batch 58, Loss 0.16897645592689514\n","[Training Epoch 98] Batch 59, Loss 0.16553375124931335\n","[Training Epoch 98] Batch 60, Loss 0.17496570944786072\n","[Training Epoch 98] Batch 61, Loss 0.17939195036888123\n","[Training Epoch 98] Batch 62, Loss 0.15362024307250977\n","[Training Epoch 98] Batch 63, Loss 0.14460523426532745\n","[Training Epoch 98] Batch 64, Loss 0.17020104825496674\n","[Training Epoch 98] Batch 65, Loss 0.1327919214963913\n","[Training Epoch 98] Batch 66, Loss 0.2000296413898468\n","[Training Epoch 98] Batch 67, Loss 0.1538599282503128\n","[Training Epoch 98] Batch 68, Loss 0.1930733174085617\n","[Training Epoch 98] Batch 69, Loss 0.16082127392292023\n","[Training Epoch 98] Batch 70, Loss 0.15363115072250366\n","[Training Epoch 98] Batch 71, Loss 0.15437164902687073\n","[Training Epoch 98] Batch 72, Loss 0.15881821513175964\n","[Training Epoch 98] Batch 73, Loss 0.1682034134864807\n","[Training Epoch 98] Batch 74, Loss 0.1620829999446869\n","[Training Epoch 98] Batch 75, Loss 0.1526089608669281\n","[Training Epoch 98] Batch 76, Loss 0.15123121440410614\n","[Training Epoch 98] Batch 77, Loss 0.18188819289207458\n","[Training Epoch 98] Batch 78, Loss 0.18178509175777435\n","[Training Epoch 98] Batch 79, Loss 0.1524476408958435\n","[Training Epoch 98] Batch 80, Loss 0.1764765977859497\n","[Training Epoch 98] Batch 81, Loss 0.1543579399585724\n","[Training Epoch 98] Batch 82, Loss 0.16585037112236023\n","[Training Epoch 98] Batch 83, Loss 0.16647784411907196\n","[Training Epoch 98] Batch 84, Loss 0.17552563548088074\n","[Training Epoch 98] Batch 85, Loss 0.1547674685716629\n","[Training Epoch 98] Batch 86, Loss 0.166437029838562\n","[Training Epoch 98] Batch 87, Loss 0.16480159759521484\n","[Training Epoch 98] Batch 88, Loss 0.16079455614089966\n","[Training Epoch 98] Batch 89, Loss 0.15056070685386658\n","[Training Epoch 98] Batch 90, Loss 0.1755981594324112\n","[Training Epoch 98] Batch 91, Loss 0.14952413737773895\n","[Training Epoch 98] Batch 92, Loss 0.1606837958097458\n","[Training Epoch 98] Batch 93, Loss 0.16956740617752075\n","[Training Epoch 98] Batch 94, Loss 0.16257598996162415\n","[Training Epoch 98] Batch 95, Loss 0.1745918095111847\n","[Training Epoch 98] Batch 96, Loss 0.17998670041561127\n","[Training Epoch 98] Batch 97, Loss 0.16670246422290802\n","[Training Epoch 98] Batch 98, Loss 0.14163467288017273\n","[Training Epoch 98] Batch 99, Loss 0.1718212068080902\n","[Training Epoch 98] Batch 100, Loss 0.15406987071037292\n","[Training Epoch 98] Batch 101, Loss 0.14564485847949982\n","[Training Epoch 98] Batch 102, Loss 0.1658037006855011\n","[Training Epoch 98] Batch 103, Loss 0.17269811034202576\n","[Training Epoch 98] Batch 104, Loss 0.13915634155273438\n","[Training Epoch 98] Batch 105, Loss 0.16085413098335266\n","[Training Epoch 98] Batch 106, Loss 0.1446247100830078\n","[Training Epoch 98] Batch 107, Loss 0.17597942054271698\n","[Training Epoch 98] Batch 108, Loss 0.16524100303649902\n","[Training Epoch 98] Batch 109, Loss 0.16363555192947388\n","[Training Epoch 98] Batch 110, Loss 0.15335296094417572\n","[Training Epoch 98] Batch 111, Loss 0.18204686045646667\n","[Training Epoch 98] Batch 112, Loss 0.1575021892786026\n","[Training Epoch 98] Batch 113, Loss 0.18497833609580994\n","[Training Epoch 98] Batch 114, Loss 0.16686281561851501\n","[Training Epoch 98] Batch 115, Loss 0.15954840183258057\n","[Training Epoch 98] Batch 116, Loss 0.16372402012348175\n","[Training Epoch 98] Batch 117, Loss 0.15022233128547668\n","[Training Epoch 98] Batch 118, Loss 0.1560453474521637\n","[Training Epoch 98] Batch 119, Loss 0.18669894337654114\n","[Training Epoch 98] Batch 120, Loss 0.15470853447914124\n","[Training Epoch 98] Batch 121, Loss 0.173972487449646\n","[Training Epoch 98] Batch 122, Loss 0.16772782802581787\n","[Training Epoch 98] Batch 123, Loss 0.17332451045513153\n","[Training Epoch 98] Batch 124, Loss 0.14875197410583496\n","[Training Epoch 98] Batch 125, Loss 0.16068947315216064\n","[Training Epoch 98] Batch 126, Loss 0.1724686175584793\n","[Training Epoch 98] Batch 127, Loss 0.17451874911785126\n","[Training Epoch 98] Batch 128, Loss 0.15762251615524292\n","[Training Epoch 98] Batch 129, Loss 0.17484024167060852\n","[Training Epoch 98] Batch 130, Loss 0.14962977170944214\n","[Training Epoch 98] Batch 131, Loss 0.14731717109680176\n","[Training Epoch 98] Batch 132, Loss 0.1622449904680252\n","[Training Epoch 98] Batch 133, Loss 0.16862037777900696\n","[Training Epoch 98] Batch 134, Loss 0.17344926297664642\n","[Training Epoch 98] Batch 135, Loss 0.19036538898944855\n","[Training Epoch 98] Batch 136, Loss 0.17613816261291504\n","[Training Epoch 98] Batch 137, Loss 0.1724768579006195\n","[Training Epoch 98] Batch 138, Loss 0.16569626331329346\n","[Training Epoch 98] Batch 139, Loss 0.15170995891094208\n","[Training Epoch 98] Batch 140, Loss 0.18316668272018433\n","[Training Epoch 98] Batch 141, Loss 0.166519433259964\n","[Training Epoch 98] Batch 142, Loss 0.1498185098171234\n","[Training Epoch 98] Batch 143, Loss 0.1782035529613495\n","[Training Epoch 98] Batch 144, Loss 0.1799638271331787\n","[Training Epoch 98] Batch 145, Loss 0.16645050048828125\n","[Training Epoch 98] Batch 146, Loss 0.16611653566360474\n","[Training Epoch 98] Batch 147, Loss 0.16043156385421753\n","[Training Epoch 98] Batch 148, Loss 0.16481725871562958\n","[Training Epoch 98] Batch 149, Loss 0.15776857733726501\n","[Training Epoch 98] Batch 150, Loss 0.18303415179252625\n","[Training Epoch 98] Batch 151, Loss 0.17690667510032654\n","[Training Epoch 98] Batch 152, Loss 0.16939470171928406\n","[Training Epoch 98] Batch 153, Loss 0.1641775667667389\n","[Training Epoch 98] Batch 154, Loss 0.16617637872695923\n","[Training Epoch 98] Batch 155, Loss 0.16982927918434143\n","[Training Epoch 98] Batch 156, Loss 0.1659960150718689\n","[Training Epoch 98] Batch 157, Loss 0.15980730950832367\n","[Training Epoch 98] Batch 158, Loss 0.16367657482624054\n","[Training Epoch 98] Batch 159, Loss 0.1876894235610962\n","[Training Epoch 98] Batch 160, Loss 0.1784605085849762\n","[Training Epoch 98] Batch 161, Loss 0.16169072687625885\n","[Training Epoch 98] Batch 162, Loss 0.15845605731010437\n","[Training Epoch 98] Batch 163, Loss 0.1568368375301361\n","[Training Epoch 98] Batch 164, Loss 0.16990172863006592\n","[Training Epoch 98] Batch 165, Loss 0.14157432317733765\n","[Training Epoch 98] Batch 166, Loss 0.14843767881393433\n","[Training Epoch 98] Batch 167, Loss 0.20092007517814636\n","[Training Epoch 98] Batch 168, Loss 0.1344877928495407\n","[Training Epoch 98] Batch 169, Loss 0.16616329550743103\n","[Training Epoch 98] Batch 170, Loss 0.1591619849205017\n","[Training Epoch 98] Batch 171, Loss 0.16997474431991577\n","[Training Epoch 98] Batch 172, Loss 0.17278315126895905\n","[Training Epoch 98] Batch 173, Loss 0.15267185866832733\n","[Training Epoch 98] Batch 174, Loss 0.17450085282325745\n","[Training Epoch 98] Batch 175, Loss 0.15439537167549133\n","[Training Epoch 98] Batch 176, Loss 0.14967980980873108\n","[Training Epoch 98] Batch 177, Loss 0.1741398423910141\n","[Training Epoch 98] Batch 178, Loss 0.17205627262592316\n","[Training Epoch 98] Batch 179, Loss 0.16767996549606323\n","[Training Epoch 98] Batch 180, Loss 0.15483787655830383\n","[Training Epoch 98] Batch 181, Loss 0.16806519031524658\n","[Training Epoch 98] Batch 182, Loss 0.15448281168937683\n","[Training Epoch 98] Batch 183, Loss 0.1743158996105194\n","[Training Epoch 98] Batch 184, Loss 0.16609326004981995\n","[Training Epoch 98] Batch 185, Loss 0.1694885939359665\n","[Training Epoch 98] Batch 186, Loss 0.16034090518951416\n","[Training Epoch 98] Batch 187, Loss 0.16214147210121155\n","[Training Epoch 98] Batch 188, Loss 0.16935542225837708\n","[Training Epoch 98] Batch 189, Loss 0.1620640605688095\n","[Training Epoch 98] Batch 190, Loss 0.15282224118709564\n","[Training Epoch 98] Batch 191, Loss 0.17105180025100708\n","[Training Epoch 98] Batch 192, Loss 0.1670108139514923\n","[Training Epoch 98] Batch 193, Loss 0.16270318627357483\n","[Training Epoch 98] Batch 194, Loss 0.16504034399986267\n","[Training Epoch 98] Batch 195, Loss 0.16787824034690857\n","[Training Epoch 98] Batch 196, Loss 0.15635308623313904\n","[Training Epoch 98] Batch 197, Loss 0.16701024770736694\n","[Training Epoch 98] Batch 198, Loss 0.15481093525886536\n","[Training Epoch 98] Batch 199, Loss 0.18639105558395386\n","[Training Epoch 98] Batch 200, Loss 0.16547733545303345\n","[Training Epoch 98] Batch 201, Loss 0.16785071790218353\n","[Training Epoch 98] Batch 202, Loss 0.1490612030029297\n","[Training Epoch 98] Batch 203, Loss 0.16414278745651245\n","[Training Epoch 98] Batch 204, Loss 0.1781594455242157\n","[Training Epoch 98] Batch 205, Loss 0.15193122625350952\n","[Training Epoch 98] Batch 206, Loss 0.14884087443351746\n","[Training Epoch 98] Batch 207, Loss 0.17799058556556702\n","[Training Epoch 98] Batch 208, Loss 0.16643178462982178\n","[Training Epoch 98] Batch 209, Loss 0.1307723969221115\n","[Training Epoch 98] Batch 210, Loss 0.1722167730331421\n","[Training Epoch 98] Batch 211, Loss 0.15985427796840668\n","[Training Epoch 98] Batch 212, Loss 0.1638726145029068\n","[Training Epoch 98] Batch 213, Loss 0.16597838699817657\n","[Training Epoch 98] Batch 214, Loss 0.17935407161712646\n","[Training Epoch 98] Batch 215, Loss 0.18274827301502228\n","[Training Epoch 98] Batch 216, Loss 0.16091465950012207\n","[Training Epoch 98] Batch 217, Loss 0.1702631413936615\n","[Training Epoch 98] Batch 218, Loss 0.15804174542427063\n","[Training Epoch 98] Batch 219, Loss 0.1419394612312317\n","[Training Epoch 98] Batch 220, Loss 0.17102967202663422\n","[Training Epoch 98] Batch 221, Loss 0.16750001907348633\n","[Training Epoch 98] Batch 222, Loss 0.18294061720371246\n","[Training Epoch 98] Batch 223, Loss 0.18276160955429077\n","[Training Epoch 98] Batch 224, Loss 0.15214857459068298\n","[Training Epoch 98] Batch 225, Loss 0.15667854249477386\n","[Training Epoch 98] Batch 226, Loss 0.17306038737297058\n","[Training Epoch 98] Batch 227, Loss 0.16786564886569977\n","[Training Epoch 98] Batch 228, Loss 0.15363353490829468\n","[Training Epoch 98] Batch 229, Loss 0.15732787549495697\n","[Training Epoch 98] Batch 230, Loss 0.1654847264289856\n","[Training Epoch 98] Batch 231, Loss 0.1743352711200714\n","[Training Epoch 98] Batch 232, Loss 0.16177193820476532\n","[Training Epoch 98] Batch 233, Loss 0.16973349452018738\n","[Training Epoch 98] Batch 234, Loss 0.17149882018566132\n","[Training Epoch 98] Batch 235, Loss 0.17163881659507751\n","[Training Epoch 98] Batch 236, Loss 0.17792098224163055\n","[Training Epoch 98] Batch 237, Loss 0.17078569531440735\n","[Training Epoch 98] Batch 238, Loss 0.15847966074943542\n","[Training Epoch 98] Batch 239, Loss 0.15604373812675476\n","[Training Epoch 98] Batch 240, Loss 0.1682949662208557\n","[Training Epoch 98] Batch 241, Loss 0.16467474400997162\n","[Training Epoch 98] Batch 242, Loss 0.13911862671375275\n","[Training Epoch 98] Batch 243, Loss 0.1791595220565796\n","[Training Epoch 98] Batch 244, Loss 0.17088237404823303\n","[Training Epoch 98] Batch 245, Loss 0.17098137736320496\n","[Training Epoch 98] Batch 246, Loss 0.1546485722064972\n","[Training Epoch 98] Batch 247, Loss 0.19949296116828918\n","[Training Epoch 98] Batch 248, Loss 0.18389242887496948\n","[Training Epoch 98] Batch 249, Loss 0.15686431527137756\n","[Training Epoch 98] Batch 250, Loss 0.1687418520450592\n","[Training Epoch 98] Batch 251, Loss 0.14787918329238892\n","[Training Epoch 98] Batch 252, Loss 0.16955937445163727\n","[Training Epoch 98] Batch 253, Loss 0.17764687538146973\n","[Training Epoch 98] Batch 254, Loss 0.16529586911201477\n","[Training Epoch 98] Batch 255, Loss 0.17299535870552063\n","[Training Epoch 98] Batch 256, Loss 0.15194787085056305\n","[Training Epoch 98] Batch 257, Loss 0.17172709107398987\n","[Training Epoch 98] Batch 258, Loss 0.15206003189086914\n","[Training Epoch 98] Batch 259, Loss 0.1820317506790161\n","[Training Epoch 98] Batch 260, Loss 0.15020714700222015\n","[Training Epoch 98] Batch 261, Loss 0.15950608253479004\n","[Training Epoch 98] Batch 262, Loss 0.15579840540885925\n","[Training Epoch 98] Batch 263, Loss 0.16015933454036713\n","[Training Epoch 98] Batch 264, Loss 0.15667420625686646\n","[Training Epoch 98] Batch 265, Loss 0.19145995378494263\n","[Training Epoch 98] Batch 266, Loss 0.17611071467399597\n","[Training Epoch 98] Batch 267, Loss 0.19417522847652435\n","[Training Epoch 98] Batch 268, Loss 0.15691787004470825\n","[Training Epoch 98] Batch 269, Loss 0.18026202917099\n","[Training Epoch 98] Batch 270, Loss 0.17217504978179932\n","[Training Epoch 98] Batch 271, Loss 0.1444656401872635\n","[Training Epoch 98] Batch 272, Loss 0.15446670353412628\n","[Training Epoch 98] Batch 273, Loss 0.18175959587097168\n","[Training Epoch 98] Batch 274, Loss 0.15666024386882782\n","[Training Epoch 98] Batch 275, Loss 0.15468385815620422\n","[Training Epoch 98] Batch 276, Loss 0.13503216207027435\n","[Training Epoch 98] Batch 277, Loss 0.16464395821094513\n","[Training Epoch 98] Batch 278, Loss 0.16934999823570251\n","[Training Epoch 98] Batch 279, Loss 0.17147976160049438\n","[Training Epoch 98] Batch 280, Loss 0.1680787205696106\n","[Training Epoch 98] Batch 281, Loss 0.18175888061523438\n","[Training Epoch 98] Batch 282, Loss 0.19334760308265686\n","[Training Epoch 98] Batch 283, Loss 0.15621227025985718\n","[Training Epoch 98] Batch 284, Loss 0.14875707030296326\n","[Training Epoch 98] Batch 285, Loss 0.1502625197172165\n","[Training Epoch 98] Batch 286, Loss 0.17562979459762573\n","[Training Epoch 98] Batch 287, Loss 0.16863834857940674\n","[Training Epoch 98] Batch 288, Loss 0.1715981662273407\n","[Training Epoch 98] Batch 289, Loss 0.1738724261522293\n","[Training Epoch 98] Batch 290, Loss 0.17750391364097595\n","[Training Epoch 98] Batch 291, Loss 0.181593656539917\n","[Training Epoch 98] Batch 292, Loss 0.15597066283226013\n","[Training Epoch 98] Batch 293, Loss 0.15219548344612122\n","[Training Epoch 98] Batch 294, Loss 0.1687684953212738\n","[Training Epoch 98] Batch 295, Loss 0.15168847143650055\n","[Training Epoch 98] Batch 296, Loss 0.15478408336639404\n","[Training Epoch 98] Batch 297, Loss 0.14307798445224762\n","[Training Epoch 98] Batch 298, Loss 0.1869528740644455\n","[Training Epoch 98] Batch 299, Loss 0.17418217658996582\n","[Training Epoch 98] Batch 300, Loss 0.1489061713218689\n","[Training Epoch 98] Batch 301, Loss 0.1675502210855484\n","[Training Epoch 98] Batch 302, Loss 0.16010752320289612\n","[Training Epoch 98] Batch 303, Loss 0.1741892397403717\n","[Training Epoch 98] Batch 304, Loss 0.16011476516723633\n","[Training Epoch 98] Batch 305, Loss 0.16979563236236572\n","[Training Epoch 98] Batch 306, Loss 0.16440796852111816\n","[Training Epoch 98] Batch 307, Loss 0.16406989097595215\n","[Training Epoch 98] Batch 308, Loss 0.17034342885017395\n","[Training Epoch 98] Batch 309, Loss 0.15346501767635345\n","[Training Epoch 98] Batch 310, Loss 0.17008139193058014\n","[Training Epoch 98] Batch 311, Loss 0.16969966888427734\n","[Training Epoch 98] Batch 312, Loss 0.15740802884101868\n","[Training Epoch 98] Batch 313, Loss 0.15992963314056396\n","[Training Epoch 98] Batch 314, Loss 0.1794111132621765\n","[Training Epoch 98] Batch 315, Loss 0.1768728494644165\n","[Training Epoch 98] Batch 316, Loss 0.1703909933567047\n","[Training Epoch 98] Batch 317, Loss 0.1398952156305313\n","[Training Epoch 98] Batch 318, Loss 0.17323605716228485\n","[Training Epoch 98] Batch 319, Loss 0.17558015882968903\n","[Training Epoch 98] Batch 320, Loss 0.17071841657161713\n","[Training Epoch 98] Batch 321, Loss 0.16891509294509888\n","[Training Epoch 98] Batch 322, Loss 0.17119964957237244\n","[Training Epoch 98] Batch 323, Loss 0.1633347123861313\n","[Training Epoch 98] Batch 324, Loss 0.15634289383888245\n","[Training Epoch 98] Batch 325, Loss 0.17014563083648682\n","[Training Epoch 98] Batch 326, Loss 0.13164210319519043\n","[Training Epoch 98] Batch 327, Loss 0.19095611572265625\n","[Training Epoch 98] Batch 328, Loss 0.15515176951885223\n","[Training Epoch 98] Batch 329, Loss 0.16413013637065887\n","[Training Epoch 98] Batch 330, Loss 0.14596417546272278\n","[Training Epoch 98] Batch 331, Loss 0.1645360141992569\n","[Training Epoch 98] Batch 332, Loss 0.1609756499528885\n","[Training Epoch 98] Batch 333, Loss 0.16194650530815125\n","[Training Epoch 98] Batch 334, Loss 0.1551659256219864\n","[Training Epoch 98] Batch 335, Loss 0.188482403755188\n","[Training Epoch 98] Batch 336, Loss 0.17553208768367767\n","[Training Epoch 98] Batch 337, Loss 0.15144971013069153\n","[Training Epoch 98] Batch 338, Loss 0.15846553444862366\n","[Training Epoch 98] Batch 339, Loss 0.17656520009040833\n","[Training Epoch 98] Batch 340, Loss 0.15996888279914856\n","[Training Epoch 98] Batch 341, Loss 0.1589270979166031\n","[Training Epoch 98] Batch 342, Loss 0.17656999826431274\n","[Training Epoch 98] Batch 343, Loss 0.16108226776123047\n","[Training Epoch 98] Batch 344, Loss 0.16041919589042664\n","[Training Epoch 98] Batch 345, Loss 0.16894279420375824\n","[Training Epoch 98] Batch 346, Loss 0.15669696033000946\n","[Training Epoch 98] Batch 347, Loss 0.1693766713142395\n","[Training Epoch 98] Batch 348, Loss 0.16085875034332275\n","[Training Epoch 98] Batch 349, Loss 0.13891440629959106\n","[Training Epoch 98] Batch 350, Loss 0.18944428861141205\n","[Training Epoch 98] Batch 351, Loss 0.17073464393615723\n","[Training Epoch 98] Batch 352, Loss 0.14124611020088196\n","[Training Epoch 98] Batch 353, Loss 0.1769264042377472\n","[Training Epoch 98] Batch 354, Loss 0.17421954870224\n","[Training Epoch 98] Batch 355, Loss 0.14919432997703552\n","[Training Epoch 98] Batch 356, Loss 0.18247658014297485\n","[Training Epoch 98] Batch 357, Loss 0.1623012125492096\n","[Training Epoch 98] Batch 358, Loss 0.16930469870567322\n","[Training Epoch 98] Batch 359, Loss 0.14740057289600372\n","[Training Epoch 98] Batch 360, Loss 0.17068985104560852\n","[Training Epoch 98] Batch 361, Loss 0.1509479284286499\n","[Training Epoch 98] Batch 362, Loss 0.15422658622264862\n","[Training Epoch 98] Batch 363, Loss 0.1627746820449829\n","[Training Epoch 98] Batch 364, Loss 0.15195131301879883\n","[Training Epoch 98] Batch 365, Loss 0.1939663141965866\n","[Training Epoch 98] Batch 366, Loss 0.14537161588668823\n","[Training Epoch 98] Batch 367, Loss 0.17515408992767334\n","[Training Epoch 98] Batch 368, Loss 0.16934137046337128\n","[Training Epoch 98] Batch 369, Loss 0.13973471522331238\n","[Training Epoch 98] Batch 370, Loss 0.17610113322734833\n","[Training Epoch 98] Batch 371, Loss 0.17159497737884521\n","[Training Epoch 98] Batch 372, Loss 0.16093264520168304\n","[Training Epoch 98] Batch 373, Loss 0.17497506737709045\n","[Training Epoch 98] Batch 374, Loss 0.1653934121131897\n","[Training Epoch 98] Batch 375, Loss 0.17467963695526123\n","[Training Epoch 98] Batch 376, Loss 0.17212054133415222\n","[Training Epoch 98] Batch 377, Loss 0.1778448224067688\n","[Training Epoch 98] Batch 378, Loss 0.170381098985672\n","[Training Epoch 98] Batch 379, Loss 0.1585938036441803\n","[Training Epoch 98] Batch 380, Loss 0.16692307591438293\n","[Training Epoch 98] Batch 381, Loss 0.15681549906730652\n","[Training Epoch 98] Batch 382, Loss 0.17984923720359802\n","[Training Epoch 98] Batch 383, Loss 0.17790274322032928\n","[Training Epoch 98] Batch 384, Loss 0.1466519832611084\n","[Training Epoch 98] Batch 385, Loss 0.1514023244380951\n","[Training Epoch 98] Batch 386, Loss 0.1692715287208557\n","[Training Epoch 98] Batch 387, Loss 0.17004942893981934\n","[Training Epoch 98] Batch 388, Loss 0.16834814846515656\n","[Training Epoch 98] Batch 389, Loss 0.16543272137641907\n","[Training Epoch 98] Batch 390, Loss 0.14096789062023163\n","[Training Epoch 98] Batch 391, Loss 0.17753417789936066\n","[Training Epoch 98] Batch 392, Loss 0.15519478917121887\n","[Training Epoch 98] Batch 393, Loss 0.17655420303344727\n","[Training Epoch 98] Batch 394, Loss 0.1828625202178955\n","[Training Epoch 98] Batch 395, Loss 0.15569598972797394\n","[Training Epoch 98] Batch 396, Loss 0.17495350539684296\n","[Training Epoch 98] Batch 397, Loss 0.1703166663646698\n","[Training Epoch 98] Batch 398, Loss 0.16089171171188354\n","[Training Epoch 98] Batch 399, Loss 0.16489043831825256\n","[Training Epoch 98] Batch 400, Loss 0.14651262760162354\n","[Training Epoch 98] Batch 401, Loss 0.1535865068435669\n","[Training Epoch 98] Batch 402, Loss 0.15837439894676208\n","[Training Epoch 98] Batch 403, Loss 0.17605847120285034\n","[Training Epoch 98] Batch 404, Loss 0.1554519236087799\n","[Training Epoch 98] Batch 405, Loss 0.17190486192703247\n","[Training Epoch 98] Batch 406, Loss 0.1817954182624817\n","[Training Epoch 98] Batch 407, Loss 0.17298348248004913\n","[Training Epoch 98] Batch 408, Loss 0.17255720496177673\n","[Training Epoch 98] Batch 409, Loss 0.1809496134519577\n","[Training Epoch 98] Batch 410, Loss 0.15766218304634094\n","[Training Epoch 98] Batch 411, Loss 0.16701558232307434\n","[Training Epoch 98] Batch 412, Loss 0.17124563455581665\n","[Training Epoch 98] Batch 413, Loss 0.1837393343448639\n","[Training Epoch 98] Batch 414, Loss 0.14985200762748718\n","[Training Epoch 98] Batch 415, Loss 0.1621052473783493\n","[Training Epoch 98] Batch 416, Loss 0.18057170510292053\n","[Training Epoch 98] Batch 417, Loss 0.1580822467803955\n","[Training Epoch 98] Batch 418, Loss 0.16282641887664795\n","[Training Epoch 98] Batch 419, Loss 0.15752364695072174\n","[Training Epoch 98] Batch 420, Loss 0.16263628005981445\n","[Training Epoch 98] Batch 421, Loss 0.15313538908958435\n","[Training Epoch 98] Batch 422, Loss 0.17247363924980164\n","[Training Epoch 98] Batch 423, Loss 0.18038932979106903\n","[Training Epoch 98] Batch 424, Loss 0.174419105052948\n","[Training Epoch 98] Batch 425, Loss 0.1421249657869339\n","[Training Epoch 98] Batch 426, Loss 0.1633797287940979\n","[Training Epoch 98] Batch 427, Loss 0.15385249257087708\n","[Training Epoch 98] Batch 428, Loss 0.17389151453971863\n","[Training Epoch 98] Batch 429, Loss 0.15382622182369232\n","[Training Epoch 98] Batch 430, Loss 0.15832775831222534\n","[Training Epoch 98] Batch 431, Loss 0.18062426149845123\n","[Training Epoch 98] Batch 432, Loss 0.1715928614139557\n","[Training Epoch 98] Batch 433, Loss 0.1728385090827942\n","[Training Epoch 98] Batch 434, Loss 0.16120406985282898\n","[Training Epoch 98] Batch 435, Loss 0.16388028860092163\n","[Training Epoch 98] Batch 436, Loss 0.17618027329444885\n","[Training Epoch 98] Batch 437, Loss 0.167634516954422\n","[Training Epoch 98] Batch 438, Loss 0.14240622520446777\n","[Training Epoch 98] Batch 439, Loss 0.16291558742523193\n","[Training Epoch 98] Batch 440, Loss 0.1808321177959442\n","[Training Epoch 98] Batch 441, Loss 0.15927909314632416\n","[Training Epoch 98] Batch 442, Loss 0.19318309426307678\n","[Training Epoch 98] Batch 443, Loss 0.15861517190933228\n","[Training Epoch 98] Batch 444, Loss 0.18820258975028992\n","[Training Epoch 98] Batch 445, Loss 0.17631010711193085\n","[Training Epoch 98] Batch 446, Loss 0.18010078370571136\n","[Training Epoch 98] Batch 447, Loss 0.15009599924087524\n","[Training Epoch 98] Batch 448, Loss 0.158288836479187\n","[Training Epoch 98] Batch 449, Loss 0.15376800298690796\n","[Training Epoch 98] Batch 450, Loss 0.1626751720905304\n","[Training Epoch 98] Batch 451, Loss 0.17788028717041016\n","[Training Epoch 98] Batch 452, Loss 0.1785043329000473\n","[Training Epoch 98] Batch 453, Loss 0.1685260534286499\n","[Training Epoch 98] Batch 454, Loss 0.16853225231170654\n","[Training Epoch 98] Batch 455, Loss 0.1640307903289795\n","[Training Epoch 98] Batch 456, Loss 0.17183172702789307\n","[Training Epoch 98] Batch 457, Loss 0.1552436649799347\n","[Training Epoch 98] Batch 458, Loss 0.1697593331336975\n","[Training Epoch 98] Batch 459, Loss 0.17071595788002014\n","[Training Epoch 98] Batch 460, Loss 0.14919579029083252\n","[Training Epoch 98] Batch 461, Loss 0.18335434794425964\n","[Training Epoch 98] Batch 462, Loss 0.15489739179611206\n","[Training Epoch 98] Batch 463, Loss 0.15807212889194489\n","[Training Epoch 98] Batch 464, Loss 0.16657105088233948\n","[Training Epoch 98] Batch 465, Loss 0.16254451870918274\n","[Training Epoch 98] Batch 466, Loss 0.1745491325855255\n","[Training Epoch 98] Batch 467, Loss 0.18169009685516357\n","[Training Epoch 98] Batch 468, Loss 0.1704034060239792\n","[Training Epoch 98] Batch 469, Loss 0.1716156005859375\n","[Training Epoch 98] Batch 470, Loss 0.1714441478252411\n","[Training Epoch 98] Batch 471, Loss 0.1796937882900238\n","[Training Epoch 98] Batch 472, Loss 0.16719308495521545\n","[Training Epoch 98] Batch 473, Loss 0.18346688151359558\n","[Training Epoch 98] Batch 474, Loss 0.15580794215202332\n","[Training Epoch 98] Batch 475, Loss 0.14538736641407013\n","[Training Epoch 98] Batch 476, Loss 0.16236524283885956\n","[Training Epoch 98] Batch 477, Loss 0.15902301669120789\n","[Training Epoch 98] Batch 478, Loss 0.16561934351921082\n","[Training Epoch 98] Batch 479, Loss 0.16506728529930115\n","[Training Epoch 98] Batch 480, Loss 0.16948959231376648\n","[Training Epoch 98] Batch 481, Loss 0.16217158734798431\n","[Training Epoch 98] Batch 482, Loss 0.14904873073101044\n","[Training Epoch 98] Batch 483, Loss 0.15790270268917084\n","[Training Epoch 98] Batch 484, Loss 0.1579636037349701\n","[Training Epoch 98] Batch 485, Loss 0.15386813879013062\n","[Training Epoch 98] Batch 486, Loss 0.16621840000152588\n","[Training Epoch 98] Batch 487, Loss 0.16804037988185883\n","[Training Epoch 98] Batch 488, Loss 0.16074103116989136\n","[Training Epoch 98] Batch 489, Loss 0.1722644865512848\n","[Training Epoch 98] Batch 490, Loss 0.15235888957977295\n","[Training Epoch 98] Batch 491, Loss 0.16929183900356293\n","[Training Epoch 98] Batch 492, Loss 0.14353540539741516\n","[Training Epoch 98] Batch 493, Loss 0.17776305973529816\n","[Training Epoch 98] Batch 494, Loss 0.17118218541145325\n","[Training Epoch 98] Batch 495, Loss 0.15132752060890198\n","[Training Epoch 98] Batch 496, Loss 0.16421684622764587\n","[Training Epoch 98] Batch 497, Loss 0.16463764011859894\n","[Training Epoch 98] Batch 498, Loss 0.1654694378376007\n","[Training Epoch 98] Batch 499, Loss 0.169578418135643\n","[Training Epoch 98] Batch 500, Loss 0.1612841933965683\n","[Training Epoch 98] Batch 501, Loss 0.1774677336215973\n","[Training Epoch 98] Batch 502, Loss 0.15907494723796844\n","[Training Epoch 98] Batch 503, Loss 0.15999163687229156\n","[Training Epoch 98] Batch 504, Loss 0.1788347363471985\n","[Training Epoch 98] Batch 505, Loss 0.15185940265655518\n","[Training Epoch 98] Batch 506, Loss 0.1638413965702057\n","[Training Epoch 98] Batch 507, Loss 0.160051628947258\n","[Training Epoch 98] Batch 508, Loss 0.1721181869506836\n","[Training Epoch 98] Batch 509, Loss 0.16515517234802246\n","[Training Epoch 98] Batch 510, Loss 0.17362619936466217\n","[Training Epoch 98] Batch 511, Loss 0.16638758778572083\n","[Training Epoch 98] Batch 512, Loss 0.16070720553398132\n","[Training Epoch 98] Batch 513, Loss 0.15305981040000916\n","[Training Epoch 98] Batch 514, Loss 0.16069117188453674\n","[Training Epoch 98] Batch 515, Loss 0.1761074662208557\n","[Training Epoch 98] Batch 516, Loss 0.15838605165481567\n","[Training Epoch 98] Batch 517, Loss 0.1720675677061081\n","[Training Epoch 98] Batch 518, Loss 0.1738719344139099\n","[Training Epoch 98] Batch 519, Loss 0.18362171947956085\n","[Training Epoch 98] Batch 520, Loss 0.1568477898836136\n","[Training Epoch 98] Batch 521, Loss 0.16981980204582214\n","[Training Epoch 98] Batch 522, Loss 0.15100301802158356\n","[Training Epoch 98] Batch 523, Loss 0.16886356472969055\n","[Training Epoch 98] Batch 524, Loss 0.16144752502441406\n","[Training Epoch 98] Batch 525, Loss 0.17462211847305298\n","[Training Epoch 98] Batch 526, Loss 0.16750109195709229\n","[Training Epoch 98] Batch 527, Loss 0.16811415553092957\n","[Training Epoch 98] Batch 528, Loss 0.14109744131565094\n","[Training Epoch 98] Batch 529, Loss 0.18673649430274963\n","[Training Epoch 98] Batch 530, Loss 0.1744631826877594\n","[Training Epoch 98] Batch 531, Loss 0.16824784874916077\n","[Training Epoch 98] Batch 532, Loss 0.15775081515312195\n","[Training Epoch 98] Batch 533, Loss 0.16666707396507263\n","[Training Epoch 98] Batch 534, Loss 0.18012115359306335\n","[Training Epoch 98] Batch 535, Loss 0.1841156929731369\n","[Training Epoch 98] Batch 536, Loss 0.14788414537906647\n","[Training Epoch 98] Batch 537, Loss 0.16505682468414307\n","[Training Epoch 98] Batch 538, Loss 0.18820849061012268\n","[Training Epoch 98] Batch 539, Loss 0.18293675780296326\n","[Training Epoch 98] Batch 540, Loss 0.15242008864879608\n","[Training Epoch 98] Batch 541, Loss 0.16469520330429077\n","[Training Epoch 98] Batch 542, Loss 0.1791647970676422\n","[Training Epoch 98] Batch 543, Loss 0.15271177887916565\n","[Training Epoch 98] Batch 544, Loss 0.16546319425106049\n","[Training Epoch 98] Batch 545, Loss 0.17084048688411713\n","[Training Epoch 98] Batch 546, Loss 0.154735267162323\n","[Training Epoch 98] Batch 547, Loss 0.16186553239822388\n","[Training Epoch 98] Batch 548, Loss 0.17494511604309082\n","[Training Epoch 98] Batch 549, Loss 0.1762104630470276\n","[Training Epoch 98] Batch 550, Loss 0.17381450533866882\n","[Training Epoch 98] Batch 551, Loss 0.16936054825782776\n","[Training Epoch 98] Batch 552, Loss 0.16274423897266388\n","[Training Epoch 98] Batch 553, Loss 0.17445892095565796\n","[Training Epoch 98] Batch 554, Loss 0.15217721462249756\n","[Training Epoch 98] Batch 555, Loss 0.1495896726846695\n","[Training Epoch 98] Batch 556, Loss 0.151422917842865\n","[Training Epoch 98] Batch 557, Loss 0.1643676608800888\n","[Training Epoch 98] Batch 558, Loss 0.1763531118631363\n","[Training Epoch 98] Batch 559, Loss 0.15309056639671326\n","[Training Epoch 98] Batch 560, Loss 0.15852278470993042\n","[Training Epoch 98] Batch 561, Loss 0.15795603394508362\n","[Training Epoch 98] Batch 562, Loss 0.17828461527824402\n","[Training Epoch 98] Batch 563, Loss 0.14765897393226624\n","[Training Epoch 98] Batch 564, Loss 0.18595482409000397\n","[Training Epoch 98] Batch 565, Loss 0.1582353115081787\n","[Training Epoch 98] Batch 566, Loss 0.16980096697807312\n","[Training Epoch 98] Batch 567, Loss 0.16774405539035797\n","[Training Epoch 98] Batch 568, Loss 0.16651740670204163\n","[Training Epoch 98] Batch 569, Loss 0.1941739022731781\n","[Training Epoch 98] Batch 570, Loss 0.15032562613487244\n","[Training Epoch 98] Batch 571, Loss 0.17212215065956116\n","[Training Epoch 98] Batch 572, Loss 0.1552666425704956\n","[Training Epoch 98] Batch 573, Loss 0.17976194620132446\n","[Training Epoch 98] Batch 574, Loss 0.1749505251646042\n","[Training Epoch 98] Batch 575, Loss 0.16344359517097473\n","[Training Epoch 98] Batch 576, Loss 0.15995249152183533\n","[Training Epoch 98] Batch 577, Loss 0.15399587154388428\n","[Training Epoch 98] Batch 578, Loss 0.16875863075256348\n","[Training Epoch 98] Batch 579, Loss 0.15004655718803406\n","[Training Epoch 98] Batch 580, Loss 0.18191595375537872\n","[Training Epoch 98] Batch 581, Loss 0.1493615210056305\n","[Training Epoch 98] Batch 582, Loss 0.18392062187194824\n","[Training Epoch 98] Batch 583, Loss 0.17340002954006195\n","[Training Epoch 98] Batch 584, Loss 0.1449948251247406\n","[Training Epoch 98] Batch 585, Loss 0.17605315148830414\n","[Training Epoch 98] Batch 586, Loss 0.17323347926139832\n","[Training Epoch 98] Batch 587, Loss 0.1474226415157318\n","[Training Epoch 98] Batch 588, Loss 0.16863873600959778\n","[Training Epoch 98] Batch 589, Loss 0.1486344039440155\n","[Training Epoch 98] Batch 590, Loss 0.15692803263664246\n","[Training Epoch 98] Batch 591, Loss 0.17787618935108185\n","[Training Epoch 98] Batch 592, Loss 0.1689205765724182\n","[Training Epoch 98] Batch 593, Loss 0.16913241147994995\n","[Training Epoch 98] Batch 594, Loss 0.1587904691696167\n","[Training Epoch 98] Batch 595, Loss 0.13936331868171692\n","[Training Epoch 98] Batch 596, Loss 0.144074946641922\n","[Training Epoch 98] Batch 597, Loss 0.17159754037857056\n","[Training Epoch 98] Batch 598, Loss 0.18811294436454773\n","[Training Epoch 98] Batch 599, Loss 0.16097807884216309\n","[Training Epoch 98] Batch 600, Loss 0.17288854718208313\n","[Training Epoch 98] Batch 601, Loss 0.18702389299869537\n","[Training Epoch 98] Batch 602, Loss 0.16686902940273285\n","[Training Epoch 98] Batch 603, Loss 0.17494359612464905\n","[Training Epoch 98] Batch 604, Loss 0.17661091685295105\n","[Training Epoch 98] Batch 605, Loss 0.1691609025001526\n","[Training Epoch 98] Batch 606, Loss 0.1830139458179474\n","[Training Epoch 98] Batch 607, Loss 0.17146188020706177\n","[Training Epoch 98] Batch 608, Loss 0.18713371455669403\n","[Training Epoch 98] Batch 609, Loss 0.15998320281505585\n","[Training Epoch 98] Batch 610, Loss 0.1427701711654663\n","[Training Epoch 98] Batch 611, Loss 0.16488465666770935\n","[Training Epoch 98] Batch 612, Loss 0.18788333237171173\n","[Training Epoch 98] Batch 613, Loss 0.1697576940059662\n","[Training Epoch 98] Batch 614, Loss 0.17776897549629211\n","[Training Epoch 98] Batch 615, Loss 0.17780126631259918\n","[Training Epoch 98] Batch 616, Loss 0.15466539561748505\n","[Training Epoch 98] Batch 617, Loss 0.17076241970062256\n","[Training Epoch 98] Batch 618, Loss 0.1654554307460785\n","[Training Epoch 98] Batch 619, Loss 0.18030232191085815\n","[Training Epoch 98] Batch 620, Loss 0.17974478006362915\n","[Training Epoch 98] Batch 621, Loss 0.1634746789932251\n","[Training Epoch 98] Batch 622, Loss 0.17637276649475098\n","[Training Epoch 98] Batch 623, Loss 0.17287074029445648\n","[Training Epoch 98] Batch 624, Loss 0.15098503232002258\n","[Training Epoch 98] Batch 625, Loss 0.1631329357624054\n","[Training Epoch 98] Batch 626, Loss 0.17111736536026\n","[Training Epoch 98] Batch 627, Loss 0.15578222274780273\n","[Training Epoch 98] Batch 628, Loss 0.176425501704216\n","[Training Epoch 98] Batch 629, Loss 0.1819775104522705\n","[Training Epoch 98] Batch 630, Loss 0.15950490534305573\n","[Training Epoch 98] Batch 631, Loss 0.1603001058101654\n","[Training Epoch 98] Batch 632, Loss 0.1746283769607544\n","[Training Epoch 98] Batch 633, Loss 0.15398207306861877\n","[Training Epoch 98] Batch 634, Loss 0.1545465588569641\n","[Training Epoch 98] Batch 635, Loss 0.17657092213630676\n","[Training Epoch 98] Batch 636, Loss 0.17699545621871948\n","[Training Epoch 98] Batch 637, Loss 0.1424323171377182\n","[Training Epoch 98] Batch 638, Loss 0.14348992705345154\n","[Training Epoch 98] Batch 639, Loss 0.15718621015548706\n","[Training Epoch 98] Batch 640, Loss 0.15744715929031372\n","[Training Epoch 98] Batch 641, Loss 0.17031344771385193\n","[Training Epoch 98] Batch 642, Loss 0.17336463928222656\n","[Training Epoch 98] Batch 643, Loss 0.16026267409324646\n","[Training Epoch 98] Batch 644, Loss 0.1588684618473053\n","[Training Epoch 98] Batch 645, Loss 0.1673719733953476\n","[Training Epoch 98] Batch 646, Loss 0.16952145099639893\n","[Training Epoch 98] Batch 647, Loss 0.17011825740337372\n","[Training Epoch 98] Batch 648, Loss 0.18348777294158936\n","[Training Epoch 98] Batch 649, Loss 0.16528265178203583\n","[Training Epoch 98] Batch 650, Loss 0.17799367010593414\n","[Training Epoch 98] Batch 651, Loss 0.14974522590637207\n","[Training Epoch 98] Batch 652, Loss 0.17130231857299805\n","[Training Epoch 98] Batch 653, Loss 0.16384956240653992\n","[Training Epoch 98] Batch 654, Loss 0.1925017535686493\n","[Training Epoch 98] Batch 655, Loss 0.15195614099502563\n","[Training Epoch 98] Batch 656, Loss 0.1628509759902954\n","[Training Epoch 98] Batch 657, Loss 0.1710408627986908\n","[Training Epoch 98] Batch 658, Loss 0.18790830671787262\n","[Training Epoch 98] Batch 659, Loss 0.16355815529823303\n","[Training Epoch 98] Batch 660, Loss 0.16115975379943848\n","[Training Epoch 98] Batch 661, Loss 0.15846258401870728\n","[Training Epoch 98] Batch 662, Loss 0.18936729431152344\n","[Training Epoch 98] Batch 663, Loss 0.17451107501983643\n","[Training Epoch 98] Batch 664, Loss 0.17734931409358978\n","[Training Epoch 98] Batch 665, Loss 0.14555864036083221\n","[Training Epoch 98] Batch 666, Loss 0.18293285369873047\n","[Training Epoch 98] Batch 667, Loss 0.17658111453056335\n","[Training Epoch 98] Batch 668, Loss 0.18168090283870697\n","[Training Epoch 98] Batch 669, Loss 0.16555841267108917\n","[Training Epoch 98] Batch 670, Loss 0.22049382328987122\n","[Training Epoch 98] Batch 671, Loss 0.14418184757232666\n","[Training Epoch 98] Batch 672, Loss 0.18078124523162842\n","[Training Epoch 98] Batch 673, Loss 0.1825641393661499\n","[Training Epoch 98] Batch 674, Loss 0.16974923014640808\n","[Training Epoch 98] Batch 675, Loss 0.16087070107460022\n","[Training Epoch 98] Batch 676, Loss 0.17163123190402985\n","[Training Epoch 98] Batch 677, Loss 0.14975780248641968\n","[Training Epoch 98] Batch 678, Loss 0.1802131086587906\n","[Training Epoch 98] Batch 679, Loss 0.15091818571090698\n","[Training Epoch 98] Batch 680, Loss 0.16226032376289368\n","[Training Epoch 98] Batch 681, Loss 0.16650184988975525\n","[Training Epoch 98] Batch 682, Loss 0.16977465152740479\n","[Training Epoch 98] Batch 683, Loss 0.15936143696308136\n","[Training Epoch 98] Batch 684, Loss 0.16348399221897125\n","[Training Epoch 98] Batch 685, Loss 0.15219256281852722\n","[Training Epoch 98] Batch 686, Loss 0.1578214168548584\n","[Training Epoch 98] Batch 687, Loss 0.1673283725976944\n","[Training Epoch 98] Batch 688, Loss 0.1447416990995407\n","[Training Epoch 98] Batch 689, Loss 0.16456307470798492\n","[Training Epoch 98] Batch 690, Loss 0.17432697117328644\n","[Training Epoch 98] Batch 691, Loss 0.17470240592956543\n","[Training Epoch 98] Batch 692, Loss 0.1598244607448578\n","[Training Epoch 98] Batch 693, Loss 0.16581152379512787\n","[Training Epoch 98] Batch 694, Loss 0.14028000831604004\n","[Training Epoch 98] Batch 695, Loss 0.16263903677463531\n","[Training Epoch 98] Batch 696, Loss 0.1428891122341156\n","[Training Epoch 98] Batch 697, Loss 0.1420842707157135\n","[Training Epoch 98] Batch 698, Loss 0.1517333686351776\n","[Training Epoch 98] Batch 699, Loss 0.17621663212776184\n","[Training Epoch 98] Batch 700, Loss 0.14762529730796814\n","[Training Epoch 98] Batch 701, Loss 0.1344214528799057\n","[Training Epoch 98] Batch 702, Loss 0.1538657546043396\n","[Training Epoch 98] Batch 703, Loss 0.16217689216136932\n","[Training Epoch 98] Batch 704, Loss 0.1937258243560791\n","[Training Epoch 98] Batch 705, Loss 0.16358911991119385\n","[Training Epoch 98] Batch 706, Loss 0.17153824865818024\n","[Training Epoch 98] Batch 707, Loss 0.16846159100532532\n","[Training Epoch 98] Batch 708, Loss 0.17958401143550873\n","[Training Epoch 98] Batch 709, Loss 0.17295271158218384\n","[Training Epoch 98] Batch 710, Loss 0.15226604044437408\n","[Training Epoch 98] Batch 711, Loss 0.15967437624931335\n","[Training Epoch 98] Batch 712, Loss 0.1761905699968338\n","[Training Epoch 98] Batch 713, Loss 0.18944129347801208\n","[Training Epoch 98] Batch 714, Loss 0.1749102771282196\n","[Training Epoch 98] Batch 715, Loss 0.1813131868839264\n","[Training Epoch 98] Batch 716, Loss 0.16653719544410706\n","[Training Epoch 98] Batch 717, Loss 0.1634218394756317\n","[Training Epoch 98] Batch 718, Loss 0.1757393181324005\n","[Training Epoch 98] Batch 719, Loss 0.18971210718154907\n","[Training Epoch 98] Batch 720, Loss 0.15253841876983643\n","[Training Epoch 98] Batch 721, Loss 0.14963345229625702\n","[Training Epoch 98] Batch 722, Loss 0.1632876843214035\n","[Training Epoch 98] Batch 723, Loss 0.1763555407524109\n","[Training Epoch 98] Batch 724, Loss 0.1550818532705307\n","[Training Epoch 98] Batch 725, Loss 0.154719740152359\n","[Training Epoch 98] Batch 726, Loss 0.15445944666862488\n","[Training Epoch 98] Batch 727, Loss 0.1755724549293518\n","[Training Epoch 98] Batch 728, Loss 0.15817177295684814\n","[Training Epoch 98] Batch 729, Loss 0.15572232007980347\n","[Training Epoch 98] Batch 730, Loss 0.13498950004577637\n","[Training Epoch 98] Batch 731, Loss 0.15680287778377533\n","[Training Epoch 98] Batch 732, Loss 0.1709556132555008\n","[Training Epoch 98] Batch 733, Loss 0.16525444388389587\n","[Training Epoch 98] Batch 734, Loss 0.17407917976379395\n","[Training Epoch 98] Batch 735, Loss 0.16484840214252472\n","[Training Epoch 98] Batch 736, Loss 0.1649126261472702\n","[Training Epoch 98] Batch 737, Loss 0.1506105661392212\n","[Training Epoch 98] Batch 738, Loss 0.17789477109909058\n","[Training Epoch 98] Batch 739, Loss 0.17499618232250214\n","[Training Epoch 98] Batch 740, Loss 0.1641368269920349\n","[Training Epoch 98] Batch 741, Loss 0.15716387331485748\n","[Training Epoch 98] Batch 742, Loss 0.19245140254497528\n","[Training Epoch 98] Batch 743, Loss 0.18002602458000183\n","[Training Epoch 98] Batch 744, Loss 0.16109445691108704\n","[Training Epoch 98] Batch 745, Loss 0.16465352475643158\n","[Training Epoch 98] Batch 746, Loss 0.16025419533252716\n","[Training Epoch 98] Batch 747, Loss 0.14910423755645752\n","[Training Epoch 98] Batch 748, Loss 0.18480123579502106\n","[Training Epoch 98] Batch 749, Loss 0.16537019610404968\n","[Training Epoch 98] Batch 750, Loss 0.16160714626312256\n","[Training Epoch 98] Batch 751, Loss 0.1787368357181549\n","[Training Epoch 98] Batch 752, Loss 0.16433009505271912\n","[Training Epoch 98] Batch 753, Loss 0.17179277539253235\n","[Training Epoch 98] Batch 754, Loss 0.18785378336906433\n","[Training Epoch 98] Batch 755, Loss 0.16963253915309906\n","[Training Epoch 98] Batch 756, Loss 0.16400247812271118\n","[Training Epoch 98] Batch 757, Loss 0.1651298999786377\n","[Training Epoch 98] Batch 758, Loss 0.18034115433692932\n","[Training Epoch 98] Batch 759, Loss 0.16410040855407715\n","[Training Epoch 98] Batch 760, Loss 0.1963115632534027\n","[Training Epoch 98] Batch 761, Loss 0.15873095393180847\n","[Training Epoch 98] Batch 762, Loss 0.1455536037683487\n","[Training Epoch 98] Batch 763, Loss 0.18093907833099365\n","[Training Epoch 98] Batch 764, Loss 0.17566189169883728\n","[Training Epoch 98] Batch 765, Loss 0.16694486141204834\n","[Training Epoch 98] Batch 766, Loss 0.174485981464386\n","[Training Epoch 98] Batch 767, Loss 0.13839834928512573\n","[Training Epoch 98] Batch 768, Loss 0.1587485671043396\n","[Training Epoch 98] Batch 769, Loss 0.16030970215797424\n","[Training Epoch 98] Batch 770, Loss 0.16469213366508484\n","[Training Epoch 98] Batch 771, Loss 0.13997289538383484\n","[Training Epoch 98] Batch 772, Loss 0.19251704216003418\n","[Training Epoch 98] Batch 773, Loss 0.15680989623069763\n","[Training Epoch 98] Batch 774, Loss 0.16216574609279633\n","[Training Epoch 98] Batch 775, Loss 0.1540784388780594\n","[Training Epoch 98] Batch 776, Loss 0.16827723383903503\n","[Training Epoch 98] Batch 777, Loss 0.14989019930362701\n","[Training Epoch 98] Batch 778, Loss 0.17422765493392944\n","[Training Epoch 98] Batch 779, Loss 0.17797008156776428\n","[Training Epoch 98] Batch 780, Loss 0.1586211621761322\n","[Training Epoch 98] Batch 781, Loss 0.18946798145771027\n","[Training Epoch 98] Batch 782, Loss 0.16862794756889343\n","[Training Epoch 98] Batch 783, Loss 0.1742851734161377\n","[Training Epoch 98] Batch 784, Loss 0.14990369975566864\n","[Training Epoch 98] Batch 785, Loss 0.16567806899547577\n","[Training Epoch 98] Batch 786, Loss 0.2033485770225525\n","[Training Epoch 98] Batch 787, Loss 0.16291116178035736\n","[Training Epoch 98] Batch 788, Loss 0.15748491883277893\n","[Training Epoch 98] Batch 789, Loss 0.1547790914773941\n","[Training Epoch 98] Batch 790, Loss 0.16438038647174835\n","[Training Epoch 98] Batch 791, Loss 0.17391306161880493\n","[Training Epoch 98] Batch 792, Loss 0.1756671816110611\n","[Training Epoch 98] Batch 793, Loss 0.16800729930400848\n","[Training Epoch 98] Batch 794, Loss 0.18420831859111786\n","[Training Epoch 98] Batch 795, Loss 0.16163286566734314\n","[Training Epoch 98] Batch 796, Loss 0.15173178911209106\n","[Training Epoch 98] Batch 797, Loss 0.1703525334596634\n","[Training Epoch 98] Batch 798, Loss 0.17422786355018616\n","[Training Epoch 98] Batch 799, Loss 0.15268510580062866\n","[Training Epoch 98] Batch 800, Loss 0.16181580722332\n","[Training Epoch 98] Batch 801, Loss 0.16456669569015503\n","[Training Epoch 98] Batch 802, Loss 0.13738881051540375\n","[Training Epoch 98] Batch 803, Loss 0.15652430057525635\n","[Training Epoch 98] Batch 804, Loss 0.18798533082008362\n","[Training Epoch 98] Batch 805, Loss 0.16159918904304504\n","[Training Epoch 98] Batch 806, Loss 0.17721086740493774\n","[Training Epoch 98] Batch 807, Loss 0.16448161005973816\n","[Training Epoch 98] Batch 808, Loss 0.18314200639724731\n","[Training Epoch 98] Batch 809, Loss 0.16473187506198883\n","[Training Epoch 98] Batch 810, Loss 0.15874609351158142\n","[Training Epoch 98] Batch 811, Loss 0.16635742783546448\n","[Training Epoch 98] Batch 812, Loss 0.16330355405807495\n","[Training Epoch 98] Batch 813, Loss 0.16891953349113464\n","[Training Epoch 98] Batch 814, Loss 0.16743037104606628\n","[Training Epoch 98] Batch 815, Loss 0.1649363934993744\n","[Training Epoch 98] Batch 816, Loss 0.15860725939273834\n","[Training Epoch 98] Batch 817, Loss 0.16551300883293152\n","[Training Epoch 98] Batch 818, Loss 0.15288686752319336\n","[Training Epoch 98] Batch 819, Loss 0.1577862799167633\n","[Training Epoch 98] Batch 820, Loss 0.160403311252594\n","[Training Epoch 98] Batch 821, Loss 0.17250996828079224\n","[Training Epoch 98] Batch 822, Loss 0.18417367339134216\n","[Training Epoch 98] Batch 823, Loss 0.17836175858974457\n","[Training Epoch 98] Batch 824, Loss 0.17867408692836761\n","[Training Epoch 98] Batch 825, Loss 0.177679643034935\n","[Training Epoch 98] Batch 826, Loss 0.15990924835205078\n","[Training Epoch 98] Batch 827, Loss 0.15672567486763\n","[Training Epoch 98] Batch 828, Loss 0.1709514707326889\n","[Training Epoch 98] Batch 829, Loss 0.18209657073020935\n","[Training Epoch 98] Batch 830, Loss 0.15655583143234253\n","[Training Epoch 98] Batch 831, Loss 0.17785035073757172\n","[Training Epoch 98] Batch 832, Loss 0.18066781759262085\n","[Training Epoch 98] Batch 833, Loss 0.1654200553894043\n","[Training Epoch 98] Batch 834, Loss 0.17592285573482513\n","[Training Epoch 98] Batch 835, Loss 0.15033335983753204\n","[Training Epoch 98] Batch 836, Loss 0.18305784463882446\n","[Training Epoch 98] Batch 837, Loss 0.15799114108085632\n","[Training Epoch 98] Batch 838, Loss 0.15664175152778625\n","[Training Epoch 98] Batch 839, Loss 0.16816875338554382\n","[Training Epoch 98] Batch 840, Loss 0.154878631234169\n","[Training Epoch 98] Batch 841, Loss 0.16288065910339355\n","[Training Epoch 98] Batch 842, Loss 0.16933897137641907\n","[Training Epoch 98] Batch 843, Loss 0.1945311725139618\n","[Training Epoch 98] Batch 844, Loss 0.16715282201766968\n","[Training Epoch 98] Batch 845, Loss 0.15885773301124573\n","[Training Epoch 98] Batch 846, Loss 0.1660662293434143\n","[Training Epoch 98] Batch 847, Loss 0.17948958277702332\n","[Training Epoch 98] Batch 848, Loss 0.17315427958965302\n","[Training Epoch 98] Batch 849, Loss 0.17197296023368835\n","[Training Epoch 98] Batch 850, Loss 0.16245609521865845\n","[Training Epoch 98] Batch 851, Loss 0.17139483988285065\n","[Training Epoch 98] Batch 852, Loss 0.18780812621116638\n","[Training Epoch 98] Batch 853, Loss 0.16203847527503967\n","[Training Epoch 98] Batch 854, Loss 0.14883267879486084\n","[Training Epoch 98] Batch 855, Loss 0.15753887593746185\n","[Training Epoch 98] Batch 856, Loss 0.15772667527198792\n","[Training Epoch 98] Batch 857, Loss 0.15479648113250732\n","[Training Epoch 98] Batch 858, Loss 0.17587114870548248\n","[Training Epoch 98] Batch 859, Loss 0.18143528699874878\n","[Training Epoch 98] Batch 860, Loss 0.17158976197242737\n","[Training Epoch 98] Batch 861, Loss 0.17846110463142395\n","[Training Epoch 98] Batch 862, Loss 0.18151457607746124\n","[Training Epoch 98] Batch 863, Loss 0.1761050522327423\n","[Training Epoch 98] Batch 864, Loss 0.15688812732696533\n","[Training Epoch 98] Batch 865, Loss 0.17238476872444153\n","[Training Epoch 98] Batch 866, Loss 0.1507996916770935\n","[Training Epoch 98] Batch 867, Loss 0.1746109127998352\n","[Training Epoch 98] Batch 868, Loss 0.19055378437042236\n","[Training Epoch 98] Batch 869, Loss 0.15331009030342102\n","[Training Epoch 98] Batch 870, Loss 0.16358348727226257\n","[Training Epoch 98] Batch 871, Loss 0.19169342517852783\n","[Training Epoch 98] Batch 872, Loss 0.16466493904590607\n","[Training Epoch 98] Batch 873, Loss 0.16178593039512634\n","[Training Epoch 98] Batch 874, Loss 0.15874333679676056\n","[Training Epoch 98] Batch 875, Loss 0.17093506455421448\n","[Training Epoch 98] Batch 876, Loss 0.188945010304451\n","[Training Epoch 98] Batch 877, Loss 0.16909493505954742\n","[Training Epoch 98] Batch 878, Loss 0.1690119504928589\n","[Training Epoch 98] Batch 879, Loss 0.15401864051818848\n","[Training Epoch 98] Batch 880, Loss 0.17926755547523499\n","[Training Epoch 98] Batch 881, Loss 0.1792568862438202\n","[Training Epoch 98] Batch 882, Loss 0.17101749777793884\n","[Training Epoch 98] Batch 883, Loss 0.1899392008781433\n","[Training Epoch 98] Batch 884, Loss 0.17505942285060883\n","[Training Epoch 98] Batch 885, Loss 0.19335471093654633\n","[Training Epoch 98] Batch 886, Loss 0.15497992932796478\n","[Training Epoch 98] Batch 887, Loss 0.1596854329109192\n","[Training Epoch 98] Batch 888, Loss 0.15814590454101562\n","[Training Epoch 98] Batch 889, Loss 0.16798818111419678\n","[Training Epoch 98] Batch 890, Loss 0.16692577302455902\n","[Training Epoch 98] Batch 891, Loss 0.1424294412136078\n","[Training Epoch 98] Batch 892, Loss 0.1619824767112732\n","[Training Epoch 98] Batch 893, Loss 0.18896639347076416\n","[Training Epoch 98] Batch 894, Loss 0.16903838515281677\n","[Training Epoch 98] Batch 895, Loss 0.15237754583358765\n","[Training Epoch 98] Batch 896, Loss 0.16970273852348328\n","[Training Epoch 98] Batch 897, Loss 0.15441855788230896\n","[Training Epoch 98] Batch 898, Loss 0.1746293157339096\n","[Training Epoch 98] Batch 899, Loss 0.159247025847435\n","[Training Epoch 98] Batch 900, Loss 0.18346445262432098\n","[Training Epoch 98] Batch 901, Loss 0.17145852744579315\n","[Training Epoch 98] Batch 902, Loss 0.18415609002113342\n","[Training Epoch 98] Batch 903, Loss 0.17264941334724426\n","[Training Epoch 98] Batch 904, Loss 0.18617698550224304\n","[Training Epoch 98] Batch 905, Loss 0.17436543107032776\n","[Training Epoch 98] Batch 906, Loss 0.17443794012069702\n","[Training Epoch 98] Batch 907, Loss 0.1748523712158203\n","[Training Epoch 98] Batch 908, Loss 0.16310037672519684\n","[Training Epoch 98] Batch 909, Loss 0.16913335025310516\n","[Training Epoch 98] Batch 910, Loss 0.17549550533294678\n","[Training Epoch 98] Batch 911, Loss 0.1829245686531067\n","[Training Epoch 98] Batch 912, Loss 0.18329069018363953\n","[Training Epoch 98] Batch 913, Loss 0.17048586905002594\n","[Training Epoch 98] Batch 914, Loss 0.16921046376228333\n","[Training Epoch 98] Batch 915, Loss 0.16490913927555084\n","[Training Epoch 98] Batch 916, Loss 0.17768855392932892\n","[Training Epoch 98] Batch 917, Loss 0.1496453583240509\n","[Training Epoch 98] Batch 918, Loss 0.1324319839477539\n","[Training Epoch 98] Batch 919, Loss 0.1371016800403595\n","[Training Epoch 98] Batch 920, Loss 0.17522341012954712\n","[Training Epoch 98] Batch 921, Loss 0.1730465590953827\n","[Training Epoch 98] Batch 922, Loss 0.16314971446990967\n","[Training Epoch 98] Batch 923, Loss 0.16344764828681946\n","[Training Epoch 98] Batch 924, Loss 0.18413054943084717\n","[Training Epoch 98] Batch 925, Loss 0.17568758130073547\n","[Training Epoch 98] Batch 926, Loss 0.18561191856861115\n","[Training Epoch 98] Batch 927, Loss 0.1540476530790329\n","[Training Epoch 98] Batch 928, Loss 0.17814084887504578\n","[Training Epoch 98] Batch 929, Loss 0.17250308394432068\n","[Training Epoch 98] Batch 930, Loss 0.17544983327388763\n","[Training Epoch 98] Batch 931, Loss 0.14161911606788635\n","[Training Epoch 98] Batch 932, Loss 0.15778706967830658\n","[Training Epoch 98] Batch 933, Loss 0.1601121723651886\n","[Training Epoch 98] Batch 934, Loss 0.15614256262779236\n","[Training Epoch 98] Batch 935, Loss 0.17014779150485992\n","[Training Epoch 98] Batch 936, Loss 0.16562479734420776\n","[Training Epoch 98] Batch 937, Loss 0.17531946301460266\n","[Training Epoch 98] Batch 938, Loss 0.1684134304523468\n","[Training Epoch 98] Batch 939, Loss 0.16341623663902283\n","[Training Epoch 98] Batch 940, Loss 0.17397744953632355\n","[Training Epoch 98] Batch 941, Loss 0.16642476618289948\n","[Training Epoch 98] Batch 942, Loss 0.15698760747909546\n","[Training Epoch 98] Batch 943, Loss 0.1591014564037323\n","[Training Epoch 98] Batch 944, Loss 0.19032378494739532\n","[Training Epoch 98] Batch 945, Loss 0.1595066636800766\n","[Training Epoch 98] Batch 946, Loss 0.164313405752182\n","[Training Epoch 98] Batch 947, Loss 0.17280460894107819\n","[Training Epoch 98] Batch 948, Loss 0.16847111284732819\n","[Training Epoch 98] Batch 949, Loss 0.1531137228012085\n","[Training Epoch 98] Batch 950, Loss 0.16053718328475952\n","[Training Epoch 98] Batch 951, Loss 0.18465635180473328\n","[Training Epoch 98] Batch 952, Loss 0.14838598668575287\n","[Training Epoch 98] Batch 953, Loss 0.18158304691314697\n","[Training Epoch 98] Batch 954, Loss 0.156950905919075\n","[Training Epoch 98] Batch 955, Loss 0.15488040447235107\n","[Training Epoch 98] Batch 956, Loss 0.15986166894435883\n","[Training Epoch 98] Batch 957, Loss 0.16984397172927856\n","[Training Epoch 98] Batch 958, Loss 0.15380412340164185\n","[Training Epoch 98] Batch 959, Loss 0.16414985060691833\n","[Training Epoch 98] Batch 960, Loss 0.16731402277946472\n","[Training Epoch 98] Batch 961, Loss 0.1715608537197113\n","[Training Epoch 98] Batch 962, Loss 0.17822793126106262\n","[Training Epoch 98] Batch 963, Loss 0.18315058946609497\n","[Training Epoch 98] Batch 964, Loss 0.18536069989204407\n","[Training Epoch 98] Batch 965, Loss 0.16326895356178284\n","[Training Epoch 98] Batch 966, Loss 0.1588326394557953\n","[Training Epoch 98] Batch 967, Loss 0.16012339293956757\n","[Training Epoch 98] Batch 968, Loss 0.17626136541366577\n","[Training Epoch 98] Batch 969, Loss 0.1794467568397522\n","[Training Epoch 98] Batch 970, Loss 0.15855249762535095\n","[Training Epoch 98] Batch 971, Loss 0.17771440744400024\n","[Training Epoch 98] Batch 972, Loss 0.16916251182556152\n","[Training Epoch 98] Batch 973, Loss 0.20267486572265625\n","[Training Epoch 98] Batch 974, Loss 0.186314195394516\n","[Training Epoch 98] Batch 975, Loss 0.16922158002853394\n","[Training Epoch 98] Batch 976, Loss 0.17810842394828796\n","[Training Epoch 98] Batch 977, Loss 0.17371970415115356\n","[Training Epoch 98] Batch 978, Loss 0.17978262901306152\n","[Training Epoch 98] Batch 979, Loss 0.15547479689121246\n","[Training Epoch 98] Batch 980, Loss 0.17308275401592255\n","[Training Epoch 98] Batch 981, Loss 0.1547873318195343\n","[Training Epoch 98] Batch 982, Loss 0.1700083613395691\n","[Training Epoch 98] Batch 983, Loss 0.15482112765312195\n","[Training Epoch 98] Batch 984, Loss 0.16693302989006042\n","[Training Epoch 98] Batch 985, Loss 0.1691218465566635\n","[Training Epoch 98] Batch 986, Loss 0.15551802515983582\n","[Training Epoch 98] Batch 987, Loss 0.17124135792255402\n","[Training Epoch 98] Batch 988, Loss 0.14351004362106323\n","[Training Epoch 98] Batch 989, Loss 0.16558900475502014\n","[Training Epoch 98] Batch 990, Loss 0.1666882038116455\n","[Training Epoch 98] Batch 991, Loss 0.16952729225158691\n","[Training Epoch 98] Batch 992, Loss 0.15587495267391205\n","[Training Epoch 98] Batch 993, Loss 0.16255713999271393\n","[Training Epoch 98] Batch 994, Loss 0.17469698190689087\n","[Training Epoch 98] Batch 995, Loss 0.16462799906730652\n","[Training Epoch 98] Batch 996, Loss 0.18981723487377167\n","[Training Epoch 98] Batch 997, Loss 0.17844098806381226\n","[Training Epoch 98] Batch 998, Loss 0.18339842557907104\n","[Training Epoch 98] Batch 999, Loss 0.19946593046188354\n","[Training Epoch 98] Batch 1000, Loss 0.15761296451091766\n","[Training Epoch 98] Batch 1001, Loss 0.17025676369667053\n","[Training Epoch 98] Batch 1002, Loss 0.17092660069465637\n","[Training Epoch 98] Batch 1003, Loss 0.18101701140403748\n","[Training Epoch 98] Batch 1004, Loss 0.15060186386108398\n","[Training Epoch 98] Batch 1005, Loss 0.17296606302261353\n","[Training Epoch 98] Batch 1006, Loss 0.177208811044693\n","[Training Epoch 98] Batch 1007, Loss 0.1710476279258728\n","[Training Epoch 98] Batch 1008, Loss 0.15335096418857574\n","[Training Epoch 98] Batch 1009, Loss 0.17059513926506042\n","[Training Epoch 98] Batch 1010, Loss 0.13910534977912903\n","[Training Epoch 98] Batch 1011, Loss 0.1860826015472412\n","[Training Epoch 98] Batch 1012, Loss 0.17113757133483887\n","[Training Epoch 98] Batch 1013, Loss 0.18029974400997162\n","[Training Epoch 98] Batch 1014, Loss 0.1814257651567459\n","[Training Epoch 98] Batch 1015, Loss 0.17466041445732117\n","[Training Epoch 98] Batch 1016, Loss 0.1654457449913025\n","[Training Epoch 98] Batch 1017, Loss 0.17268265783786774\n","[Training Epoch 98] Batch 1018, Loss 0.15280039608478546\n","[Training Epoch 98] Batch 1019, Loss 0.18734422326087952\n","[Training Epoch 98] Batch 1020, Loss 0.16118274629116058\n","[Training Epoch 98] Batch 1021, Loss 0.1731727570295334\n","[Training Epoch 98] Batch 1022, Loss 0.16969013214111328\n","[Training Epoch 98] Batch 1023, Loss 0.1438073366880417\n","[Training Epoch 98] Batch 1024, Loss 0.18436096608638763\n","[Training Epoch 98] Batch 1025, Loss 0.17369475960731506\n","[Training Epoch 98] Batch 1026, Loss 0.174503356218338\n","[Training Epoch 98] Batch 1027, Loss 0.16727082431316376\n","[Training Epoch 98] Batch 1028, Loss 0.1446477770805359\n","[Training Epoch 98] Batch 1029, Loss 0.15795227885246277\n","[Training Epoch 98] Batch 1030, Loss 0.15842124819755554\n","[Training Epoch 98] Batch 1031, Loss 0.16129857301712036\n","[Training Epoch 98] Batch 1032, Loss 0.1789039969444275\n","[Training Epoch 98] Batch 1033, Loss 0.1780053675174713\n","[Training Epoch 98] Batch 1034, Loss 0.1780114322900772\n","[Training Epoch 98] Batch 1035, Loss 0.18000000715255737\n","[Training Epoch 98] Batch 1036, Loss 0.14824096858501434\n","[Training Epoch 98] Batch 1037, Loss 0.16863740980625153\n","[Training Epoch 98] Batch 1038, Loss 0.15503369271755219\n","[Training Epoch 98] Batch 1039, Loss 0.18357661366462708\n","[Training Epoch 98] Batch 1040, Loss 0.16278526186943054\n","[Training Epoch 98] Batch 1041, Loss 0.18713191151618958\n","[Training Epoch 98] Batch 1042, Loss 0.16100068390369415\n","[Training Epoch 98] Batch 1043, Loss 0.1644349992275238\n","[Training Epoch 98] Batch 1044, Loss 0.17189478874206543\n","[Training Epoch 98] Batch 1045, Loss 0.15266023576259613\n","[Training Epoch 98] Batch 1046, Loss 0.16936537623405457\n","[Training Epoch 98] Batch 1047, Loss 0.1926020383834839\n","[Training Epoch 98] Batch 1048, Loss 0.16372472047805786\n","[Training Epoch 98] Batch 1049, Loss 0.16714075207710266\n","[Training Epoch 98] Batch 1050, Loss 0.16887792944908142\n","[Training Epoch 98] Batch 1051, Loss 0.1716655045747757\n","[Training Epoch 98] Batch 1052, Loss 0.16725404560565948\n","[Training Epoch 98] Batch 1053, Loss 0.16796496510505676\n","[Training Epoch 98] Batch 1054, Loss 0.17718085646629333\n","[Training Epoch 98] Batch 1055, Loss 0.18468688428401947\n","[Training Epoch 98] Batch 1056, Loss 0.1590590626001358\n","[Training Epoch 98] Batch 1057, Loss 0.14426341652870178\n","[Training Epoch 98] Batch 1058, Loss 0.15449023246765137\n","[Training Epoch 98] Batch 1059, Loss 0.167727530002594\n","[Training Epoch 98] Batch 1060, Loss 0.1709686815738678\n","[Training Epoch 98] Batch 1061, Loss 0.15979057550430298\n","[Training Epoch 98] Batch 1062, Loss 0.21225884556770325\n","[Training Epoch 98] Batch 1063, Loss 0.1435474455356598\n","[Training Epoch 98] Batch 1064, Loss 0.16996131837368011\n","[Training Epoch 98] Batch 1065, Loss 0.1681685894727707\n","[Training Epoch 98] Batch 1066, Loss 0.1702701449394226\n","[Training Epoch 98] Batch 1067, Loss 0.1385810375213623\n","[Training Epoch 98] Batch 1068, Loss 0.15791407227516174\n","[Training Epoch 98] Batch 1069, Loss 0.1646260917186737\n","[Training Epoch 98] Batch 1070, Loss 0.1757529079914093\n","[Training Epoch 98] Batch 1071, Loss 0.16639478504657745\n","[Training Epoch 98] Batch 1072, Loss 0.16591131687164307\n","[Training Epoch 98] Batch 1073, Loss 0.16838526725769043\n","[Training Epoch 98] Batch 1074, Loss 0.16816681623458862\n","[Training Epoch 98] Batch 1075, Loss 0.16228848695755005\n","[Training Epoch 98] Batch 1076, Loss 0.18933247029781342\n","[Training Epoch 98] Batch 1077, Loss 0.17435681819915771\n","[Training Epoch 98] Batch 1078, Loss 0.18013986945152283\n","[Training Epoch 98] Batch 1079, Loss 0.16325724124908447\n","[Training Epoch 98] Batch 1080, Loss 0.15127718448638916\n","[Training Epoch 98] Batch 1081, Loss 0.17234623432159424\n","[Training Epoch 98] Batch 1082, Loss 0.1442318558692932\n","[Training Epoch 98] Batch 1083, Loss 0.1916758120059967\n","[Training Epoch 98] Batch 1084, Loss 0.1542835831642151\n","[Training Epoch 98] Batch 1085, Loss 0.15454497933387756\n","[Training Epoch 98] Batch 1086, Loss 0.16435852646827698\n","[Training Epoch 98] Batch 1087, Loss 0.17041417956352234\n","[Training Epoch 98] Batch 1088, Loss 0.1697784662246704\n","[Training Epoch 98] Batch 1089, Loss 0.15926580131053925\n","[Training Epoch 98] Batch 1090, Loss 0.18936792016029358\n","[Training Epoch 98] Batch 1091, Loss 0.16741496324539185\n","[Training Epoch 98] Batch 1092, Loss 0.1698991060256958\n","[Training Epoch 98] Batch 1093, Loss 0.16653364896774292\n","[Training Epoch 98] Batch 1094, Loss 0.1703629493713379\n","[Training Epoch 98] Batch 1095, Loss 0.16438472270965576\n","[Training Epoch 98] Batch 1096, Loss 0.17726373672485352\n","[Training Epoch 98] Batch 1097, Loss 0.1852266788482666\n","[Training Epoch 98] Batch 1098, Loss 0.1581089198589325\n","[Training Epoch 98] Batch 1099, Loss 0.1698012501001358\n","[Training Epoch 98] Batch 1100, Loss 0.19357308745384216\n","[Training Epoch 98] Batch 1101, Loss 0.15487359464168549\n","[Training Epoch 98] Batch 1102, Loss 0.17131096124649048\n","[Training Epoch 98] Batch 1103, Loss 0.18489107489585876\n","[Training Epoch 98] Batch 1104, Loss 0.16223543882369995\n","[Training Epoch 98] Batch 1105, Loss 0.16285693645477295\n","[Training Epoch 98] Batch 1106, Loss 0.13956771790981293\n","[Training Epoch 98] Batch 1107, Loss 0.18595454096794128\n","[Training Epoch 98] Batch 1108, Loss 0.17019324004650116\n","[Training Epoch 98] Batch 1109, Loss 0.15322141349315643\n","[Training Epoch 98] Batch 1110, Loss 0.17492976784706116\n","[Training Epoch 98] Batch 1111, Loss 0.17482610046863556\n","[Training Epoch 98] Batch 1112, Loss 0.14952240884304047\n","[Training Epoch 98] Batch 1113, Loss 0.1566220223903656\n","[Training Epoch 98] Batch 1114, Loss 0.14701920747756958\n","[Training Epoch 98] Batch 1115, Loss 0.17371854186058044\n","[Training Epoch 98] Batch 1116, Loss 0.1699356734752655\n","[Training Epoch 98] Batch 1117, Loss 0.19611307978630066\n","[Training Epoch 98] Batch 1118, Loss 0.18347880244255066\n","[Training Epoch 98] Batch 1119, Loss 0.1967095136642456\n","[Training Epoch 98] Batch 1120, Loss 0.1632574498653412\n","[Training Epoch 98] Batch 1121, Loss 0.1547575294971466\n","[Training Epoch 98] Batch 1122, Loss 0.1733071208000183\n","[Training Epoch 98] Batch 1123, Loss 0.16353096067905426\n","[Training Epoch 98] Batch 1124, Loss 0.17117390036582947\n","[Training Epoch 98] Batch 1125, Loss 0.18077021837234497\n","[Training Epoch 98] Batch 1126, Loss 0.14653219282627106\n","[Training Epoch 98] Batch 1127, Loss 0.17053550481796265\n","[Training Epoch 98] Batch 1128, Loss 0.15907779335975647\n","[Training Epoch 98] Batch 1129, Loss 0.15420596301555634\n","[Training Epoch 98] Batch 1130, Loss 0.17089825868606567\n","[Training Epoch 98] Batch 1131, Loss 0.1565360426902771\n","[Training Epoch 98] Batch 1132, Loss 0.16847309470176697\n","[Training Epoch 98] Batch 1133, Loss 0.1811176836490631\n","[Training Epoch 98] Batch 1134, Loss 0.17544624209403992\n","[Training Epoch 98] Batch 1135, Loss 0.19879917800426483\n","[Training Epoch 98] Batch 1136, Loss 0.16958753764629364\n","[Training Epoch 98] Batch 1137, Loss 0.1615150272846222\n","[Training Epoch 98] Batch 1138, Loss 0.1545664519071579\n","[Training Epoch 98] Batch 1139, Loss 0.13370294868946075\n","[Training Epoch 98] Batch 1140, Loss 0.15869468450546265\n","[Training Epoch 98] Batch 1141, Loss 0.20006579160690308\n","[Training Epoch 98] Batch 1142, Loss 0.16969045996665955\n","[Training Epoch 98] Batch 1143, Loss 0.1702253818511963\n","[Training Epoch 98] Batch 1144, Loss 0.1600957214832306\n","[Training Epoch 98] Batch 1145, Loss 0.15740227699279785\n","[Training Epoch 98] Batch 1146, Loss 0.1634182631969452\n","[Training Epoch 98] Batch 1147, Loss 0.16653941571712494\n","[Training Epoch 98] Batch 1148, Loss 0.15933771431446075\n","[Training Epoch 98] Batch 1149, Loss 0.18789935111999512\n","[Training Epoch 98] Batch 1150, Loss 0.18775801360607147\n","[Training Epoch 98] Batch 1151, Loss 0.17576353251934052\n","[Training Epoch 98] Batch 1152, Loss 0.1507141888141632\n","[Training Epoch 98] Batch 1153, Loss 0.17031767964363098\n","[Training Epoch 98] Batch 1154, Loss 0.16845066845417023\n","[Training Epoch 98] Batch 1155, Loss 0.1600506603717804\n","[Training Epoch 98] Batch 1156, Loss 0.18617868423461914\n","[Training Epoch 98] Batch 1157, Loss 0.16741658747196198\n","[Training Epoch 98] Batch 1158, Loss 0.16312673687934875\n","[Training Epoch 98] Batch 1159, Loss 0.1507556438446045\n","[Training Epoch 98] Batch 1160, Loss 0.16616715490818024\n","/content/drive/MyDrive/Neural-CF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 98] HR = 0.7162, NDCG = 0.3496\n","Epoch 99 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 99] Batch 0, Loss 0.1384885013103485\n","[Training Epoch 99] Batch 1, Loss 0.1634041965007782\n","[Training Epoch 99] Batch 2, Loss 0.1304929554462433\n","[Training Epoch 99] Batch 3, Loss 0.15001818537712097\n","[Training Epoch 99] Batch 4, Loss 0.19571486115455627\n","[Training Epoch 99] Batch 5, Loss 0.1705821305513382\n","[Training Epoch 99] Batch 6, Loss 0.18322300910949707\n","[Training Epoch 99] Batch 7, Loss 0.16665291786193848\n","[Training Epoch 99] Batch 8, Loss 0.16886427998542786\n","[Training Epoch 99] Batch 9, Loss 0.1591622233390808\n","[Training Epoch 99] Batch 10, Loss 0.16646811366081238\n","[Training Epoch 99] Batch 11, Loss 0.16083279252052307\n","[Training Epoch 99] Batch 12, Loss 0.1713958978652954\n","[Training Epoch 99] Batch 13, Loss 0.14394748210906982\n","[Training Epoch 99] Batch 14, Loss 0.16000036895275116\n","[Training Epoch 99] Batch 15, Loss 0.1623552143573761\n","[Training Epoch 99] Batch 16, Loss 0.16920125484466553\n","[Training Epoch 99] Batch 17, Loss 0.14612139761447906\n","[Training Epoch 99] Batch 18, Loss 0.20017054677009583\n","[Training Epoch 99] Batch 19, Loss 0.18811795115470886\n","[Training Epoch 99] Batch 20, Loss 0.14150184392929077\n","[Training Epoch 99] Batch 21, Loss 0.14242418110370636\n","[Training Epoch 99] Batch 22, Loss 0.1421836018562317\n","[Training Epoch 99] Batch 23, Loss 0.16133297979831696\n","[Training Epoch 99] Batch 24, Loss 0.1729690283536911\n","[Training Epoch 99] Batch 25, Loss 0.1442529559135437\n","[Training Epoch 99] Batch 26, Loss 0.15361973643302917\n","[Training Epoch 99] Batch 27, Loss 0.16578131914138794\n","[Training Epoch 99] Batch 28, Loss 0.15017375349998474\n","[Training Epoch 99] Batch 29, Loss 0.1661975085735321\n","[Training Epoch 99] Batch 30, Loss 0.14754384756088257\n","[Training Epoch 99] Batch 31, Loss 0.14584049582481384\n","[Training Epoch 99] Batch 32, Loss 0.17596086859703064\n","[Training Epoch 99] Batch 33, Loss 0.14599663019180298\n","[Training Epoch 99] Batch 34, Loss 0.15787506103515625\n","[Training Epoch 99] Batch 35, Loss 0.1458970457315445\n","[Training Epoch 99] Batch 36, Loss 0.17531460523605347\n","[Training Epoch 99] Batch 37, Loss 0.14995206892490387\n","[Training Epoch 99] Batch 38, Loss 0.15698379278182983\n","[Training Epoch 99] Batch 39, Loss 0.1625528335571289\n","[Training Epoch 99] Batch 40, Loss 0.17499223351478577\n","[Training Epoch 99] Batch 41, Loss 0.16876430809497833\n","[Training Epoch 99] Batch 42, Loss 0.1680988073348999\n","[Training Epoch 99] Batch 43, Loss 0.16778618097305298\n","[Training Epoch 99] Batch 44, Loss 0.17380477488040924\n","[Training Epoch 99] Batch 45, Loss 0.16954070329666138\n","[Training Epoch 99] Batch 46, Loss 0.16432492434978485\n","[Training Epoch 99] Batch 47, Loss 0.14114952087402344\n","[Training Epoch 99] Batch 48, Loss 0.18104657530784607\n","[Training Epoch 99] Batch 49, Loss 0.171016126871109\n","[Training Epoch 99] Batch 50, Loss 0.1672443300485611\n","[Training Epoch 99] Batch 51, Loss 0.17626607418060303\n","[Training Epoch 99] Batch 52, Loss 0.1477113962173462\n","[Training Epoch 99] Batch 53, Loss 0.1963217854499817\n","[Training Epoch 99] Batch 54, Loss 0.17302681505680084\n","[Training Epoch 99] Batch 55, Loss 0.15570834279060364\n","[Training Epoch 99] Batch 56, Loss 0.16035670042037964\n","[Training Epoch 99] Batch 57, Loss 0.15389417111873627\n","[Training Epoch 99] Batch 58, Loss 0.17774361371994019\n","[Training Epoch 99] Batch 59, Loss 0.14783307909965515\n","[Training Epoch 99] Batch 60, Loss 0.1826736330986023\n","[Training Epoch 99] Batch 61, Loss 0.15384742617607117\n","[Training Epoch 99] Batch 62, Loss 0.15613220632076263\n","[Training Epoch 99] Batch 63, Loss 0.17485851049423218\n","[Training Epoch 99] Batch 64, Loss 0.1571657359600067\n","[Training Epoch 99] Batch 65, Loss 0.1611657738685608\n","[Training Epoch 99] Batch 66, Loss 0.15819674730300903\n","[Training Epoch 99] Batch 67, Loss 0.17413334548473358\n","[Training Epoch 99] Batch 68, Loss 0.16722148656845093\n","[Training Epoch 99] Batch 69, Loss 0.15622752904891968\n","[Training Epoch 99] Batch 70, Loss 0.16234621405601501\n","[Training Epoch 99] Batch 71, Loss 0.18474678695201874\n","[Training Epoch 99] Batch 72, Loss 0.16037362813949585\n","[Training Epoch 99] Batch 73, Loss 0.16076180338859558\n","[Training Epoch 99] Batch 74, Loss 0.1791885793209076\n","[Training Epoch 99] Batch 75, Loss 0.16545841097831726\n","[Training Epoch 99] Batch 76, Loss 0.16409361362457275\n","[Training Epoch 99] Batch 77, Loss 0.17940492928028107\n","[Training Epoch 99] Batch 78, Loss 0.1666094809770584\n","[Training Epoch 99] Batch 79, Loss 0.1513977199792862\n","[Training Epoch 99] Batch 80, Loss 0.14695551991462708\n","[Training Epoch 99] Batch 81, Loss 0.14613303542137146\n","[Training Epoch 99] Batch 82, Loss 0.17820173501968384\n","[Training Epoch 99] Batch 83, Loss 0.16143798828125\n","[Training Epoch 99] Batch 84, Loss 0.1517440676689148\n","[Training Epoch 99] Batch 85, Loss 0.17538732290267944\n","[Training Epoch 99] Batch 86, Loss 0.15543632209300995\n","[Training Epoch 99] Batch 87, Loss 0.16298407316207886\n","[Training Epoch 99] Batch 88, Loss 0.18598762154579163\n","[Training Epoch 99] Batch 89, Loss 0.15944519639015198\n","[Training Epoch 99] Batch 90, Loss 0.14788195490837097\n","[Training Epoch 99] Batch 91, Loss 0.15882357954978943\n","[Training Epoch 99] Batch 92, Loss 0.16129650175571442\n","[Training Epoch 99] Batch 93, Loss 0.17817598581314087\n","[Training Epoch 99] Batch 94, Loss 0.18550896644592285\n","[Training Epoch 99] Batch 95, Loss 0.17290601134300232\n","[Training Epoch 99] Batch 96, Loss 0.1926688253879547\n","[Training Epoch 99] Batch 97, Loss 0.16086144745349884\n","[Training Epoch 99] Batch 98, Loss 0.15641359984874725\n","[Training Epoch 99] Batch 99, Loss 0.16271670162677765\n","[Training Epoch 99] Batch 100, Loss 0.1576785445213318\n","[Training Epoch 99] Batch 101, Loss 0.1691584587097168\n","[Training Epoch 99] Batch 102, Loss 0.17496198415756226\n","[Training Epoch 99] Batch 103, Loss 0.1712060272693634\n","[Training Epoch 99] Batch 104, Loss 0.18323345482349396\n","[Training Epoch 99] Batch 105, Loss 0.1722889244556427\n","[Training Epoch 99] Batch 106, Loss 0.1665332019329071\n","[Training Epoch 99] Batch 107, Loss 0.15120743215084076\n","[Training Epoch 99] Batch 108, Loss 0.1728615015745163\n","[Training Epoch 99] Batch 109, Loss 0.16029846668243408\n","[Training Epoch 99] Batch 110, Loss 0.18337517976760864\n","[Training Epoch 99] Batch 111, Loss 0.16446168720722198\n","[Training Epoch 99] Batch 112, Loss 0.15694987773895264\n","[Training Epoch 99] Batch 113, Loss 0.13728269934654236\n","[Training Epoch 99] Batch 114, Loss 0.1580202877521515\n","[Training Epoch 99] Batch 115, Loss 0.17596933245658875\n","[Training Epoch 99] Batch 116, Loss 0.1551024466753006\n","[Training Epoch 99] Batch 117, Loss 0.1712862253189087\n","[Training Epoch 99] Batch 118, Loss 0.14211252331733704\n","[Training Epoch 99] Batch 119, Loss 0.17513418197631836\n","[Training Epoch 99] Batch 120, Loss 0.18426723778247833\n","[Training Epoch 99] Batch 121, Loss 0.14338013529777527\n","[Training Epoch 99] Batch 122, Loss 0.20407351851463318\n","[Training Epoch 99] Batch 123, Loss 0.13554003834724426\n","[Training Epoch 99] Batch 124, Loss 0.15042176842689514\n","[Training Epoch 99] Batch 125, Loss 0.14865565299987793\n","[Training Epoch 99] Batch 126, Loss 0.18267473578453064\n","[Training Epoch 99] Batch 127, Loss 0.15814003348350525\n","[Training Epoch 99] Batch 128, Loss 0.1703314483165741\n","[Training Epoch 99] Batch 129, Loss 0.17204852402210236\n","[Training Epoch 99] Batch 130, Loss 0.17276057600975037\n","[Training Epoch 99] Batch 131, Loss 0.15842708945274353\n","[Training Epoch 99] Batch 132, Loss 0.14839079976081848\n","[Training Epoch 99] Batch 133, Loss 0.17039117217063904\n","[Training Epoch 99] Batch 134, Loss 0.1630355417728424\n","[Training Epoch 99] Batch 135, Loss 0.15725359320640564\n","[Training Epoch 99] Batch 136, Loss 0.17341449856758118\n","[Training Epoch 99] Batch 137, Loss 0.17755620181560516\n","[Training Epoch 99] Batch 138, Loss 0.16879378259181976\n","[Training Epoch 99] Batch 139, Loss 0.16699492931365967\n","[Training Epoch 99] Batch 140, Loss 0.17633777856826782\n","[Training Epoch 99] Batch 141, Loss 0.16924327611923218\n","[Training Epoch 99] Batch 142, Loss 0.15490493178367615\n","[Training Epoch 99] Batch 143, Loss 0.16872766613960266\n","[Training Epoch 99] Batch 144, Loss 0.18191927671432495\n","[Training Epoch 99] Batch 145, Loss 0.15487557649612427\n","[Training Epoch 99] Batch 146, Loss 0.1657819002866745\n","[Training Epoch 99] Batch 147, Loss 0.16858790814876556\n","[Training Epoch 99] Batch 148, Loss 0.1662074476480484\n","[Training Epoch 99] Batch 149, Loss 0.15772360563278198\n","[Training Epoch 99] Batch 150, Loss 0.18347477912902832\n","[Training Epoch 99] Batch 151, Loss 0.1718740612268448\n","[Training Epoch 99] Batch 152, Loss 0.1545354127883911\n","[Training Epoch 99] Batch 153, Loss 0.14398282766342163\n","[Training Epoch 99] Batch 154, Loss 0.1804734766483307\n","[Training Epoch 99] Batch 155, Loss 0.16525323688983917\n","[Training Epoch 99] Batch 156, Loss 0.1649458110332489\n","[Training Epoch 99] Batch 157, Loss 0.1924734115600586\n","[Training Epoch 99] Batch 158, Loss 0.15817612409591675\n","[Training Epoch 99] Batch 159, Loss 0.1535084843635559\n","[Training Epoch 99] Batch 160, Loss 0.1514887511730194\n","[Training Epoch 99] Batch 161, Loss 0.16260898113250732\n","[Training Epoch 99] Batch 162, Loss 0.1596742868423462\n","[Training Epoch 99] Batch 163, Loss 0.16710254549980164\n","[Training Epoch 99] Batch 164, Loss 0.17041656374931335\n","[Training Epoch 99] Batch 165, Loss 0.1753387749195099\n","[Training Epoch 99] Batch 166, Loss 0.16235396265983582\n","[Training Epoch 99] Batch 167, Loss 0.16408368945121765\n","[Training Epoch 99] Batch 168, Loss 0.17999139428138733\n","[Training Epoch 99] Batch 169, Loss 0.1425933837890625\n","[Training Epoch 99] Batch 170, Loss 0.16716217994689941\n","[Training Epoch 99] Batch 171, Loss 0.1794981211423874\n","[Training Epoch 99] Batch 172, Loss 0.18884536623954773\n","[Training Epoch 99] Batch 173, Loss 0.17722123861312866\n","[Training Epoch 99] Batch 174, Loss 0.16059787571430206\n","[Training Epoch 99] Batch 175, Loss 0.14811024069786072\n","[Training Epoch 99] Batch 176, Loss 0.1485637128353119\n","[Training Epoch 99] Batch 177, Loss 0.1719086468219757\n","[Training Epoch 99] Batch 178, Loss 0.16117048263549805\n","[Training Epoch 99] Batch 179, Loss 0.1857290267944336\n","[Training Epoch 99] Batch 180, Loss 0.1398099660873413\n","[Training Epoch 99] Batch 181, Loss 0.16654160618782043\n","[Training Epoch 99] Batch 182, Loss 0.16077089309692383\n","[Training Epoch 99] Batch 183, Loss 0.1680876761674881\n","[Training Epoch 99] Batch 184, Loss 0.16781103610992432\n","[Training Epoch 99] Batch 185, Loss 0.160020112991333\n","[Training Epoch 99] Batch 186, Loss 0.14765051007270813\n","[Training Epoch 99] Batch 187, Loss 0.1621183156967163\n","[Training Epoch 99] Batch 188, Loss 0.16332489252090454\n","[Training Epoch 99] Batch 189, Loss 0.16081084311008453\n","[Training Epoch 99] Batch 190, Loss 0.15825414657592773\n","[Training Epoch 99] Batch 191, Loss 0.1551939994096756\n","[Training Epoch 99] Batch 192, Loss 0.15142615139484406\n","[Training Epoch 99] Batch 193, Loss 0.17289067804813385\n","[Training Epoch 99] Batch 194, Loss 0.1503094583749771\n","[Training Epoch 99] Batch 195, Loss 0.15974950790405273\n","[Training Epoch 99] Batch 196, Loss 0.17554306983947754\n","[Training Epoch 99] Batch 197, Loss 0.1640174686908722\n","[Training Epoch 99] Batch 198, Loss 0.148820698261261\n","[Training Epoch 99] Batch 199, Loss 0.16286447644233704\n","[Training Epoch 99] Batch 200, Loss 0.15397369861602783\n","[Training Epoch 99] Batch 201, Loss 0.13608458638191223\n","[Training Epoch 99] Batch 202, Loss 0.1539946347475052\n","[Training Epoch 99] Batch 203, Loss 0.17043882608413696\n","[Training Epoch 99] Batch 204, Loss 0.15877878665924072\n","[Training Epoch 99] Batch 205, Loss 0.14404022693634033\n","[Training Epoch 99] Batch 206, Loss 0.19527825713157654\n","[Training Epoch 99] Batch 207, Loss 0.16727036237716675\n","[Training Epoch 99] Batch 208, Loss 0.15232014656066895\n","[Training Epoch 99] Batch 209, Loss 0.17627164721488953\n","[Training Epoch 99] Batch 210, Loss 0.1657542735338211\n","[Training Epoch 99] Batch 211, Loss 0.1631098985671997\n","[Training Epoch 99] Batch 212, Loss 0.16382241249084473\n","[Training Epoch 99] Batch 213, Loss 0.15328267216682434\n","[Training Epoch 99] Batch 214, Loss 0.16885341703891754\n","[Training Epoch 99] Batch 215, Loss 0.1742742955684662\n","[Training Epoch 99] Batch 216, Loss 0.14697566628456116\n","[Training Epoch 99] Batch 217, Loss 0.1572718769311905\n","[Training Epoch 99] Batch 218, Loss 0.16531673073768616\n","[Training Epoch 99] Batch 219, Loss 0.16640253365039825\n","[Training Epoch 99] Batch 220, Loss 0.15387657284736633\n","[Training Epoch 99] Batch 221, Loss 0.1578594446182251\n","[Training Epoch 99] Batch 222, Loss 0.16377988457679749\n","[Training Epoch 99] Batch 223, Loss 0.16701194643974304\n","[Training Epoch 99] Batch 224, Loss 0.17088326811790466\n","[Training Epoch 99] Batch 225, Loss 0.1424761414527893\n","[Training Epoch 99] Batch 226, Loss 0.16631655395030975\n","[Training Epoch 99] Batch 227, Loss 0.15423211455345154\n","[Training Epoch 99] Batch 228, Loss 0.18733014166355133\n","[Training Epoch 99] Batch 229, Loss 0.1695314645767212\n","[Training Epoch 99] Batch 230, Loss 0.15665936470031738\n","[Training Epoch 99] Batch 231, Loss 0.17800401151180267\n","[Training Epoch 99] Batch 232, Loss 0.148875892162323\n","[Training Epoch 99] Batch 233, Loss 0.1555054485797882\n","[Training Epoch 99] Batch 234, Loss 0.137131929397583\n","[Training Epoch 99] Batch 235, Loss 0.18296271562576294\n","[Training Epoch 99] Batch 236, Loss 0.16683728992938995\n","[Training Epoch 99] Batch 237, Loss 0.15254205465316772\n","[Training Epoch 99] Batch 238, Loss 0.16084367036819458\n","[Training Epoch 99] Batch 239, Loss 0.17472806572914124\n","[Training Epoch 99] Batch 240, Loss 0.18326255679130554\n","[Training Epoch 99] Batch 241, Loss 0.15247231721878052\n","[Training Epoch 99] Batch 242, Loss 0.14518582820892334\n","[Training Epoch 99] Batch 243, Loss 0.1661567986011505\n","[Training Epoch 99] Batch 244, Loss 0.17145566642284393\n","[Training Epoch 99] Batch 245, Loss 0.160160630941391\n","[Training Epoch 99] Batch 246, Loss 0.1667976975440979\n","[Training Epoch 99] Batch 247, Loss 0.17965440452098846\n","[Training Epoch 99] Batch 248, Loss 0.1708054542541504\n","[Training Epoch 99] Batch 249, Loss 0.18253672122955322\n","[Training Epoch 99] Batch 250, Loss 0.1522698849439621\n","[Training Epoch 99] Batch 251, Loss 0.15595078468322754\n","[Training Epoch 99] Batch 252, Loss 0.17964135110378265\n","[Training Epoch 99] Batch 253, Loss 0.17274630069732666\n","[Training Epoch 99] Batch 254, Loss 0.1810196042060852\n","[Training Epoch 99] Batch 255, Loss 0.17366144061088562\n","[Training Epoch 99] Batch 256, Loss 0.1605662703514099\n","[Training Epoch 99] Batch 257, Loss 0.15376690030097961\n","[Training Epoch 99] Batch 258, Loss 0.14296725392341614\n","[Training Epoch 99] Batch 259, Loss 0.1665402203798294\n","[Training Epoch 99] Batch 260, Loss 0.16494911909103394\n","[Training Epoch 99] Batch 261, Loss 0.1605028510093689\n","[Training Epoch 99] Batch 262, Loss 0.16486822068691254\n","[Training Epoch 99] Batch 263, Loss 0.15370813012123108\n","[Training Epoch 99] Batch 264, Loss 0.1702854037284851\n","[Training Epoch 99] Batch 265, Loss 0.17787384986877441\n","[Training Epoch 99] Batch 266, Loss 0.1432165801525116\n","[Training Epoch 99] Batch 267, Loss 0.15472747385501862\n","[Training Epoch 99] Batch 268, Loss 0.1647954285144806\n","[Training Epoch 99] Batch 269, Loss 0.16332772374153137\n","[Training Epoch 99] Batch 270, Loss 0.1497885286808014\n","[Training Epoch 99] Batch 271, Loss 0.1767992228269577\n","[Training Epoch 99] Batch 272, Loss 0.13328509032726288\n","[Training Epoch 99] Batch 273, Loss 0.19334915280342102\n","[Training Epoch 99] Batch 274, Loss 0.17115665972232819\n","[Training Epoch 99] Batch 275, Loss 0.1775570809841156\n","[Training Epoch 99] Batch 276, Loss 0.17295068502426147\n","[Training Epoch 99] Batch 277, Loss 0.1586262285709381\n","[Training Epoch 99] Batch 278, Loss 0.15303201973438263\n","[Training Epoch 99] Batch 279, Loss 0.15011271834373474\n","[Training Epoch 99] Batch 280, Loss 0.17067798972129822\n","[Training Epoch 99] Batch 281, Loss 0.16072480380535126\n","[Training Epoch 99] Batch 282, Loss 0.1763647198677063\n","[Training Epoch 99] Batch 283, Loss 0.16451355814933777\n","[Training Epoch 99] Batch 284, Loss 0.17453166842460632\n","[Training Epoch 99] Batch 285, Loss 0.17207849025726318\n","[Training Epoch 99] Batch 286, Loss 0.15404602885246277\n","[Training Epoch 99] Batch 287, Loss 0.15786564350128174\n","[Training Epoch 99] Batch 288, Loss 0.1378529816865921\n","[Training Epoch 99] Batch 289, Loss 0.15986870229244232\n","[Training Epoch 99] Batch 290, Loss 0.15994793176651\n","[Training Epoch 99] Batch 291, Loss 0.17810282111167908\n","[Training Epoch 99] Batch 292, Loss 0.17416194081306458\n","[Training Epoch 99] Batch 293, Loss 0.14974725246429443\n","[Training Epoch 99] Batch 294, Loss 0.16965574026107788\n","[Training Epoch 99] Batch 295, Loss 0.18433214724063873\n","[Training Epoch 99] Batch 296, Loss 0.15276697278022766\n","[Training Epoch 99] Batch 297, Loss 0.16958732903003693\n","[Training Epoch 99] Batch 298, Loss 0.17995455861091614\n","[Training Epoch 99] Batch 299, Loss 0.1784166395664215\n","[Training Epoch 99] Batch 300, Loss 0.15389952063560486\n","[Training Epoch 99] Batch 301, Loss 0.16505572199821472\n","[Training Epoch 99] Batch 302, Loss 0.16456735134124756\n","[Training Epoch 99] Batch 303, Loss 0.17245741188526154\n","[Training Epoch 99] Batch 304, Loss 0.16523919999599457\n","[Training Epoch 99] Batch 305, Loss 0.16245172917842865\n","[Training Epoch 99] Batch 306, Loss 0.172970712184906\n","[Training Epoch 99] Batch 307, Loss 0.1597098559141159\n","[Training Epoch 99] Batch 308, Loss 0.17356792092323303\n","[Training Epoch 99] Batch 309, Loss 0.152347594499588\n","[Training Epoch 99] Batch 310, Loss 0.16180692613124847\n","[Training Epoch 99] Batch 311, Loss 0.16608041524887085\n","[Training Epoch 99] Batch 312, Loss 0.17714279890060425\n","[Training Epoch 99] Batch 313, Loss 0.1849772334098816\n","[Training Epoch 99] Batch 314, Loss 0.17857587337493896\n","[Training Epoch 99] Batch 315, Loss 0.14891743659973145\n","[Training Epoch 99] Batch 316, Loss 0.15804225206375122\n","[Training Epoch 99] Batch 317, Loss 0.14403177797794342\n","[Training Epoch 99] Batch 318, Loss 0.1658690869808197\n","[Training Epoch 99] Batch 319, Loss 0.1534993052482605\n","[Training Epoch 99] Batch 320, Loss 0.17464473843574524\n","[Training Epoch 99] Batch 321, Loss 0.15796583890914917\n","[Training Epoch 99] Batch 322, Loss 0.16591012477874756\n","[Training Epoch 99] Batch 323, Loss 0.15883146226406097\n","[Training Epoch 99] Batch 324, Loss 0.16294944286346436\n","[Training Epoch 99] Batch 325, Loss 0.14264804124832153\n","[Training Epoch 99] Batch 326, Loss 0.16596180200576782\n","[Training Epoch 99] Batch 327, Loss 0.19097928702831268\n","[Training Epoch 99] Batch 328, Loss 0.1683424711227417\n","[Training Epoch 99] Batch 329, Loss 0.1542460024356842\n","[Training Epoch 99] Batch 330, Loss 0.15287606418132782\n","[Training Epoch 99] Batch 331, Loss 0.17469486594200134\n","[Training Epoch 99] Batch 332, Loss 0.16155987977981567\n","[Training Epoch 99] Batch 333, Loss 0.1746845692396164\n","[Training Epoch 99] Batch 334, Loss 0.15034426748752594\n","[Training Epoch 99] Batch 335, Loss 0.18921223282814026\n","[Training Epoch 99] Batch 336, Loss 0.15689346194267273\n","[Training Epoch 99] Batch 337, Loss 0.1863410621881485\n","[Training Epoch 99] Batch 338, Loss 0.16865494847297668\n","[Training Epoch 99] Batch 339, Loss 0.1682051718235016\n","[Training Epoch 99] Batch 340, Loss 0.18041576445102692\n","[Training Epoch 99] Batch 341, Loss 0.15434539318084717\n","[Training Epoch 99] Batch 342, Loss 0.13629595935344696\n","[Training Epoch 99] Batch 343, Loss 0.1372302621603012\n","[Training Epoch 99] Batch 344, Loss 0.15167784690856934\n","[Training Epoch 99] Batch 345, Loss 0.1665097177028656\n","[Training Epoch 99] Batch 346, Loss 0.19441497325897217\n","[Training Epoch 99] Batch 347, Loss 0.13699887692928314\n","[Training Epoch 99] Batch 348, Loss 0.1549844741821289\n","[Training Epoch 99] Batch 349, Loss 0.1432884782552719\n","[Training Epoch 99] Batch 350, Loss 0.16107526421546936\n","[Training Epoch 99] Batch 351, Loss 0.16097259521484375\n","[Training Epoch 99] Batch 352, Loss 0.17524637281894684\n","[Training Epoch 99] Batch 353, Loss 0.16130568087100983\n","[Training Epoch 99] Batch 354, Loss 0.18149593472480774\n","[Training Epoch 99] Batch 355, Loss 0.12609760463237762\n","[Training Epoch 99] Batch 356, Loss 0.18653132021427155\n","[Training Epoch 99] Batch 357, Loss 0.17174366116523743\n","[Training Epoch 99] Batch 358, Loss 0.14366552233695984\n","[Training Epoch 99] Batch 359, Loss 0.16263654828071594\n","[Training Epoch 99] Batch 360, Loss 0.1844301074743271\n","[Training Epoch 99] Batch 361, Loss 0.18477989733219147\n","[Training Epoch 99] Batch 362, Loss 0.1545010209083557\n","[Training Epoch 99] Batch 363, Loss 0.19131548702716827\n","[Training Epoch 99] Batch 364, Loss 0.1719180941581726\n","[Training Epoch 99] Batch 365, Loss 0.15703462064266205\n","[Training Epoch 99] Batch 366, Loss 0.17017461359500885\n","[Training Epoch 99] Batch 367, Loss 0.16191203892230988\n","[Training Epoch 99] Batch 368, Loss 0.15970155596733093\n","[Training Epoch 99] Batch 369, Loss 0.16141024231910706\n","[Training Epoch 99] Batch 370, Loss 0.17062340676784515\n","[Training Epoch 99] Batch 371, Loss 0.17995502054691315\n","[Training Epoch 99] Batch 372, Loss 0.16883502900600433\n","[Training Epoch 99] Batch 373, Loss 0.2018139809370041\n","[Training Epoch 99] Batch 374, Loss 0.16489824652671814\n","[Training Epoch 99] Batch 375, Loss 0.18028037250041962\n","[Training Epoch 99] Batch 376, Loss 0.16802018880844116\n","[Training Epoch 99] Batch 377, Loss 0.15796901285648346\n","[Training Epoch 99] Batch 378, Loss 0.15618622303009033\n","[Training Epoch 99] Batch 379, Loss 0.15124104917049408\n","[Training Epoch 99] Batch 380, Loss 0.1563800871372223\n","[Training Epoch 99] Batch 381, Loss 0.18292883038520813\n","[Training Epoch 99] Batch 382, Loss 0.1686812937259674\n","[Training Epoch 99] Batch 383, Loss 0.15068745613098145\n","[Training Epoch 99] Batch 384, Loss 0.16580814123153687\n","[Training Epoch 99] Batch 385, Loss 0.13883084058761597\n","[Training Epoch 99] Batch 386, Loss 0.1681644320487976\n","[Training Epoch 99] Batch 387, Loss 0.18351122736930847\n","[Training Epoch 99] Batch 388, Loss 0.17759940028190613\n","[Training Epoch 99] Batch 389, Loss 0.14669065177440643\n","[Training Epoch 99] Batch 390, Loss 0.1552923172712326\n","[Training Epoch 99] Batch 391, Loss 0.16004058718681335\n","[Training Epoch 99] Batch 392, Loss 0.14836356043815613\n","[Training Epoch 99] Batch 393, Loss 0.1673838049173355\n","[Training Epoch 99] Batch 394, Loss 0.14905144274234772\n","[Training Epoch 99] Batch 395, Loss 0.16926628351211548\n","[Training Epoch 99] Batch 396, Loss 0.16368822753429413\n","[Training Epoch 99] Batch 397, Loss 0.1498468816280365\n","[Training Epoch 99] Batch 398, Loss 0.18226373195648193\n","[Training Epoch 99] Batch 399, Loss 0.17945267260074615\n","[Training Epoch 99] Batch 400, Loss 0.15900932252407074\n","[Training Epoch 99] Batch 401, Loss 0.19210803508758545\n","[Training Epoch 99] Batch 402, Loss 0.16271696984767914\n","[Training Epoch 99] Batch 403, Loss 0.1727491021156311\n","[Training Epoch 99] Batch 404, Loss 0.15139371156692505\n","[Training Epoch 99] Batch 405, Loss 0.16313967108726501\n","[Training Epoch 99] Batch 406, Loss 0.17190676927566528\n","[Training Epoch 99] Batch 407, Loss 0.166925847530365\n","[Training Epoch 99] Batch 408, Loss 0.14302033185958862\n","[Training Epoch 99] Batch 409, Loss 0.1679631769657135\n","[Training Epoch 99] Batch 410, Loss 0.165925532579422\n","[Training Epoch 99] Batch 411, Loss 0.1650576889514923\n","[Training Epoch 99] Batch 412, Loss 0.1643708348274231\n","[Training Epoch 99] Batch 413, Loss 0.16835609078407288\n","[Training Epoch 99] Batch 414, Loss 0.15466800332069397\n","[Training Epoch 99] Batch 415, Loss 0.15874835848808289\n","[Training Epoch 99] Batch 416, Loss 0.16853883862495422\n","[Training Epoch 99] Batch 417, Loss 0.15274374186992645\n","[Training Epoch 99] Batch 418, Loss 0.16399364173412323\n","[Training Epoch 99] Batch 419, Loss 0.15441666543483734\n","[Training Epoch 99] Batch 420, Loss 0.1749960333108902\n","[Training Epoch 99] Batch 421, Loss 0.1608024537563324\n","[Training Epoch 99] Batch 422, Loss 0.14478221535682678\n","[Training Epoch 99] Batch 423, Loss 0.16439807415008545\n","[Training Epoch 99] Batch 424, Loss 0.16006115078926086\n","[Training Epoch 99] Batch 425, Loss 0.17262613773345947\n","[Training Epoch 99] Batch 426, Loss 0.1606498509645462\n","[Training Epoch 99] Batch 427, Loss 0.1514468491077423\n","[Training Epoch 99] Batch 428, Loss 0.17035320401191711\n","[Training Epoch 99] Batch 429, Loss 0.17936846613883972\n","[Training Epoch 99] Batch 430, Loss 0.16697967052459717\n","[Training Epoch 99] Batch 431, Loss 0.1697748303413391\n","[Training Epoch 99] Batch 432, Loss 0.17095166444778442\n","[Training Epoch 99] Batch 433, Loss 0.15740609169006348\n","[Training Epoch 99] Batch 434, Loss 0.17000709474086761\n","[Training Epoch 99] Batch 435, Loss 0.14271050691604614\n","[Training Epoch 99] Batch 436, Loss 0.16897499561309814\n","[Training Epoch 99] Batch 437, Loss 0.15483292937278748\n","[Training Epoch 99] Batch 438, Loss 0.1651148796081543\n","[Training Epoch 99] Batch 439, Loss 0.18150447309017181\n","[Training Epoch 99] Batch 440, Loss 0.154690682888031\n","[Training Epoch 99] Batch 441, Loss 0.15801380574703217\n","[Training Epoch 99] Batch 442, Loss 0.16530850529670715\n","[Training Epoch 99] Batch 443, Loss 0.18875394761562347\n","[Training Epoch 99] Batch 444, Loss 0.17208974063396454\n","[Training Epoch 99] Batch 445, Loss 0.160007506608963\n","[Training Epoch 99] Batch 446, Loss 0.1660565733909607\n","[Training Epoch 99] Batch 447, Loss 0.16397517919540405\n","[Training Epoch 99] Batch 448, Loss 0.174680694937706\n","[Training Epoch 99] Batch 449, Loss 0.16317157447338104\n","[Training Epoch 99] Batch 450, Loss 0.18475580215454102\n","[Training Epoch 99] Batch 451, Loss 0.16737627983093262\n","[Training Epoch 99] Batch 452, Loss 0.17510992288589478\n","[Training Epoch 99] Batch 453, Loss 0.14656192064285278\n","[Training Epoch 99] Batch 454, Loss 0.1814022809267044\n","[Training Epoch 99] Batch 455, Loss 0.1503729224205017\n","[Training Epoch 99] Batch 456, Loss 0.19168958067893982\n","[Training Epoch 99] Batch 457, Loss 0.15703967213630676\n","[Training Epoch 99] Batch 458, Loss 0.16708627343177795\n","[Training Epoch 99] Batch 459, Loss 0.17965233325958252\n","[Training Epoch 99] Batch 460, Loss 0.16600467264652252\n","[Training Epoch 99] Batch 461, Loss 0.16886213421821594\n","[Training Epoch 99] Batch 462, Loss 0.1856657862663269\n","[Training Epoch 99] Batch 463, Loss 0.15941141545772552\n","[Training Epoch 99] Batch 464, Loss 0.15299926698207855\n","[Training Epoch 99] Batch 465, Loss 0.16610178351402283\n","[Training Epoch 99] Batch 466, Loss 0.1717982292175293\n","[Training Epoch 99] Batch 467, Loss 0.1719788908958435\n","[Training Epoch 99] Batch 468, Loss 0.1794542670249939\n","[Training Epoch 99] Batch 469, Loss 0.17309969663619995\n","[Training Epoch 99] Batch 470, Loss 0.1502225697040558\n","[Training Epoch 99] Batch 471, Loss 0.16518016159534454\n","[Training Epoch 99] Batch 472, Loss 0.16691941022872925\n","[Training Epoch 99] Batch 473, Loss 0.17917270958423615\n","[Training Epoch 99] Batch 474, Loss 0.1586335003376007\n","[Training Epoch 99] Batch 475, Loss 0.17377731204032898\n","[Training Epoch 99] Batch 476, Loss 0.19196724891662598\n","[Training Epoch 99] Batch 477, Loss 0.17830073833465576\n","[Training Epoch 99] Batch 478, Loss 0.18953099846839905\n","[Training Epoch 99] Batch 479, Loss 0.1462409943342209\n","[Training Epoch 99] Batch 480, Loss 0.1624634712934494\n","[Training Epoch 99] Batch 481, Loss 0.19206705689430237\n","[Training Epoch 99] Batch 482, Loss 0.16627207398414612\n","[Training Epoch 99] Batch 483, Loss 0.15313942730426788\n","[Training Epoch 99] Batch 484, Loss 0.20134475827217102\n","[Training Epoch 99] Batch 485, Loss 0.18795213103294373\n","[Training Epoch 99] Batch 486, Loss 0.1463141143321991\n","[Training Epoch 99] Batch 487, Loss 0.15820986032485962\n","[Training Epoch 99] Batch 488, Loss 0.1459656059741974\n","[Training Epoch 99] Batch 489, Loss 0.15815700590610504\n","[Training Epoch 99] Batch 490, Loss 0.17020149528980255\n","[Training Epoch 99] Batch 491, Loss 0.17014004290103912\n","[Training Epoch 99] Batch 492, Loss 0.1836891919374466\n","[Training Epoch 99] Batch 493, Loss 0.1660488247871399\n","[Training Epoch 99] Batch 494, Loss 0.1827813684940338\n","[Training Epoch 99] Batch 495, Loss 0.18245097994804382\n","[Training Epoch 99] Batch 496, Loss 0.18887212872505188\n","[Training Epoch 99] Batch 497, Loss 0.1726119965314865\n","[Training Epoch 99] Batch 498, Loss 0.1561204195022583\n","[Training Epoch 99] Batch 499, Loss 0.16531778872013092\n","[Training Epoch 99] Batch 500, Loss 0.14090833067893982\n","[Training Epoch 99] Batch 501, Loss 0.15990334749221802\n","[Training Epoch 99] Batch 502, Loss 0.1869305968284607\n","[Training Epoch 99] Batch 503, Loss 0.19250255823135376\n","[Training Epoch 99] Batch 504, Loss 0.15891575813293457\n","[Training Epoch 99] Batch 505, Loss 0.1677594631910324\n","[Training Epoch 99] Batch 506, Loss 0.1623486578464508\n","[Training Epoch 99] Batch 507, Loss 0.15123184025287628\n","[Training Epoch 99] Batch 508, Loss 0.18711590766906738\n","[Training Epoch 99] Batch 509, Loss 0.17070311307907104\n","[Training Epoch 99] Batch 510, Loss 0.17593207955360413\n","[Training Epoch 99] Batch 511, Loss 0.15188705921173096\n","[Training Epoch 99] Batch 512, Loss 0.1480790674686432\n","[Training Epoch 99] Batch 513, Loss 0.1525556743144989\n","[Training Epoch 99] Batch 514, Loss 0.17048916220664978\n","[Training Epoch 99] Batch 515, Loss 0.17385216057300568\n","[Training Epoch 99] Batch 516, Loss 0.1498948484659195\n","[Training Epoch 99] Batch 517, Loss 0.1890060007572174\n","[Training Epoch 99] Batch 518, Loss 0.1848042756319046\n","[Training Epoch 99] Batch 519, Loss 0.177263081073761\n","[Training Epoch 99] Batch 520, Loss 0.15778246521949768\n","[Training Epoch 99] Batch 521, Loss 0.18301644921302795\n","[Training Epoch 99] Batch 522, Loss 0.145114928483963\n","[Training Epoch 99] Batch 523, Loss 0.1692052185535431\n","[Training Epoch 99] Batch 524, Loss 0.14675724506378174\n","[Training Epoch 99] Batch 525, Loss 0.15783359110355377\n","[Training Epoch 99] Batch 526, Loss 0.1678955852985382\n","[Training Epoch 99] Batch 527, Loss 0.14981324970722198\n","[Training Epoch 99] Batch 528, Loss 0.15304946899414062\n","[Training Epoch 99] Batch 529, Loss 0.16592851281166077\n","[Training Epoch 99] Batch 530, Loss 0.1753016710281372\n","[Training Epoch 99] Batch 531, Loss 0.1686699092388153\n","[Training Epoch 99] Batch 532, Loss 0.1722976565361023\n","[Training Epoch 99] Batch 533, Loss 0.1624171882867813\n","[Training Epoch 99] Batch 534, Loss 0.16831481456756592\n","[Training Epoch 99] Batch 535, Loss 0.16172541677951813\n","[Training Epoch 99] Batch 536, Loss 0.16382208466529846\n","[Training Epoch 99] Batch 537, Loss 0.1643146276473999\n","[Training Epoch 99] Batch 538, Loss 0.15667185187339783\n","[Training Epoch 99] Batch 539, Loss 0.1717309057712555\n","[Training Epoch 99] Batch 540, Loss 0.15304253995418549\n","[Training Epoch 99] Batch 541, Loss 0.18296712636947632\n","[Training Epoch 99] Batch 542, Loss 0.15724533796310425\n","[Training Epoch 99] Batch 543, Loss 0.16032284498214722\n","[Training Epoch 99] Batch 544, Loss 0.1253952831029892\n","[Training Epoch 99] Batch 545, Loss 0.17448988556861877\n","[Training Epoch 99] Batch 546, Loss 0.18675300478935242\n","[Training Epoch 99] Batch 547, Loss 0.1482110172510147\n","[Training Epoch 99] Batch 548, Loss 0.15344643592834473\n","[Training Epoch 99] Batch 549, Loss 0.15735170245170593\n","[Training Epoch 99] Batch 550, Loss 0.16679416596889496\n","[Training Epoch 99] Batch 551, Loss 0.14712396264076233\n","[Training Epoch 99] Batch 552, Loss 0.17598167061805725\n","[Training Epoch 99] Batch 553, Loss 0.17293933033943176\n","[Training Epoch 99] Batch 554, Loss 0.1428259313106537\n","[Training Epoch 99] Batch 555, Loss 0.14122170209884644\n","[Training Epoch 99] Batch 556, Loss 0.19999368488788605\n","[Training Epoch 99] Batch 557, Loss 0.181647390127182\n","[Training Epoch 99] Batch 558, Loss 0.15095089375972748\n","[Training Epoch 99] Batch 559, Loss 0.15010550618171692\n","[Training Epoch 99] Batch 560, Loss 0.1827673316001892\n","[Training Epoch 99] Batch 561, Loss 0.13121670484542847\n","[Training Epoch 99] Batch 562, Loss 0.16009576618671417\n","[Training Epoch 99] Batch 563, Loss 0.15722297132015228\n","[Training Epoch 99] Batch 564, Loss 0.16188472509384155\n","[Training Epoch 99] Batch 565, Loss 0.1759255826473236\n","[Training Epoch 99] Batch 566, Loss 0.16416235268115997\n","[Training Epoch 99] Batch 567, Loss 0.18625453114509583\n","[Training Epoch 99] Batch 568, Loss 0.14510329067707062\n","[Training Epoch 99] Batch 569, Loss 0.18387441337108612\n","[Training Epoch 99] Batch 570, Loss 0.16640782356262207\n","[Training Epoch 99] Batch 571, Loss 0.13801686465740204\n","[Training Epoch 99] Batch 572, Loss 0.17746374011039734\n","[Training Epoch 99] Batch 573, Loss 0.1688445508480072\n","[Training Epoch 99] Batch 574, Loss 0.17397624254226685\n","[Training Epoch 99] Batch 575, Loss 0.19167307019233704\n","[Training Epoch 99] Batch 576, Loss 0.15543824434280396\n","[Training Epoch 99] Batch 577, Loss 0.16365765035152435\n","[Training Epoch 99] Batch 578, Loss 0.18484723567962646\n","[Training Epoch 99] Batch 579, Loss 0.1518266797065735\n","[Training Epoch 99] Batch 580, Loss 0.202803373336792\n","[Training Epoch 99] Batch 581, Loss 0.1731780767440796\n","[Training Epoch 99] Batch 582, Loss 0.1619797945022583\n","[Training Epoch 99] Batch 583, Loss 0.1671425998210907\n","[Training Epoch 99] Batch 584, Loss 0.16937756538391113\n","[Training Epoch 99] Batch 585, Loss 0.1882612109184265\n","[Training Epoch 99] Batch 586, Loss 0.16541598737239838\n","[Training Epoch 99] Batch 587, Loss 0.1725277304649353\n","[Training Epoch 99] Batch 588, Loss 0.1843910813331604\n","[Training Epoch 99] Batch 589, Loss 0.1710723340511322\n","[Training Epoch 99] Batch 590, Loss 0.146016925573349\n","[Training Epoch 99] Batch 591, Loss 0.17779122292995453\n","[Training Epoch 99] Batch 592, Loss 0.17480647563934326\n","[Training Epoch 99] Batch 593, Loss 0.1839349865913391\n","[Training Epoch 99] Batch 594, Loss 0.157620370388031\n","[Training Epoch 99] Batch 595, Loss 0.18071812391281128\n","[Training Epoch 99] Batch 596, Loss 0.1661054939031601\n","[Training Epoch 99] Batch 597, Loss 0.16064569354057312\n","[Training Epoch 99] Batch 598, Loss 0.15472082793712616\n","[Training Epoch 99] Batch 599, Loss 0.1553281992673874\n","[Training Epoch 99] Batch 600, Loss 0.1703326404094696\n","[Training Epoch 99] Batch 601, Loss 0.16395817697048187\n","[Training Epoch 99] Batch 602, Loss 0.14505858719348907\n","[Training Epoch 99] Batch 603, Loss 0.16989484429359436\n","[Training Epoch 99] Batch 604, Loss 0.15603390336036682\n","[Training Epoch 99] Batch 605, Loss 0.16854247450828552\n","[Training Epoch 99] Batch 606, Loss 0.18718399107456207\n","[Training Epoch 99] Batch 607, Loss 0.15504252910614014\n","[Training Epoch 99] Batch 608, Loss 0.1545822024345398\n","[Training Epoch 99] Batch 609, Loss 0.1749255359172821\n","[Training Epoch 99] Batch 610, Loss 0.15417185425758362\n","[Training Epoch 99] Batch 611, Loss 0.16123296320438385\n","[Training Epoch 99] Batch 612, Loss 0.16852009296417236\n","[Training Epoch 99] Batch 613, Loss 0.15781360864639282\n","[Training Epoch 99] Batch 614, Loss 0.15833786129951477\n","[Training Epoch 99] Batch 615, Loss 0.15626516938209534\n","[Training Epoch 99] Batch 616, Loss 0.18360812962055206\n","[Training Epoch 99] Batch 617, Loss 0.1580175757408142\n","[Training Epoch 99] Batch 618, Loss 0.158155620098114\n","[Training Epoch 99] Batch 619, Loss 0.15097232162952423\n","[Training Epoch 99] Batch 620, Loss 0.1721085160970688\n","[Training Epoch 99] Batch 621, Loss 0.15093883872032166\n","[Training Epoch 99] Batch 622, Loss 0.1793399453163147\n","[Training Epoch 99] Batch 623, Loss 0.17256560921669006\n","[Training Epoch 99] Batch 624, Loss 0.15617632865905762\n","[Training Epoch 99] Batch 625, Loss 0.15680614113807678\n","[Training Epoch 99] Batch 626, Loss 0.16152414679527283\n","[Training Epoch 99] Batch 627, Loss 0.17983633279800415\n","[Training Epoch 99] Batch 628, Loss 0.1655159294605255\n","[Training Epoch 99] Batch 629, Loss 0.16584277153015137\n","[Training Epoch 99] Batch 630, Loss 0.1833421289920807\n","[Training Epoch 99] Batch 631, Loss 0.18564829230308533\n","[Training Epoch 99] Batch 632, Loss 0.17080941796302795\n","[Training Epoch 99] Batch 633, Loss 0.16098880767822266\n","[Training Epoch 99] Batch 634, Loss 0.14533938467502594\n","[Training Epoch 99] Batch 635, Loss 0.15782910585403442\n","[Training Epoch 99] Batch 636, Loss 0.18428589403629303\n","[Training Epoch 99] Batch 637, Loss 0.15568211674690247\n","[Training Epoch 99] Batch 638, Loss 0.15298855304718018\n","[Training Epoch 99] Batch 639, Loss 0.181209534406662\n","[Training Epoch 99] Batch 640, Loss 0.1669851392507553\n","[Training Epoch 99] Batch 641, Loss 0.17692068219184875\n","[Training Epoch 99] Batch 642, Loss 0.18178340792655945\n","[Training Epoch 99] Batch 643, Loss 0.16069987416267395\n","[Training Epoch 99] Batch 644, Loss 0.14630210399627686\n","[Training Epoch 99] Batch 645, Loss 0.16608616709709167\n","[Training Epoch 99] Batch 646, Loss 0.19458934664726257\n","[Training Epoch 99] Batch 647, Loss 0.18579944968223572\n","[Training Epoch 99] Batch 648, Loss 0.15493617951869965\n","[Training Epoch 99] Batch 649, Loss 0.15296334028244019\n","[Training Epoch 99] Batch 650, Loss 0.17888134717941284\n","[Training Epoch 99] Batch 651, Loss 0.16802434623241425\n","[Training Epoch 99] Batch 652, Loss 0.1514301598072052\n","[Training Epoch 99] Batch 653, Loss 0.17007744312286377\n","[Training Epoch 99] Batch 654, Loss 0.15376177430152893\n","[Training Epoch 99] Batch 655, Loss 0.16324542462825775\n","[Training Epoch 99] Batch 656, Loss 0.16022253036499023\n","[Training Epoch 99] Batch 657, Loss 0.16117998957633972\n","[Training Epoch 99] Batch 658, Loss 0.18294374644756317\n","[Training Epoch 99] Batch 659, Loss 0.1511061191558838\n","[Training Epoch 99] Batch 660, Loss 0.1495894491672516\n","[Training Epoch 99] Batch 661, Loss 0.17444662749767303\n","[Training Epoch 99] Batch 662, Loss 0.1594136655330658\n","[Training Epoch 99] Batch 663, Loss 0.170608788728714\n","[Training Epoch 99] Batch 664, Loss 0.17485710978507996\n","[Training Epoch 99] Batch 665, Loss 0.15063782036304474\n","[Training Epoch 99] Batch 666, Loss 0.15278539061546326\n","[Training Epoch 99] Batch 667, Loss 0.16010472178459167\n","[Training Epoch 99] Batch 668, Loss 0.20381379127502441\n","[Training Epoch 99] Batch 669, Loss 0.14281317591667175\n","[Training Epoch 99] Batch 670, Loss 0.18223914504051208\n","[Training Epoch 99] Batch 671, Loss 0.19212299585342407\n","[Training Epoch 99] Batch 672, Loss 0.1595357060432434\n","[Training Epoch 99] Batch 673, Loss 0.16209524869918823\n","[Training Epoch 99] Batch 674, Loss 0.1643277406692505\n","[Training Epoch 99] Batch 675, Loss 0.16384461522102356\n","[Training Epoch 99] Batch 676, Loss 0.16931679844856262\n","[Training Epoch 99] Batch 677, Loss 0.19340303540229797\n","[Training Epoch 99] Batch 678, Loss 0.15688714385032654\n","[Training Epoch 99] Batch 679, Loss 0.16329020261764526\n","[Training Epoch 99] Batch 680, Loss 0.16096535325050354\n","[Training Epoch 99] Batch 681, Loss 0.14981709420681\n","[Training Epoch 99] Batch 682, Loss 0.1558263599872589\n","[Training Epoch 99] Batch 683, Loss 0.15890535712242126\n","[Training Epoch 99] Batch 684, Loss 0.1481708586215973\n","[Training Epoch 99] Batch 685, Loss 0.14623185992240906\n","[Training Epoch 99] Batch 686, Loss 0.15541701018810272\n","[Training Epoch 99] Batch 687, Loss 0.16116154193878174\n","[Training Epoch 99] Batch 688, Loss 0.18179745972156525\n","[Training Epoch 99] Batch 689, Loss 0.15468254685401917\n","[Training Epoch 99] Batch 690, Loss 0.15578725934028625\n","[Training Epoch 99] Batch 691, Loss 0.17230257391929626\n","[Training Epoch 99] Batch 692, Loss 0.1609758883714676\n","[Training Epoch 99] Batch 693, Loss 0.15993648767471313\n","[Training Epoch 99] Batch 694, Loss 0.1620558500289917\n","[Training Epoch 99] Batch 695, Loss 0.1843118965625763\n","[Training Epoch 99] Batch 696, Loss 0.16798096895217896\n","[Training Epoch 99] Batch 697, Loss 0.14466047286987305\n","[Training Epoch 99] Batch 698, Loss 0.17054718732833862\n","[Training Epoch 99] Batch 699, Loss 0.16093069314956665\n","[Training Epoch 99] Batch 700, Loss 0.16046808660030365\n","[Training Epoch 99] Batch 701, Loss 0.19868417084217072\n","[Training Epoch 99] Batch 702, Loss 0.18049994111061096\n","[Training Epoch 99] Batch 703, Loss 0.16609811782836914\n","[Training Epoch 99] Batch 704, Loss 0.194685697555542\n","[Training Epoch 99] Batch 705, Loss 0.16169191896915436\n","[Training Epoch 99] Batch 706, Loss 0.17368462681770325\n","[Training Epoch 99] Batch 707, Loss 0.15530535578727722\n","[Training Epoch 99] Batch 708, Loss 0.18876580893993378\n","[Training Epoch 99] Batch 709, Loss 0.17729735374450684\n","[Training Epoch 99] Batch 710, Loss 0.1854015737771988\n","[Training Epoch 99] Batch 711, Loss 0.15110421180725098\n","[Training Epoch 99] Batch 712, Loss 0.17587000131607056\n","[Training Epoch 99] Batch 713, Loss 0.17192207276821136\n","[Training Epoch 99] Batch 714, Loss 0.16919225454330444\n","[Training Epoch 99] Batch 715, Loss 0.1535278558731079\n","[Training Epoch 99] Batch 716, Loss 0.1534053087234497\n","[Training Epoch 99] Batch 717, Loss 0.1777627319097519\n","[Training Epoch 99] Batch 718, Loss 0.1697559654712677\n","[Training Epoch 99] Batch 719, Loss 0.17171865701675415\n","[Training Epoch 99] Batch 720, Loss 0.15982051193714142\n","[Training Epoch 99] Batch 721, Loss 0.14046788215637207\n","[Training Epoch 99] Batch 722, Loss 0.17132285237312317\n","[Training Epoch 99] Batch 723, Loss 0.1820022314786911\n","[Training Epoch 99] Batch 724, Loss 0.17699527740478516\n","[Training Epoch 99] Batch 725, Loss 0.15119576454162598\n","[Training Epoch 99] Batch 726, Loss 0.1531817615032196\n","[Training Epoch 99] Batch 727, Loss 0.15477696061134338\n","[Training Epoch 99] Batch 728, Loss 0.17170849442481995\n","[Training Epoch 99] Batch 729, Loss 0.1805056631565094\n","[Training Epoch 99] Batch 730, Loss 0.16658146679401398\n","[Training Epoch 99] Batch 731, Loss 0.15770600736141205\n","[Training Epoch 99] Batch 732, Loss 0.15530487895011902\n","[Training Epoch 99] Batch 733, Loss 0.15656426548957825\n","[Training Epoch 99] Batch 734, Loss 0.1717265248298645\n","[Training Epoch 99] Batch 735, Loss 0.1807279735803604\n","[Training Epoch 99] Batch 736, Loss 0.16576778888702393\n","[Training Epoch 99] Batch 737, Loss 0.17861026525497437\n","[Training Epoch 99] Batch 738, Loss 0.14312681555747986\n","[Training Epoch 99] Batch 739, Loss 0.16635078191757202\n","[Training Epoch 99] Batch 740, Loss 0.16408029198646545\n","[Training Epoch 99] Batch 741, Loss 0.15981870889663696\n","[Training Epoch 99] Batch 742, Loss 0.16529136896133423\n","[Training Epoch 99] Batch 743, Loss 0.1764838844537735\n","[Training Epoch 99] Batch 744, Loss 0.15886466205120087\n","[Training Epoch 99] Batch 745, Loss 0.15925732254981995\n","[Training Epoch 99] Batch 746, Loss 0.1684848964214325\n","[Training Epoch 99] Batch 747, Loss 0.14182209968566895\n","[Training Epoch 99] Batch 748, Loss 0.17786091566085815\n","[Training Epoch 99] Batch 749, Loss 0.16412632167339325\n","[Training Epoch 99] Batch 750, Loss 0.18448367714881897\n","[Training Epoch 99] Batch 751, Loss 0.17579719424247742\n","[Training Epoch 99] Batch 752, Loss 0.18864478170871735\n","[Training Epoch 99] Batch 753, Loss 0.1743173897266388\n","[Training Epoch 99] Batch 754, Loss 0.17194083333015442\n","[Training Epoch 99] Batch 755, Loss 0.17012175917625427\n","[Training Epoch 99] Batch 756, Loss 0.16260044276714325\n","[Training Epoch 99] Batch 757, Loss 0.15631060302257538\n","[Training Epoch 99] Batch 758, Loss 0.16184180974960327\n","[Training Epoch 99] Batch 759, Loss 0.18421146273612976\n","[Training Epoch 99] Batch 760, Loss 0.13875460624694824\n","[Training Epoch 99] Batch 761, Loss 0.15259701013565063\n","[Training Epoch 99] Batch 762, Loss 0.1653202921152115\n","[Training Epoch 99] Batch 763, Loss 0.16027869284152985\n","[Training Epoch 99] Batch 764, Loss 0.18251103162765503\n","[Training Epoch 99] Batch 765, Loss 0.1529994010925293\n","[Training Epoch 99] Batch 766, Loss 0.15904642641544342\n","[Training Epoch 99] Batch 767, Loss 0.17780020833015442\n","[Training Epoch 99] Batch 768, Loss 0.14001746475696564\n","[Training Epoch 99] Batch 769, Loss 0.17922300100326538\n","[Training Epoch 99] Batch 770, Loss 0.169866681098938\n","[Training Epoch 99] Batch 771, Loss 0.16045191884040833\n","[Training Epoch 99] Batch 772, Loss 0.20166337490081787\n","[Training Epoch 99] Batch 773, Loss 0.17671386897563934\n","[Training Epoch 99] Batch 774, Loss 0.17620083689689636\n","[Training Epoch 99] Batch 775, Loss 0.18304601311683655\n","[Training Epoch 99] Batch 776, Loss 0.15146663784980774\n","[Training Epoch 99] Batch 777, Loss 0.16996130347251892\n","[Training Epoch 99] Batch 778, Loss 0.16108404099941254\n","[Training Epoch 99] Batch 779, Loss 0.18391655385494232\n","[Training Epoch 99] Batch 780, Loss 0.15718631446361542\n","[Training Epoch 99] Batch 781, Loss 0.1737084984779358\n","[Training Epoch 99] Batch 782, Loss 0.1347050666809082\n","[Training Epoch 99] Batch 783, Loss 0.17490385472774506\n","[Training Epoch 99] Batch 784, Loss 0.1867872178554535\n","[Training Epoch 99] Batch 785, Loss 0.16129633784294128\n","[Training Epoch 99] Batch 786, Loss 0.15201608836650848\n","[Training Epoch 99] Batch 787, Loss 0.17586584389209747\n","[Training Epoch 99] Batch 788, Loss 0.16501279175281525\n","[Training Epoch 99] Batch 789, Loss 0.1572178304195404\n","[Training Epoch 99] Batch 790, Loss 0.17364159226417542\n","[Training Epoch 99] Batch 791, Loss 0.18437427282333374\n","[Training Epoch 99] Batch 792, Loss 0.14863693714141846\n","[Training Epoch 99] Batch 793, Loss 0.17736589908599854\n","[Training Epoch 99] Batch 794, Loss 0.17240817844867706\n","[Training Epoch 99] Batch 795, Loss 0.16761426627635956\n","[Training Epoch 99] Batch 796, Loss 0.17708483338356018\n","[Training Epoch 99] Batch 797, Loss 0.14523041248321533\n","[Training Epoch 99] Batch 798, Loss 0.17387691140174866\n","[Training Epoch 99] Batch 799, Loss 0.17772820591926575\n","[Training Epoch 99] Batch 800, Loss 0.20484226942062378\n","[Training Epoch 99] Batch 801, Loss 0.16426260769367218\n","[Training Epoch 99] Batch 802, Loss 0.15934807062149048\n","[Training Epoch 99] Batch 803, Loss 0.1556713879108429\n","[Training Epoch 99] Batch 804, Loss 0.16126839816570282\n","[Training Epoch 99] Batch 805, Loss 0.15586617588996887\n","[Training Epoch 99] Batch 806, Loss 0.19358378648757935\n","[Training Epoch 99] Batch 807, Loss 0.1666986048221588\n","[Training Epoch 99] Batch 808, Loss 0.17419180274009705\n","[Training Epoch 99] Batch 809, Loss 0.17371723055839539\n","[Training Epoch 99] Batch 810, Loss 0.16668710112571716\n","[Training Epoch 99] Batch 811, Loss 0.18490545451641083\n","[Training Epoch 99] Batch 812, Loss 0.17438116669654846\n","[Training Epoch 99] Batch 813, Loss 0.17128749191761017\n","[Training Epoch 99] Batch 814, Loss 0.1761787235736847\n","[Training Epoch 99] Batch 815, Loss 0.17337781190872192\n","[Training Epoch 99] Batch 816, Loss 0.17324203252792358\n","[Training Epoch 99] Batch 817, Loss 0.17508026957511902\n","[Training Epoch 99] Batch 818, Loss 0.19008150696754456\n","[Training Epoch 99] Batch 819, Loss 0.19765907526016235\n","[Training Epoch 99] Batch 820, Loss 0.16129070520401\n","[Training Epoch 99] Batch 821, Loss 0.18105167150497437\n","[Training Epoch 99] Batch 822, Loss 0.1568247228860855\n","[Training Epoch 99] Batch 823, Loss 0.15850111842155457\n","[Training Epoch 99] Batch 824, Loss 0.15794706344604492\n","[Training Epoch 99] Batch 825, Loss 0.15135914087295532\n","[Training Epoch 99] Batch 826, Loss 0.17290155589580536\n","[Training Epoch 99] Batch 827, Loss 0.15462911128997803\n","[Training Epoch 99] Batch 828, Loss 0.18329185247421265\n","[Training Epoch 99] Batch 829, Loss 0.1628129482269287\n","[Training Epoch 99] Batch 830, Loss 0.1564614623785019\n","[Training Epoch 99] Batch 831, Loss 0.17484715580940247\n","[Training Epoch 99] Batch 832, Loss 0.15940742194652557\n","[Training Epoch 99] Batch 833, Loss 0.15008729696273804\n","[Training Epoch 99] Batch 834, Loss 0.18033790588378906\n","[Training Epoch 99] Batch 835, Loss 0.18591079115867615\n","[Training Epoch 99] Batch 836, Loss 0.16979438066482544\n","[Training Epoch 99] Batch 837, Loss 0.14420852065086365\n","[Training Epoch 99] Batch 838, Loss 0.17939651012420654\n","[Training Epoch 99] Batch 839, Loss 0.17042583227157593\n","[Training Epoch 99] Batch 840, Loss 0.1743057370185852\n","[Training Epoch 99] Batch 841, Loss 0.19275949895381927\n","[Training Epoch 99] Batch 842, Loss 0.1610751748085022\n","[Training Epoch 99] Batch 843, Loss 0.18162013590335846\n","[Training Epoch 99] Batch 844, Loss 0.15215857326984406\n","[Training Epoch 99] Batch 845, Loss 0.1650826781988144\n","[Training Epoch 99] Batch 846, Loss 0.19501012563705444\n","[Training Epoch 99] Batch 847, Loss 0.17055442929267883\n","[Training Epoch 99] Batch 848, Loss 0.15504586696624756\n","[Training Epoch 99] Batch 849, Loss 0.19831636548042297\n","[Training Epoch 99] Batch 850, Loss 0.16088128089904785\n","[Training Epoch 99] Batch 851, Loss 0.15692031383514404\n","[Training Epoch 99] Batch 852, Loss 0.17936314642429352\n","[Training Epoch 99] Batch 853, Loss 0.17046889662742615\n","[Training Epoch 99] Batch 854, Loss 0.16199642419815063\n","[Training Epoch 99] Batch 855, Loss 0.14796701073646545\n","[Training Epoch 99] Batch 856, Loss 0.17117920517921448\n","[Training Epoch 99] Batch 857, Loss 0.18003974854946136\n","[Training Epoch 99] Batch 858, Loss 0.1423908919095993\n","[Training Epoch 99] Batch 859, Loss 0.1680029034614563\n","[Training Epoch 99] Batch 860, Loss 0.16763010621070862\n","[Training Epoch 99] Batch 861, Loss 0.1554941087961197\n","[Training Epoch 99] Batch 862, Loss 0.15230631828308105\n","[Training Epoch 99] Batch 863, Loss 0.15424439311027527\n","[Training Epoch 99] Batch 864, Loss 0.16474305093288422\n","[Training Epoch 99] Batch 865, Loss 0.1634731888771057\n","[Training Epoch 99] Batch 866, Loss 0.17223531007766724\n","[Training Epoch 99] Batch 867, Loss 0.156540647149086\n","[Training Epoch 99] Batch 868, Loss 0.17249153554439545\n","[Training Epoch 99] Batch 869, Loss 0.16072064638137817\n","[Training Epoch 99] Batch 870, Loss 0.15914729237556458\n","[Training Epoch 99] Batch 871, Loss 0.1663709431886673\n","[Training Epoch 99] Batch 872, Loss 0.16079428791999817\n","[Training Epoch 99] Batch 873, Loss 0.15175500512123108\n","[Training Epoch 99] Batch 874, Loss 0.1723821461200714\n","[Training Epoch 99] Batch 875, Loss 0.1490994095802307\n","[Training Epoch 99] Batch 876, Loss 0.14799317717552185\n","[Training Epoch 99] Batch 877, Loss 0.16349568963050842\n","[Training Epoch 99] Batch 878, Loss 0.14303052425384521\n","[Training Epoch 99] Batch 879, Loss 0.16709914803504944\n","[Training Epoch 99] Batch 880, Loss 0.1459820419549942\n","[Training Epoch 99] Batch 881, Loss 0.15488791465759277\n","[Training Epoch 99] Batch 882, Loss 0.17611469328403473\n","[Training Epoch 99] Batch 883, Loss 0.1447879523038864\n","[Training Epoch 99] Batch 884, Loss 0.15821781754493713\n","[Training Epoch 99] Batch 885, Loss 0.15004795789718628\n","[Training Epoch 99] Batch 886, Loss 0.16762953996658325\n","[Training Epoch 99] Batch 887, Loss 0.17203962802886963\n","[Training Epoch 99] Batch 888, Loss 0.1573534458875656\n","[Training Epoch 99] Batch 889, Loss 0.16147005558013916\n","[Training Epoch 99] Batch 890, Loss 0.1565496325492859\n","[Training Epoch 99] Batch 891, Loss 0.14348480105400085\n","[Training Epoch 99] Batch 892, Loss 0.15984803438186646\n","[Training Epoch 99] Batch 893, Loss 0.16365209221839905\n","[Training Epoch 99] Batch 894, Loss 0.1527034193277359\n","[Training Epoch 99] Batch 895, Loss 0.15819478034973145\n","[Training Epoch 99] Batch 896, Loss 0.16625460982322693\n","[Training Epoch 99] Batch 897, Loss 0.1552337408065796\n","[Training Epoch 99] Batch 898, Loss 0.18173670768737793\n","[Training Epoch 99] Batch 899, Loss 0.15405221283435822\n","[Training Epoch 99] Batch 900, Loss 0.17980051040649414\n","[Training Epoch 99] Batch 901, Loss 0.1841721385717392\n","[Training Epoch 99] Batch 902, Loss 0.13469454646110535\n","[Training Epoch 99] Batch 903, Loss 0.16468659043312073\n","[Training Epoch 99] Batch 904, Loss 0.18155765533447266\n","[Training Epoch 99] Batch 905, Loss 0.15788279473781586\n","[Training Epoch 99] Batch 906, Loss 0.16416576504707336\n","[Training Epoch 99] Batch 907, Loss 0.16246916353702545\n","[Training Epoch 99] Batch 908, Loss 0.19332355260849\n","[Training Epoch 99] Batch 909, Loss 0.1532459259033203\n","[Training Epoch 99] Batch 910, Loss 0.17401322722434998\n","[Training Epoch 99] Batch 911, Loss 0.175253227353096\n","[Training Epoch 99] Batch 912, Loss 0.16752171516418457\n","[Training Epoch 99] Batch 913, Loss 0.18040074408054352\n","[Training Epoch 99] Batch 914, Loss 0.15952232480049133\n","[Training Epoch 99] Batch 915, Loss 0.1618848443031311\n","[Training Epoch 99] Batch 916, Loss 0.16477590799331665\n","[Training Epoch 99] Batch 917, Loss 0.1627184897661209\n","[Training Epoch 99] Batch 918, Loss 0.18029049038887024\n","[Training Epoch 99] Batch 919, Loss 0.17719614505767822\n","[Training Epoch 99] Batch 920, Loss 0.17287155985832214\n","[Training Epoch 99] Batch 921, Loss 0.1635075807571411\n","[Training Epoch 99] Batch 922, Loss 0.1685323864221573\n","[Training Epoch 99] Batch 923, Loss 0.1733275204896927\n","[Training Epoch 99] Batch 924, Loss 0.17262032628059387\n","[Training Epoch 99] Batch 925, Loss 0.13423246145248413\n","[Training Epoch 99] Batch 926, Loss 0.19117014110088348\n","[Training Epoch 99] Batch 927, Loss 0.18107523024082184\n","[Training Epoch 99] Batch 928, Loss 0.15349552035331726\n","[Training Epoch 99] Batch 929, Loss 0.17120997607707977\n","[Training Epoch 99] Batch 930, Loss 0.156736820936203\n","[Training Epoch 99] Batch 931, Loss 0.19657278060913086\n","[Training Epoch 99] Batch 932, Loss 0.1598363071680069\n","[Training Epoch 99] Batch 933, Loss 0.17621302604675293\n","[Training Epoch 99] Batch 934, Loss 0.1568838655948639\n","[Training Epoch 99] Batch 935, Loss 0.1498410701751709\n","[Training Epoch 99] Batch 936, Loss 0.17044717073440552\n","[Training Epoch 99] Batch 937, Loss 0.18911013007164001\n","[Training Epoch 99] Batch 938, Loss 0.17055004835128784\n","[Training Epoch 99] Batch 939, Loss 0.15924589335918427\n","[Training Epoch 99] Batch 940, Loss 0.1738550364971161\n","[Training Epoch 99] Batch 941, Loss 0.1570316106081009\n","[Training Epoch 99] Batch 942, Loss 0.14128363132476807\n","[Training Epoch 99] Batch 943, Loss 0.15670070052146912\n","[Training Epoch 99] Batch 944, Loss 0.181014746427536\n","[Training Epoch 99] Batch 945, Loss 0.15753298997879028\n","[Training Epoch 99] Batch 946, Loss 0.16174781322479248\n","[Training Epoch 99] Batch 947, Loss 0.18260282278060913\n","[Training Epoch 99] Batch 948, Loss 0.15628796815872192\n","[Training Epoch 99] Batch 949, Loss 0.17170670628547668\n","[Training Epoch 99] Batch 950, Loss 0.16346433758735657\n","[Training Epoch 99] Batch 951, Loss 0.16364365816116333\n","[Training Epoch 99] Batch 952, Loss 0.1684390902519226\n","[Training Epoch 99] Batch 953, Loss 0.18651652336120605\n","[Training Epoch 99] Batch 954, Loss 0.14671391248703003\n","[Training Epoch 99] Batch 955, Loss 0.16547797620296478\n","[Training Epoch 99] Batch 956, Loss 0.14896656572818756\n","[Training Epoch 99] Batch 957, Loss 0.14825642108917236\n","[Training Epoch 99] Batch 958, Loss 0.1755075752735138\n","[Training Epoch 99] Batch 959, Loss 0.1517951786518097\n","[Training Epoch 99] Batch 960, Loss 0.16385893523693085\n","[Training Epoch 99] Batch 961, Loss 0.1877134144306183\n","[Training Epoch 99] Batch 962, Loss 0.1671677529811859\n","[Training Epoch 99] Batch 963, Loss 0.16865122318267822\n","[Training Epoch 99] Batch 964, Loss 0.1523110717535019\n","[Training Epoch 99] Batch 965, Loss 0.17899492383003235\n","[Training Epoch 99] Batch 966, Loss 0.18286490440368652\n","[Training Epoch 99] Batch 967, Loss 0.18355612456798553\n","[Training Epoch 99] Batch 968, Loss 0.16374710202217102\n","[Training Epoch 99] Batch 969, Loss 0.16576111316680908\n","[Training Epoch 99] Batch 970, Loss 0.1534089595079422\n","[Training Epoch 99] Batch 971, Loss 0.1819908320903778\n","[Training Epoch 99] Batch 972, Loss 0.15804210305213928\n","[Training Epoch 99] Batch 973, Loss 0.1639253795146942\n","[Training Epoch 99] Batch 974, Loss 0.1518113613128662\n","[Training Epoch 99] Batch 975, Loss 0.17767080664634705\n","[Training Epoch 99] Batch 976, Loss 0.14938536286354065\n","[Training Epoch 99] Batch 977, Loss 0.17548510432243347\n","[Training Epoch 99] Batch 978, Loss 0.16126814484596252\n","[Training Epoch 99] Batch 979, Loss 0.15969330072402954\n","[Training Epoch 99] Batch 980, Loss 0.18529289960861206\n","[Training Epoch 99] Batch 981, Loss 0.14176784455776215\n","[Training Epoch 99] Batch 982, Loss 0.146581768989563\n","[Training Epoch 99] Batch 983, Loss 0.17808528244495392\n","[Training Epoch 99] Batch 984, Loss 0.17222687602043152\n","[Training Epoch 99] Batch 985, Loss 0.175846666097641\n","[Training Epoch 99] Batch 986, Loss 0.18338771164417267\n","[Training Epoch 99] Batch 987, Loss 0.16602028906345367\n","[Training Epoch 99] Batch 988, Loss 0.16521818935871124\n","[Training Epoch 99] Batch 989, Loss 0.16851122677326202\n","[Training Epoch 99] Batch 990, Loss 0.15861274302005768\n","[Training Epoch 99] Batch 991, Loss 0.1546231508255005\n","[Training Epoch 99] Batch 992, Loss 0.17182347178459167\n","[Training Epoch 99] Batch 993, Loss 0.17510853707790375\n","[Training Epoch 99] Batch 994, Loss 0.16479477286338806\n","[Training Epoch 99] Batch 995, Loss 0.18857219815254211\n","[Training Epoch 99] Batch 996, Loss 0.17853429913520813\n","[Training Epoch 99] Batch 997, Loss 0.17351755499839783\n","[Training Epoch 99] Batch 998, Loss 0.18309995532035828\n","[Training Epoch 99] Batch 999, Loss 0.18102304637432098\n","[Training Epoch 99] Batch 1000, Loss 0.16324862837791443\n","[Training Epoch 99] Batch 1001, Loss 0.16839495301246643\n","[Training Epoch 99] Batch 1002, Loss 0.15980766713619232\n","[Training Epoch 99] Batch 1003, Loss 0.16225698590278625\n","[Training Epoch 99] Batch 1004, Loss 0.1591918170452118\n","[Training Epoch 99] Batch 1005, Loss 0.1727236807346344\n","[Training Epoch 99] Batch 1006, Loss 0.16144868731498718\n","[Training Epoch 99] Batch 1007, Loss 0.18115483224391937\n","[Training Epoch 99] Batch 1008, Loss 0.1828836351633072\n","[Training Epoch 99] Batch 1009, Loss 0.16921427845954895\n","[Training Epoch 99] Batch 1010, Loss 0.17385588586330414\n","[Training Epoch 99] Batch 1011, Loss 0.16553643345832825\n","[Training Epoch 99] Batch 1012, Loss 0.17118513584136963\n","[Training Epoch 99] Batch 1013, Loss 0.16495922207832336\n","[Training Epoch 99] Batch 1014, Loss 0.16174258291721344\n","[Training Epoch 99] Batch 1015, Loss 0.17844289541244507\n","[Training Epoch 99] Batch 1016, Loss 0.18206462264060974\n","[Training Epoch 99] Batch 1017, Loss 0.13795283436775208\n","[Training Epoch 99] Batch 1018, Loss 0.16916222870349884\n","[Training Epoch 99] Batch 1019, Loss 0.1706085205078125\n","[Training Epoch 99] Batch 1020, Loss 0.16528433561325073\n","[Training Epoch 99] Batch 1021, Loss 0.18206176161766052\n","[Training Epoch 99] Batch 1022, Loss 0.18696582317352295\n","[Training Epoch 99] Batch 1023, Loss 0.20024411380290985\n","[Training Epoch 99] Batch 1024, Loss 0.16021236777305603\n","[Training Epoch 99] Batch 1025, Loss 0.16197890043258667\n","[Training Epoch 99] Batch 1026, Loss 0.1579267829656601\n","[Training Epoch 99] Batch 1027, Loss 0.1692885309457779\n","[Training Epoch 99] Batch 1028, Loss 0.1557151973247528\n","[Training Epoch 99] Batch 1029, Loss 0.17244577407836914\n","[Training Epoch 99] Batch 1030, Loss 0.1719285100698471\n","[Training Epoch 99] Batch 1031, Loss 0.162289097905159\n","[Training Epoch 99] Batch 1032, Loss 0.18141260743141174\n","[Training Epoch 99] Batch 1033, Loss 0.16689985990524292\n","[Training Epoch 99] Batch 1034, Loss 0.17622782289981842\n","[Training Epoch 99] Batch 1035, Loss 0.17820492386817932\n","[Training Epoch 99] Batch 1036, Loss 0.1899203360080719\n","[Training Epoch 99] Batch 1037, Loss 0.16207846999168396\n","[Training Epoch 99] Batch 1038, Loss 0.16173356771469116\n","[Training Epoch 99] Batch 1039, Loss 0.1534007340669632\n","[Training Epoch 99] Batch 1040, Loss 0.15580300986766815\n","[Training Epoch 99] Batch 1041, Loss 0.16088856756687164\n","[Training Epoch 99] Batch 1042, Loss 0.16093340516090393\n","[Training Epoch 99] Batch 1043, Loss 0.1749967336654663\n","[Training Epoch 99] Batch 1044, Loss 0.20439261198043823\n","[Training Epoch 99] Batch 1045, Loss 0.17684638500213623\n","[Training Epoch 99] Batch 1046, Loss 0.1619226038455963\n","[Training Epoch 99] Batch 1047, Loss 0.17051741480827332\n","[Training Epoch 99] Batch 1048, Loss 0.16244709491729736\n","[Training Epoch 99] Batch 1049, Loss 0.13426297903060913\n","[Training Epoch 99] Batch 1050, Loss 0.16496405005455017\n","[Training Epoch 99] Batch 1051, Loss 0.18738201260566711\n","[Training Epoch 99] Batch 1052, Loss 0.21282190084457397\n","[Training Epoch 99] Batch 1053, Loss 0.1775101125240326\n","[Training Epoch 99] Batch 1054, Loss 0.17616471648216248\n","[Training Epoch 99] Batch 1055, Loss 0.1910882294178009\n","[Training Epoch 99] Batch 1056, Loss 0.1782563477754593\n","[Training Epoch 99] Batch 1057, Loss 0.1888422966003418\n","[Training Epoch 99] Batch 1058, Loss 0.18137803673744202\n","[Training Epoch 99] Batch 1059, Loss 0.15773384273052216\n","[Training Epoch 99] Batch 1060, Loss 0.18056567013263702\n","[Training Epoch 99] Batch 1061, Loss 0.1888611912727356\n","[Training Epoch 99] Batch 1062, Loss 0.18504875898361206\n","[Training Epoch 99] Batch 1063, Loss 0.15234366059303284\n","[Training Epoch 99] Batch 1064, Loss 0.13912075757980347\n","[Training Epoch 99] Batch 1065, Loss 0.17999060451984406\n","[Training Epoch 99] Batch 1066, Loss 0.16483020782470703\n","[Training Epoch 99] Batch 1067, Loss 0.15085244178771973\n","[Training Epoch 99] Batch 1068, Loss 0.17927342653274536\n","[Training Epoch 99] Batch 1069, Loss 0.1431819498538971\n","[Training Epoch 99] Batch 1070, Loss 0.16905808448791504\n","[Training Epoch 99] Batch 1071, Loss 0.18089427053928375\n","[Training Epoch 99] Batch 1072, Loss 0.16658857464790344\n","[Training Epoch 99] Batch 1073, Loss 0.16750448942184448\n","[Training Epoch 99] Batch 1074, Loss 0.17814232409000397\n","[Training Epoch 99] Batch 1075, Loss 0.16886702179908752\n","[Training Epoch 99] Batch 1076, Loss 0.14863114058971405\n","[Training Epoch 99] Batch 1077, Loss 0.18544258177280426\n","[Training Epoch 99] Batch 1078, Loss 0.1662890911102295\n","[Training Epoch 99] Batch 1079, Loss 0.16171245276927948\n","[Training Epoch 99] Batch 1080, Loss 0.17252475023269653\n","[Training Epoch 99] Batch 1081, Loss 0.16827675700187683\n","[Training Epoch 99] Batch 1082, Loss 0.1726667582988739\n","[Training Epoch 99] Batch 1083, Loss 0.16775760054588318\n","[Training Epoch 99] Batch 1084, Loss 0.1862429827451706\n","[Training Epoch 99] Batch 1085, Loss 0.1755799800157547\n","[Training Epoch 99] Batch 1086, Loss 0.1664276123046875\n","[Training Epoch 99] Batch 1087, Loss 0.17346370220184326\n","[Training Epoch 99] Batch 1088, Loss 0.1733582615852356\n","[Training Epoch 99] Batch 1089, Loss 0.16996727883815765\n","[Training Epoch 99] Batch 1090, Loss 0.16716542840003967\n","[Training Epoch 99] Batch 1091, Loss 0.18203501403331757\n","[Training Epoch 99] Batch 1092, Loss 0.17679864168167114\n","[Training Epoch 99] Batch 1093, Loss 0.14575332403182983\n","[Training Epoch 99] Batch 1094, Loss 0.16679957509040833\n","[Training Epoch 99] Batch 1095, Loss 0.1677199900150299\n","[Training Epoch 99] Batch 1096, Loss 0.19074329733848572\n","[Training Epoch 99] Batch 1097, Loss 0.15588155388832092\n","[Training Epoch 99] Batch 1098, Loss 0.19459404051303864\n","[Training Epoch 99] Batch 1099, Loss 0.1810886710882187\n","[Training Epoch 99] Batch 1100, Loss 0.15698859095573425\n","[Training Epoch 99] Batch 1101, Loss 0.15541818737983704\n","[Training Epoch 99] Batch 1102, Loss 0.1802114099264145\n","[Training Epoch 99] Batch 1103, Loss 0.17166157066822052\n","[Training Epoch 99] Batch 1104, Loss 0.1490502655506134\n","[Training Epoch 99] Batch 1105, Loss 0.1698712408542633\n","[Training Epoch 99] Batch 1106, Loss 0.17002013325691223\n","[Training Epoch 99] Batch 1107, Loss 0.16108062863349915\n","[Training Epoch 99] Batch 1108, Loss 0.1647552251815796\n","[Training Epoch 99] Batch 1109, Loss 0.16507959365844727\n","[Training Epoch 99] Batch 1110, Loss 0.14152219891548157\n","[Training Epoch 99] Batch 1111, Loss 0.17522060871124268\n","[Training Epoch 99] Batch 1112, Loss 0.15760448575019836\n","[Training Epoch 99] Batch 1113, Loss 0.1995905488729477\n","[Training Epoch 99] Batch 1114, Loss 0.17089928686618805\n","[Training Epoch 99] Batch 1115, Loss 0.1786375343799591\n","[Training Epoch 99] Batch 1116, Loss 0.16004598140716553\n","[Training Epoch 99] Batch 1117, Loss 0.16727709770202637\n","[Training Epoch 99] Batch 1118, Loss 0.17476463317871094\n","[Training Epoch 99] Batch 1119, Loss 0.16963231563568115\n","[Training Epoch 99] Batch 1120, Loss 0.16694200038909912\n","[Training Epoch 99] Batch 1121, Loss 0.1839650422334671\n","[Training Epoch 99] Batch 1122, Loss 0.15009397268295288\n","[Training Epoch 99] Batch 1123, Loss 0.16501069068908691\n","[Training Epoch 99] Batch 1124, Loss 0.14856015145778656\n","[Training Epoch 99] Batch 1125, Loss 0.17688682675361633\n","[Training Epoch 99] Batch 1126, Loss 0.15680983662605286\n","[Training Epoch 99] Batch 1127, Loss 0.18029183149337769\n","[Training Epoch 99] Batch 1128, Loss 0.14715754985809326\n","[Training Epoch 99] Batch 1129, Loss 0.17034144699573517\n","[Training Epoch 99] Batch 1130, Loss 0.16426466405391693\n","[Training Epoch 99] Batch 1131, Loss 0.16239748895168304\n","[Training Epoch 99] Batch 1132, Loss 0.18280532956123352\n","[Training Epoch 99] Batch 1133, Loss 0.1878405064344406\n","[Training Epoch 99] Batch 1134, Loss 0.14585010707378387\n","[Training Epoch 99] Batch 1135, Loss 0.1456221491098404\n","[Training Epoch 99] Batch 1136, Loss 0.19590982794761658\n","[Training Epoch 99] Batch 1137, Loss 0.16441935300827026\n","[Training Epoch 99] Batch 1138, Loss 0.138863205909729\n","[Training Epoch 99] Batch 1139, Loss 0.17022845149040222\n","[Training Epoch 99] Batch 1140, Loss 0.191947340965271\n","[Training Epoch 99] Batch 1141, Loss 0.15436717867851257\n","[Training Epoch 99] Batch 1142, Loss 0.17817765474319458\n","[Training Epoch 99] Batch 1143, Loss 0.16314536333084106\n","[Training Epoch 99] Batch 1144, Loss 0.18913322687149048\n","[Training Epoch 99] Batch 1145, Loss 0.18163591623306274\n","[Training Epoch 99] Batch 1146, Loss 0.16441462934017181\n","[Training Epoch 99] Batch 1147, Loss 0.17811885476112366\n","[Training Epoch 99] Batch 1148, Loss 0.15069255232810974\n","[Training Epoch 99] Batch 1149, Loss 0.16270974278450012\n","[Training Epoch 99] Batch 1150, Loss 0.16703088581562042\n","[Training Epoch 99] Batch 1151, Loss 0.19539500772953033\n","[Training Epoch 99] Batch 1152, Loss 0.17278096079826355\n","[Training Epoch 99] Batch 1153, Loss 0.16328191757202148\n","[Training Epoch 99] Batch 1154, Loss 0.17834173142910004\n","[Training Epoch 99] Batch 1155, Loss 0.1374545693397522\n","[Training Epoch 99] Batch 1156, Loss 0.1672368049621582\n","[Training Epoch 99] Batch 1157, Loss 0.17222973704338074\n","[Training Epoch 99] Batch 1158, Loss 0.15433046221733093\n","[Training Epoch 99] Batch 1159, Loss 0.15522977709770203\n","[Training Epoch 99] Batch 1160, Loss 0.17438173294067383\n","/content/drive/MyDrive/Neural-CF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 99] HR = 0.7227, NDCG = 0.3547\n","Epoch 100 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 100] Batch 0, Loss 0.1709848940372467\n","[Training Epoch 100] Batch 1, Loss 0.15282055735588074\n","[Training Epoch 100] Batch 2, Loss 0.15112072229385376\n","[Training Epoch 100] Batch 3, Loss 0.15772847831249237\n","[Training Epoch 100] Batch 4, Loss 0.17206886410713196\n","[Training Epoch 100] Batch 5, Loss 0.17178875207901\n","[Training Epoch 100] Batch 6, Loss 0.1429908573627472\n","[Training Epoch 100] Batch 7, Loss 0.17008893191814423\n","[Training Epoch 100] Batch 8, Loss 0.16092287003993988\n","[Training Epoch 100] Batch 9, Loss 0.18138426542282104\n","[Training Epoch 100] Batch 10, Loss 0.16460809111595154\n","[Training Epoch 100] Batch 11, Loss 0.17073914408683777\n","[Training Epoch 100] Batch 12, Loss 0.1666090488433838\n","[Training Epoch 100] Batch 13, Loss 0.1508072465658188\n","[Training Epoch 100] Batch 14, Loss 0.16332703828811646\n","[Training Epoch 100] Batch 15, Loss 0.16491635143756866\n","[Training Epoch 100] Batch 16, Loss 0.17323386669158936\n","[Training Epoch 100] Batch 17, Loss 0.15186823904514313\n","[Training Epoch 100] Batch 18, Loss 0.15621578693389893\n","[Training Epoch 100] Batch 19, Loss 0.16997286677360535\n","[Training Epoch 100] Batch 20, Loss 0.15112867951393127\n","[Training Epoch 100] Batch 21, Loss 0.14519988000392914\n","[Training Epoch 100] Batch 22, Loss 0.1778433471918106\n","[Training Epoch 100] Batch 23, Loss 0.17648378014564514\n","[Training Epoch 100] Batch 24, Loss 0.1713384985923767\n","[Training Epoch 100] Batch 25, Loss 0.14527128636837006\n","[Training Epoch 100] Batch 26, Loss 0.15680015087127686\n","[Training Epoch 100] Batch 27, Loss 0.16144463419914246\n","[Training Epoch 100] Batch 28, Loss 0.1600152552127838\n","[Training Epoch 100] Batch 29, Loss 0.16654987633228302\n","[Training Epoch 100] Batch 30, Loss 0.15236887335777283\n","[Training Epoch 100] Batch 31, Loss 0.17153629660606384\n","[Training Epoch 100] Batch 32, Loss 0.14844219386577606\n","[Training Epoch 100] Batch 33, Loss 0.1559249311685562\n","[Training Epoch 100] Batch 34, Loss 0.15995071828365326\n","[Training Epoch 100] Batch 35, Loss 0.16969668865203857\n","[Training Epoch 100] Batch 36, Loss 0.13856899738311768\n","[Training Epoch 100] Batch 37, Loss 0.16984057426452637\n","[Training Epoch 100] Batch 38, Loss 0.136760413646698\n","[Training Epoch 100] Batch 39, Loss 0.15565316379070282\n","[Training Epoch 100] Batch 40, Loss 0.16742539405822754\n","[Training Epoch 100] Batch 41, Loss 0.16355004906654358\n","[Training Epoch 100] Batch 42, Loss 0.15874968469142914\n","[Training Epoch 100] Batch 43, Loss 0.15786103904247284\n","[Training Epoch 100] Batch 44, Loss 0.1658860743045807\n","[Training Epoch 100] Batch 45, Loss 0.145718514919281\n","[Training Epoch 100] Batch 46, Loss 0.1728062629699707\n","[Training Epoch 100] Batch 47, Loss 0.16579127311706543\n","[Training Epoch 100] Batch 48, Loss 0.1800871640443802\n","[Training Epoch 100] Batch 49, Loss 0.16635668277740479\n","[Training Epoch 100] Batch 50, Loss 0.1443314552307129\n","[Training Epoch 100] Batch 51, Loss 0.17125244438648224\n","[Training Epoch 100] Batch 52, Loss 0.17050084471702576\n","[Training Epoch 100] Batch 53, Loss 0.1618257313966751\n","[Training Epoch 100] Batch 54, Loss 0.1549285352230072\n","[Training Epoch 100] Batch 55, Loss 0.15829534828662872\n","[Training Epoch 100] Batch 56, Loss 0.162857323884964\n","[Training Epoch 100] Batch 57, Loss 0.16493850946426392\n","[Training Epoch 100] Batch 58, Loss 0.16895511746406555\n","[Training Epoch 100] Batch 59, Loss 0.15155598521232605\n","[Training Epoch 100] Batch 60, Loss 0.17547282576560974\n","[Training Epoch 100] Batch 61, Loss 0.16635489463806152\n","[Training Epoch 100] Batch 62, Loss 0.1736839860677719\n","[Training Epoch 100] Batch 63, Loss 0.15540482103824615\n","[Training Epoch 100] Batch 64, Loss 0.164994016289711\n","[Training Epoch 100] Batch 65, Loss 0.1671677529811859\n","[Training Epoch 100] Batch 66, Loss 0.170558363199234\n","[Training Epoch 100] Batch 67, Loss 0.1533917784690857\n","[Training Epoch 100] Batch 68, Loss 0.15596067905426025\n","[Training Epoch 100] Batch 69, Loss 0.17013204097747803\n","[Training Epoch 100] Batch 70, Loss 0.15837141871452332\n","[Training Epoch 100] Batch 71, Loss 0.15929585695266724\n","[Training Epoch 100] Batch 72, Loss 0.15545085072517395\n","[Training Epoch 100] Batch 73, Loss 0.15789738297462463\n","[Training Epoch 100] Batch 74, Loss 0.1525341272354126\n","[Training Epoch 100] Batch 75, Loss 0.16838344931602478\n","[Training Epoch 100] Batch 76, Loss 0.1549234837293625\n","[Training Epoch 100] Batch 77, Loss 0.15537601709365845\n","[Training Epoch 100] Batch 78, Loss 0.15805107355117798\n","[Training Epoch 100] Batch 79, Loss 0.15629664063453674\n","[Training Epoch 100] Batch 80, Loss 0.1593492329120636\n","[Training Epoch 100] Batch 81, Loss 0.18748435378074646\n","[Training Epoch 100] Batch 82, Loss 0.16104722023010254\n","[Training Epoch 100] Batch 83, Loss 0.1566222757101059\n","[Training Epoch 100] Batch 84, Loss 0.16078223288059235\n","[Training Epoch 100] Batch 85, Loss 0.17580382525920868\n","[Training Epoch 100] Batch 86, Loss 0.14149153232574463\n","[Training Epoch 100] Batch 87, Loss 0.15635648369789124\n","[Training Epoch 100] Batch 88, Loss 0.1729588508605957\n","[Training Epoch 100] Batch 89, Loss 0.14151179790496826\n","[Training Epoch 100] Batch 90, Loss 0.1666344255208969\n","[Training Epoch 100] Batch 91, Loss 0.16947045922279358\n","[Training Epoch 100] Batch 92, Loss 0.15964606404304504\n","[Training Epoch 100] Batch 93, Loss 0.15656661987304688\n","[Training Epoch 100] Batch 94, Loss 0.1724151074886322\n","[Training Epoch 100] Batch 95, Loss 0.1936648190021515\n","[Training Epoch 100] Batch 96, Loss 0.16647715866565704\n","[Training Epoch 100] Batch 97, Loss 0.1640278398990631\n","[Training Epoch 100] Batch 98, Loss 0.13737383484840393\n","[Training Epoch 100] Batch 99, Loss 0.15137621760368347\n","[Training Epoch 100] Batch 100, Loss 0.16001877188682556\n","[Training Epoch 100] Batch 101, Loss 0.1608537882566452\n","[Training Epoch 100] Batch 102, Loss 0.1548595428466797\n","[Training Epoch 100] Batch 103, Loss 0.15277734398841858\n","[Training Epoch 100] Batch 104, Loss 0.16819098591804504\n","[Training Epoch 100] Batch 105, Loss 0.14643985033035278\n","[Training Epoch 100] Batch 106, Loss 0.16266120970249176\n","[Training Epoch 100] Batch 107, Loss 0.1741223931312561\n","[Training Epoch 100] Batch 108, Loss 0.15966108441352844\n","[Training Epoch 100] Batch 109, Loss 0.1581292599439621\n","[Training Epoch 100] Batch 110, Loss 0.18305045366287231\n","[Training Epoch 100] Batch 111, Loss 0.1700364202260971\n","[Training Epoch 100] Batch 112, Loss 0.15947507321834564\n","[Training Epoch 100] Batch 113, Loss 0.1618303656578064\n","[Training Epoch 100] Batch 114, Loss 0.1567504107952118\n","[Training Epoch 100] Batch 115, Loss 0.19089406728744507\n","[Training Epoch 100] Batch 116, Loss 0.16791020333766937\n","[Training Epoch 100] Batch 117, Loss 0.14637091755867004\n","[Training Epoch 100] Batch 118, Loss 0.15817780792713165\n","[Training Epoch 100] Batch 119, Loss 0.1693250983953476\n","[Training Epoch 100] Batch 120, Loss 0.17056143283843994\n","[Training Epoch 100] Batch 121, Loss 0.18872663378715515\n","[Training Epoch 100] Batch 122, Loss 0.16084842383861542\n","[Training Epoch 100] Batch 123, Loss 0.18917080760002136\n","[Training Epoch 100] Batch 124, Loss 0.14092020690441132\n","[Training Epoch 100] Batch 125, Loss 0.16461682319641113\n","[Training Epoch 100] Batch 126, Loss 0.155234694480896\n","[Training Epoch 100] Batch 127, Loss 0.15433311462402344\n","[Training Epoch 100] Batch 128, Loss 0.13634571433067322\n","[Training Epoch 100] Batch 129, Loss 0.17132402956485748\n","[Training Epoch 100] Batch 130, Loss 0.18023884296417236\n","[Training Epoch 100] Batch 131, Loss 0.14882400631904602\n","[Training Epoch 100] Batch 132, Loss 0.1869862973690033\n","[Training Epoch 100] Batch 133, Loss 0.16283823549747467\n","[Training Epoch 100] Batch 134, Loss 0.1516016721725464\n","[Training Epoch 100] Batch 135, Loss 0.16549117863178253\n","[Training Epoch 100] Batch 136, Loss 0.14481408894062042\n","[Training Epoch 100] Batch 137, Loss 0.17600201070308685\n","[Training Epoch 100] Batch 138, Loss 0.16246911883354187\n","[Training Epoch 100] Batch 139, Loss 0.14093470573425293\n","[Training Epoch 100] Batch 140, Loss 0.15827813744544983\n","[Training Epoch 100] Batch 141, Loss 0.17496155202388763\n","[Training Epoch 100] Batch 142, Loss 0.15911254286766052\n","[Training Epoch 100] Batch 143, Loss 0.1792404055595398\n","[Training Epoch 100] Batch 144, Loss 0.15992656350135803\n","[Training Epoch 100] Batch 145, Loss 0.1369732916355133\n","[Training Epoch 100] Batch 146, Loss 0.174017071723938\n","[Training Epoch 100] Batch 147, Loss 0.1590377241373062\n","[Training Epoch 100] Batch 148, Loss 0.16870544850826263\n","[Training Epoch 100] Batch 149, Loss 0.1731378734111786\n","[Training Epoch 100] Batch 150, Loss 0.14802420139312744\n","[Training Epoch 100] Batch 151, Loss 0.18581806123256683\n","[Training Epoch 100] Batch 152, Loss 0.19084355235099792\n","[Training Epoch 100] Batch 153, Loss 0.1612139344215393\n","[Training Epoch 100] Batch 154, Loss 0.15192857384681702\n","[Training Epoch 100] Batch 155, Loss 0.18444454669952393\n","[Training Epoch 100] Batch 156, Loss 0.19225922226905823\n","[Training Epoch 100] Batch 157, Loss 0.1633124053478241\n","[Training Epoch 100] Batch 158, Loss 0.15278321504592896\n","[Training Epoch 100] Batch 159, Loss 0.1661319136619568\n","[Training Epoch 100] Batch 160, Loss 0.15818752348423004\n","[Training Epoch 100] Batch 161, Loss 0.15660357475280762\n","[Training Epoch 100] Batch 162, Loss 0.16570326685905457\n","[Training Epoch 100] Batch 163, Loss 0.16517174243927002\n","[Training Epoch 100] Batch 164, Loss 0.1608976125717163\n","[Training Epoch 100] Batch 165, Loss 0.14716406166553497\n","[Training Epoch 100] Batch 166, Loss 0.17256084084510803\n","[Training Epoch 100] Batch 167, Loss 0.1534048616886139\n","[Training Epoch 100] Batch 168, Loss 0.16252103447914124\n","[Training Epoch 100] Batch 169, Loss 0.1727270483970642\n","[Training Epoch 100] Batch 170, Loss 0.15143822133541107\n","[Training Epoch 100] Batch 171, Loss 0.1802026480436325\n","[Training Epoch 100] Batch 172, Loss 0.16870921850204468\n","[Training Epoch 100] Batch 173, Loss 0.16052654385566711\n","[Training Epoch 100] Batch 174, Loss 0.15979376435279846\n","[Training Epoch 100] Batch 175, Loss 0.1636655330657959\n","[Training Epoch 100] Batch 176, Loss 0.1676424890756607\n","[Training Epoch 100] Batch 177, Loss 0.15582548081874847\n","[Training Epoch 100] Batch 178, Loss 0.170053631067276\n","[Training Epoch 100] Batch 179, Loss 0.1820138692855835\n","[Training Epoch 100] Batch 180, Loss 0.16851310431957245\n","[Training Epoch 100] Batch 181, Loss 0.1752113550901413\n","[Training Epoch 100] Batch 182, Loss 0.1630176603794098\n","[Training Epoch 100] Batch 183, Loss 0.18368801474571228\n","[Training Epoch 100] Batch 184, Loss 0.15976494550704956\n","[Training Epoch 100] Batch 185, Loss 0.16314099729061127\n","[Training Epoch 100] Batch 186, Loss 0.16749097406864166\n","[Training Epoch 100] Batch 187, Loss 0.15730531513690948\n","[Training Epoch 100] Batch 188, Loss 0.17586182057857513\n","[Training Epoch 100] Batch 189, Loss 0.1372830718755722\n","[Training Epoch 100] Batch 190, Loss 0.1675240397453308\n","[Training Epoch 100] Batch 191, Loss 0.1528279185295105\n","[Training Epoch 100] Batch 192, Loss 0.17097623646259308\n","[Training Epoch 100] Batch 193, Loss 0.1780649572610855\n","[Training Epoch 100] Batch 194, Loss 0.14316964149475098\n","[Training Epoch 100] Batch 195, Loss 0.17713448405265808\n","[Training Epoch 100] Batch 196, Loss 0.1773972511291504\n","[Training Epoch 100] Batch 197, Loss 0.14064879715442657\n","[Training Epoch 100] Batch 198, Loss 0.15924301743507385\n","[Training Epoch 100] Batch 199, Loss 0.15639638900756836\n","[Training Epoch 100] Batch 200, Loss 0.16708384454250336\n","[Training Epoch 100] Batch 201, Loss 0.19168561697006226\n","[Training Epoch 100] Batch 202, Loss 0.14731180667877197\n","[Training Epoch 100] Batch 203, Loss 0.1466728150844574\n","[Training Epoch 100] Batch 204, Loss 0.1576092392206192\n","[Training Epoch 100] Batch 205, Loss 0.17580965161323547\n","[Training Epoch 100] Batch 206, Loss 0.15613408386707306\n","[Training Epoch 100] Batch 207, Loss 0.18090593814849854\n","[Training Epoch 100] Batch 208, Loss 0.15494278073310852\n","[Training Epoch 100] Batch 209, Loss 0.17170372605323792\n","[Training Epoch 100] Batch 210, Loss 0.15803682804107666\n","[Training Epoch 100] Batch 211, Loss 0.16863472759723663\n","[Training Epoch 100] Batch 212, Loss 0.1671508252620697\n","[Training Epoch 100] Batch 213, Loss 0.1570737659931183\n","[Training Epoch 100] Batch 214, Loss 0.16848792135715485\n","[Training Epoch 100] Batch 215, Loss 0.18978144228458405\n","[Training Epoch 100] Batch 216, Loss 0.1419329047203064\n","[Training Epoch 100] Batch 217, Loss 0.16881877183914185\n","[Training Epoch 100] Batch 218, Loss 0.16272203624248505\n","[Training Epoch 100] Batch 219, Loss 0.15662488341331482\n","[Training Epoch 100] Batch 220, Loss 0.18453702330589294\n","[Training Epoch 100] Batch 221, Loss 0.1711033284664154\n","[Training Epoch 100] Batch 222, Loss 0.15640223026275635\n","[Training Epoch 100] Batch 223, Loss 0.16408351063728333\n","[Training Epoch 100] Batch 224, Loss 0.17461860179901123\n","[Training Epoch 100] Batch 225, Loss 0.14707522094249725\n","[Training Epoch 100] Batch 226, Loss 0.18065863847732544\n","[Training Epoch 100] Batch 227, Loss 0.15449519455432892\n","[Training Epoch 100] Batch 228, Loss 0.15495477616786957\n","[Training Epoch 100] Batch 229, Loss 0.15697656571865082\n","[Training Epoch 100] Batch 230, Loss 0.16338038444519043\n","[Training Epoch 100] Batch 231, Loss 0.1748112291097641\n","[Training Epoch 100] Batch 232, Loss 0.1457219272851944\n","[Training Epoch 100] Batch 233, Loss 0.14632946252822876\n","[Training Epoch 100] Batch 234, Loss 0.139975905418396\n","[Training Epoch 100] Batch 235, Loss 0.13695783913135529\n","[Training Epoch 100] Batch 236, Loss 0.15482717752456665\n","[Training Epoch 100] Batch 237, Loss 0.14789143204689026\n","[Training Epoch 100] Batch 238, Loss 0.159851536154747\n","[Training Epoch 100] Batch 239, Loss 0.1787915676832199\n","[Training Epoch 100] Batch 240, Loss 0.14874565601348877\n","[Training Epoch 100] Batch 241, Loss 0.18692608177661896\n","[Training Epoch 100] Batch 242, Loss 0.15579083561897278\n","[Training Epoch 100] Batch 243, Loss 0.1558849811553955\n","[Training Epoch 100] Batch 244, Loss 0.16942459344863892\n","[Training Epoch 100] Batch 245, Loss 0.16699540615081787\n","[Training Epoch 100] Batch 246, Loss 0.17840838432312012\n","[Training Epoch 100] Batch 247, Loss 0.15773209929466248\n","[Training Epoch 100] Batch 248, Loss 0.16742923855781555\n","[Training Epoch 100] Batch 249, Loss 0.14077317714691162\n","[Training Epoch 100] Batch 250, Loss 0.16877424716949463\n","[Training Epoch 100] Batch 251, Loss 0.15307678282260895\n","[Training Epoch 100] Batch 252, Loss 0.16537630558013916\n","[Training Epoch 100] Batch 253, Loss 0.15714533627033234\n","[Training Epoch 100] Batch 254, Loss 0.16483068466186523\n","[Training Epoch 100] Batch 255, Loss 0.17792318761348724\n","[Training Epoch 100] Batch 256, Loss 0.13418245315551758\n","[Training Epoch 100] Batch 257, Loss 0.17715632915496826\n","[Training Epoch 100] Batch 258, Loss 0.15372328460216522\n","[Training Epoch 100] Batch 259, Loss 0.14678055047988892\n","[Training Epoch 100] Batch 260, Loss 0.1797364503145218\n","[Training Epoch 100] Batch 261, Loss 0.16268998384475708\n","[Training Epoch 100] Batch 262, Loss 0.14122635126113892\n","[Training Epoch 100] Batch 263, Loss 0.14500977098941803\n","[Training Epoch 100] Batch 264, Loss 0.15574820339679718\n","[Training Epoch 100] Batch 265, Loss 0.1666707545518875\n","[Training Epoch 100] Batch 266, Loss 0.15571951866149902\n","[Training Epoch 100] Batch 267, Loss 0.18416368961334229\n","[Training Epoch 100] Batch 268, Loss 0.15123125910758972\n","[Training Epoch 100] Batch 269, Loss 0.1751096546649933\n","[Training Epoch 100] Batch 270, Loss 0.17785175144672394\n","[Training Epoch 100] Batch 271, Loss 0.1535709798336029\n","[Training Epoch 100] Batch 272, Loss 0.18937227129936218\n","[Training Epoch 100] Batch 273, Loss 0.1594216525554657\n","[Training Epoch 100] Batch 274, Loss 0.15485289692878723\n","[Training Epoch 100] Batch 275, Loss 0.19988873600959778\n","[Training Epoch 100] Batch 276, Loss 0.1745668351650238\n","[Training Epoch 100] Batch 277, Loss 0.16006925702095032\n","[Training Epoch 100] Batch 278, Loss 0.16726461052894592\n","[Training Epoch 100] Batch 279, Loss 0.1612960696220398\n","[Training Epoch 100] Batch 280, Loss 0.1681823879480362\n","[Training Epoch 100] Batch 281, Loss 0.17308247089385986\n","[Training Epoch 100] Batch 282, Loss 0.15096421539783478\n","[Training Epoch 100] Batch 283, Loss 0.15447184443473816\n","[Training Epoch 100] Batch 284, Loss 0.17185300588607788\n","[Training Epoch 100] Batch 285, Loss 0.18636268377304077\n","[Training Epoch 100] Batch 286, Loss 0.18575310707092285\n","[Training Epoch 100] Batch 287, Loss 0.1684681475162506\n","[Training Epoch 100] Batch 288, Loss 0.1726861596107483\n","[Training Epoch 100] Batch 289, Loss 0.14875516295433044\n","[Training Epoch 100] Batch 290, Loss 0.1606452465057373\n","[Training Epoch 100] Batch 291, Loss 0.14970801770687103\n","[Training Epoch 100] Batch 292, Loss 0.17513839900493622\n","[Training Epoch 100] Batch 293, Loss 0.16182821989059448\n","[Training Epoch 100] Batch 294, Loss 0.14114366471767426\n","[Training Epoch 100] Batch 295, Loss 0.17171183228492737\n","[Training Epoch 100] Batch 296, Loss 0.16709592938423157\n","[Training Epoch 100] Batch 297, Loss 0.16223230957984924\n","[Training Epoch 100] Batch 298, Loss 0.14953842759132385\n","[Training Epoch 100] Batch 299, Loss 0.14422106742858887\n","[Training Epoch 100] Batch 300, Loss 0.14860795438289642\n","[Training Epoch 100] Batch 301, Loss 0.16798877716064453\n","[Training Epoch 100] Batch 302, Loss 0.16538593173027039\n","[Training Epoch 100] Batch 303, Loss 0.17061187326908112\n","[Training Epoch 100] Batch 304, Loss 0.15672872960567474\n","[Training Epoch 100] Batch 305, Loss 0.18617092072963715\n","[Training Epoch 100] Batch 306, Loss 0.15090349316596985\n","[Training Epoch 100] Batch 307, Loss 0.163557231426239\n","[Training Epoch 100] Batch 308, Loss 0.15714877843856812\n","[Training Epoch 100] Batch 309, Loss 0.16517747938632965\n","[Training Epoch 100] Batch 310, Loss 0.16213072836399078\n","[Training Epoch 100] Batch 311, Loss 0.16297586262226105\n","[Training Epoch 100] Batch 312, Loss 0.14698448777198792\n","[Training Epoch 100] Batch 313, Loss 0.16566585004329681\n","[Training Epoch 100] Batch 314, Loss 0.15941676497459412\n","[Training Epoch 100] Batch 315, Loss 0.15858396887779236\n","[Training Epoch 100] Batch 316, Loss 0.16688348352909088\n","[Training Epoch 100] Batch 317, Loss 0.1661226749420166\n","[Training Epoch 100] Batch 318, Loss 0.15897032618522644\n","[Training Epoch 100] Batch 319, Loss 0.16537967324256897\n","[Training Epoch 100] Batch 320, Loss 0.14831554889678955\n","[Training Epoch 100] Batch 321, Loss 0.17576220631599426\n","[Training Epoch 100] Batch 322, Loss 0.1459992229938507\n","[Training Epoch 100] Batch 323, Loss 0.15865537524223328\n","[Training Epoch 100] Batch 324, Loss 0.1627001017332077\n","[Training Epoch 100] Batch 325, Loss 0.18019074201583862\n","[Training Epoch 100] Batch 326, Loss 0.14974690973758698\n","[Training Epoch 100] Batch 327, Loss 0.1562337577342987\n","[Training Epoch 100] Batch 328, Loss 0.16584959626197815\n","[Training Epoch 100] Batch 329, Loss 0.15993033349514008\n","[Training Epoch 100] Batch 330, Loss 0.1619100421667099\n","[Training Epoch 100] Batch 331, Loss 0.19682647287845612\n","[Training Epoch 100] Batch 332, Loss 0.14648103713989258\n","[Training Epoch 100] Batch 333, Loss 0.1654011309146881\n","[Training Epoch 100] Batch 334, Loss 0.17119106650352478\n","[Training Epoch 100] Batch 335, Loss 0.18577595055103302\n","[Training Epoch 100] Batch 336, Loss 0.18351437151432037\n","[Training Epoch 100] Batch 337, Loss 0.17475968599319458\n","[Training Epoch 100] Batch 338, Loss 0.16068634390830994\n","[Training Epoch 100] Batch 339, Loss 0.15526103973388672\n","[Training Epoch 100] Batch 340, Loss 0.1661393642425537\n","[Training Epoch 100] Batch 341, Loss 0.17721699178218842\n","[Training Epoch 100] Batch 342, Loss 0.1710895299911499\n","[Training Epoch 100] Batch 343, Loss 0.1567680388689041\n","[Training Epoch 100] Batch 344, Loss 0.1645132303237915\n","[Training Epoch 100] Batch 345, Loss 0.16763994097709656\n","[Training Epoch 100] Batch 346, Loss 0.17744854092597961\n","[Training Epoch 100] Batch 347, Loss 0.1499793827533722\n","[Training Epoch 100] Batch 348, Loss 0.17968738079071045\n","[Training Epoch 100] Batch 349, Loss 0.1659565269947052\n","[Training Epoch 100] Batch 350, Loss 0.16997134685516357\n","[Training Epoch 100] Batch 351, Loss 0.16959388554096222\n","[Training Epoch 100] Batch 352, Loss 0.1980132758617401\n","[Training Epoch 100] Batch 353, Loss 0.16129852831363678\n","[Training Epoch 100] Batch 354, Loss 0.1748662292957306\n","[Training Epoch 100] Batch 355, Loss 0.1703299731016159\n","[Training Epoch 100] Batch 356, Loss 0.16109895706176758\n","[Training Epoch 100] Batch 357, Loss 0.16210100054740906\n","[Training Epoch 100] Batch 358, Loss 0.1569320410490036\n","[Training Epoch 100] Batch 359, Loss 0.157670259475708\n","[Training Epoch 100] Batch 360, Loss 0.167071133852005\n","[Training Epoch 100] Batch 361, Loss 0.1728740930557251\n","[Training Epoch 100] Batch 362, Loss 0.17053213715553284\n","[Training Epoch 100] Batch 363, Loss 0.1734580248594284\n","[Training Epoch 100] Batch 364, Loss 0.1728905439376831\n","[Training Epoch 100] Batch 365, Loss 0.18010377883911133\n","[Training Epoch 100] Batch 366, Loss 0.16096895933151245\n","[Training Epoch 100] Batch 367, Loss 0.1588829755783081\n","[Training Epoch 100] Batch 368, Loss 0.17842119932174683\n","[Training Epoch 100] Batch 369, Loss 0.19741502404212952\n","[Training Epoch 100] Batch 370, Loss 0.1621440052986145\n","[Training Epoch 100] Batch 371, Loss 0.1847430318593979\n","[Training Epoch 100] Batch 372, Loss 0.150385320186615\n","[Training Epoch 100] Batch 373, Loss 0.14552149176597595\n","[Training Epoch 100] Batch 374, Loss 0.15820619463920593\n","[Training Epoch 100] Batch 375, Loss 0.14688509702682495\n","[Training Epoch 100] Batch 376, Loss 0.17217203974723816\n","[Training Epoch 100] Batch 377, Loss 0.1461201310157776\n","[Training Epoch 100] Batch 378, Loss 0.1607562154531479\n","[Training Epoch 100] Batch 379, Loss 0.1763763129711151\n","[Training Epoch 100] Batch 380, Loss 0.1512938141822815\n","[Training Epoch 100] Batch 381, Loss 0.1597093641757965\n","[Training Epoch 100] Batch 382, Loss 0.15680113434791565\n","[Training Epoch 100] Batch 383, Loss 0.1664159893989563\n","[Training Epoch 100] Batch 384, Loss 0.16497960686683655\n","[Training Epoch 100] Batch 385, Loss 0.17627418041229248\n","[Training Epoch 100] Batch 386, Loss 0.1711191087961197\n","[Training Epoch 100] Batch 387, Loss 0.14230939745903015\n","[Training Epoch 100] Batch 388, Loss 0.18230599164962769\n","[Training Epoch 100] Batch 389, Loss 0.17726437747478485\n","[Training Epoch 100] Batch 390, Loss 0.170155331492424\n","[Training Epoch 100] Batch 391, Loss 0.14685580134391785\n","[Training Epoch 100] Batch 392, Loss 0.16818590462207794\n","[Training Epoch 100] Batch 393, Loss 0.17670799791812897\n","[Training Epoch 100] Batch 394, Loss 0.17537462711334229\n","[Training Epoch 100] Batch 395, Loss 0.17149925231933594\n","[Training Epoch 100] Batch 396, Loss 0.1747833639383316\n","[Training Epoch 100] Batch 397, Loss 0.1786731332540512\n","[Training Epoch 100] Batch 398, Loss 0.18554964661598206\n","[Training Epoch 100] Batch 399, Loss 0.17049165070056915\n","[Training Epoch 100] Batch 400, Loss 0.1681654304265976\n","[Training Epoch 100] Batch 401, Loss 0.1742832511663437\n","[Training Epoch 100] Batch 402, Loss 0.1682334691286087\n","[Training Epoch 100] Batch 403, Loss 0.17218607664108276\n","[Training Epoch 100] Batch 404, Loss 0.16151943802833557\n","[Training Epoch 100] Batch 405, Loss 0.15808871388435364\n","[Training Epoch 100] Batch 406, Loss 0.1611185520887375\n","[Training Epoch 100] Batch 407, Loss 0.17217811942100525\n","[Training Epoch 100] Batch 408, Loss 0.1507280170917511\n","[Training Epoch 100] Batch 409, Loss 0.1648995280265808\n","[Training Epoch 100] Batch 410, Loss 0.16846150159835815\n","[Training Epoch 100] Batch 411, Loss 0.14689582586288452\n","[Training Epoch 100] Batch 412, Loss 0.16063085198402405\n","[Training Epoch 100] Batch 413, Loss 0.16458870470523834\n","[Training Epoch 100] Batch 414, Loss 0.172917902469635\n","[Training Epoch 100] Batch 415, Loss 0.16163969039916992\n","[Training Epoch 100] Batch 416, Loss 0.1776309311389923\n","[Training Epoch 100] Batch 417, Loss 0.1564190685749054\n","[Training Epoch 100] Batch 418, Loss 0.15745282173156738\n","[Training Epoch 100] Batch 419, Loss 0.14846813678741455\n","[Training Epoch 100] Batch 420, Loss 0.15604518353939056\n","[Training Epoch 100] Batch 421, Loss 0.18055184185504913\n","[Training Epoch 100] Batch 422, Loss 0.16096314787864685\n","[Training Epoch 100] Batch 423, Loss 0.1801818609237671\n","[Training Epoch 100] Batch 424, Loss 0.1642804741859436\n","[Training Epoch 100] Batch 425, Loss 0.17866888642311096\n","[Training Epoch 100] Batch 426, Loss 0.14700856804847717\n","[Training Epoch 100] Batch 427, Loss 0.1557849645614624\n","[Training Epoch 100] Batch 428, Loss 0.1823505461215973\n","[Training Epoch 100] Batch 429, Loss 0.17827537655830383\n","[Training Epoch 100] Batch 430, Loss 0.1617637574672699\n","[Training Epoch 100] Batch 431, Loss 0.19122718274593353\n","[Training Epoch 100] Batch 432, Loss 0.16838639974594116\n","[Training Epoch 100] Batch 433, Loss 0.1522158980369568\n","[Training Epoch 100] Batch 434, Loss 0.1508786678314209\n","[Training Epoch 100] Batch 435, Loss 0.16356438398361206\n","[Training Epoch 100] Batch 436, Loss 0.17823484539985657\n","[Training Epoch 100] Batch 437, Loss 0.15564268827438354\n","[Training Epoch 100] Batch 438, Loss 0.16982781887054443\n","[Training Epoch 100] Batch 439, Loss 0.19964599609375\n","[Training Epoch 100] Batch 440, Loss 0.15119659900665283\n","[Training Epoch 100] Batch 441, Loss 0.16428470611572266\n","[Training Epoch 100] Batch 442, Loss 0.1506980061531067\n","[Training Epoch 100] Batch 443, Loss 0.19006067514419556\n","[Training Epoch 100] Batch 444, Loss 0.15580317378044128\n","[Training Epoch 100] Batch 445, Loss 0.1704069823026657\n","[Training Epoch 100] Batch 446, Loss 0.14484338462352753\n","[Training Epoch 100] Batch 447, Loss 0.15020786225795746\n","[Training Epoch 100] Batch 448, Loss 0.17359045147895813\n","[Training Epoch 100] Batch 449, Loss 0.17530205845832825\n","[Training Epoch 100] Batch 450, Loss 0.1959148347377777\n","[Training Epoch 100] Batch 451, Loss 0.16460412740707397\n","[Training Epoch 100] Batch 452, Loss 0.15039151906967163\n","[Training Epoch 100] Batch 453, Loss 0.1494383066892624\n","[Training Epoch 100] Batch 454, Loss 0.15482063591480255\n","[Training Epoch 100] Batch 455, Loss 0.17840790748596191\n","[Training Epoch 100] Batch 456, Loss 0.1473531574010849\n","[Training Epoch 100] Batch 457, Loss 0.16575336456298828\n","[Training Epoch 100] Batch 458, Loss 0.15657371282577515\n","[Training Epoch 100] Batch 459, Loss 0.14898794889450073\n","[Training Epoch 100] Batch 460, Loss 0.1660822629928589\n","[Training Epoch 100] Batch 461, Loss 0.13373404741287231\n","[Training Epoch 100] Batch 462, Loss 0.1680007129907608\n","[Training Epoch 100] Batch 463, Loss 0.17979833483695984\n","[Training Epoch 100] Batch 464, Loss 0.16050802171230316\n","[Training Epoch 100] Batch 465, Loss 0.18781748414039612\n","[Training Epoch 100] Batch 466, Loss 0.1970287561416626\n","[Training Epoch 100] Batch 467, Loss 0.14843453466892242\n","[Training Epoch 100] Batch 468, Loss 0.19629542529582977\n","[Training Epoch 100] Batch 469, Loss 0.15312695503234863\n","[Training Epoch 100] Batch 470, Loss 0.15847952663898468\n","[Training Epoch 100] Batch 471, Loss 0.16878437995910645\n","[Training Epoch 100] Batch 472, Loss 0.17036598920822144\n","[Training Epoch 100] Batch 473, Loss 0.16346348822116852\n","[Training Epoch 100] Batch 474, Loss 0.17668747901916504\n","[Training Epoch 100] Batch 475, Loss 0.15889525413513184\n","[Training Epoch 100] Batch 476, Loss 0.16085170209407806\n","[Training Epoch 100] Batch 477, Loss 0.16495074331760406\n","[Training Epoch 100] Batch 478, Loss 0.16371814906597137\n","[Training Epoch 100] Batch 479, Loss 0.16101206839084625\n","[Training Epoch 100] Batch 480, Loss 0.18667472898960114\n","[Training Epoch 100] Batch 481, Loss 0.17881964147090912\n","[Training Epoch 100] Batch 482, Loss 0.15461033582687378\n","[Training Epoch 100] Batch 483, Loss 0.14859607815742493\n","[Training Epoch 100] Batch 484, Loss 0.1725310981273651\n","[Training Epoch 100] Batch 485, Loss 0.16671480238437653\n","[Training Epoch 100] Batch 486, Loss 0.16725632548332214\n","[Training Epoch 100] Batch 487, Loss 0.1892053335905075\n","[Training Epoch 100] Batch 488, Loss 0.14161445200443268\n","[Training Epoch 100] Batch 489, Loss 0.16469155251979828\n","[Training Epoch 100] Batch 490, Loss 0.16567322611808777\n","[Training Epoch 100] Batch 491, Loss 0.18063236773014069\n","[Training Epoch 100] Batch 492, Loss 0.1612156629562378\n","[Training Epoch 100] Batch 493, Loss 0.15087568759918213\n","[Training Epoch 100] Batch 494, Loss 0.15196849405765533\n","[Training Epoch 100] Batch 495, Loss 0.16411560773849487\n","[Training Epoch 100] Batch 496, Loss 0.17399539053440094\n","[Training Epoch 100] Batch 497, Loss 0.17440959811210632\n","[Training Epoch 100] Batch 498, Loss 0.1591063141822815\n","[Training Epoch 100] Batch 499, Loss 0.1669953465461731\n","[Training Epoch 100] Batch 500, Loss 0.18322902917861938\n","[Training Epoch 100] Batch 501, Loss 0.17387792468070984\n","[Training Epoch 100] Batch 502, Loss 0.1708138883113861\n","[Training Epoch 100] Batch 503, Loss 0.162818044424057\n","[Training Epoch 100] Batch 504, Loss 0.17552050948143005\n","[Training Epoch 100] Batch 505, Loss 0.17074987292289734\n","[Training Epoch 100] Batch 506, Loss 0.16707049310207367\n","[Training Epoch 100] Batch 507, Loss 0.17452871799468994\n","[Training Epoch 100] Batch 508, Loss 0.16334451735019684\n","[Training Epoch 100] Batch 509, Loss 0.17743363976478577\n","[Training Epoch 100] Batch 510, Loss 0.1645088642835617\n","[Training Epoch 100] Batch 511, Loss 0.15411090850830078\n","[Training Epoch 100] Batch 512, Loss 0.15943267941474915\n","[Training Epoch 100] Batch 513, Loss 0.18118159472942352\n","[Training Epoch 100] Batch 514, Loss 0.14649629592895508\n","[Training Epoch 100] Batch 515, Loss 0.15972496569156647\n","[Training Epoch 100] Batch 516, Loss 0.17284291982650757\n","[Training Epoch 100] Batch 517, Loss 0.16050934791564941\n","[Training Epoch 100] Batch 518, Loss 0.16002973914146423\n","[Training Epoch 100] Batch 519, Loss 0.15225717425346375\n","[Training Epoch 100] Batch 520, Loss 0.1489759087562561\n","[Training Epoch 100] Batch 521, Loss 0.1702946126461029\n","[Training Epoch 100] Batch 522, Loss 0.15936961770057678\n","[Training Epoch 100] Batch 523, Loss 0.16497769951820374\n","[Training Epoch 100] Batch 524, Loss 0.15288159251213074\n","[Training Epoch 100] Batch 525, Loss 0.13247370719909668\n","[Training Epoch 100] Batch 526, Loss 0.1804199516773224\n","[Training Epoch 100] Batch 527, Loss 0.1654069423675537\n","[Training Epoch 100] Batch 528, Loss 0.17519794404506683\n","[Training Epoch 100] Batch 529, Loss 0.1596602499485016\n","[Training Epoch 100] Batch 530, Loss 0.1811475306749344\n","[Training Epoch 100] Batch 531, Loss 0.16729110479354858\n","[Training Epoch 100] Batch 532, Loss 0.16051894426345825\n","[Training Epoch 100] Batch 533, Loss 0.18247506022453308\n","[Training Epoch 100] Batch 534, Loss 0.15676921606063843\n","[Training Epoch 100] Batch 535, Loss 0.15054628252983093\n","[Training Epoch 100] Batch 536, Loss 0.1599067747592926\n","[Training Epoch 100] Batch 537, Loss 0.18801715970039368\n","[Training Epoch 100] Batch 538, Loss 0.1817699521780014\n","[Training Epoch 100] Batch 539, Loss 0.16331273317337036\n","[Training Epoch 100] Batch 540, Loss 0.1742919683456421\n","[Training Epoch 100] Batch 541, Loss 0.15835285186767578\n","[Training Epoch 100] Batch 542, Loss 0.16423636674880981\n","[Training Epoch 100] Batch 543, Loss 0.17041298747062683\n","[Training Epoch 100] Batch 544, Loss 0.15731467306613922\n","[Training Epoch 100] Batch 545, Loss 0.17324727773666382\n","[Training Epoch 100] Batch 546, Loss 0.17014551162719727\n","[Training Epoch 100] Batch 547, Loss 0.17003583908081055\n","[Training Epoch 100] Batch 548, Loss 0.17504483461380005\n","[Training Epoch 100] Batch 549, Loss 0.14402896165847778\n","[Training Epoch 100] Batch 550, Loss 0.16798517107963562\n","[Training Epoch 100] Batch 551, Loss 0.173641636967659\n","[Training Epoch 100] Batch 552, Loss 0.16053855419158936\n","[Training Epoch 100] Batch 553, Loss 0.1669105589389801\n","[Training Epoch 100] Batch 554, Loss 0.14198344945907593\n","[Training Epoch 100] Batch 555, Loss 0.16914552450180054\n","[Training Epoch 100] Batch 556, Loss 0.17753738164901733\n","[Training Epoch 100] Batch 557, Loss 0.1727343499660492\n","[Training Epoch 100] Batch 558, Loss 0.15387049317359924\n","[Training Epoch 100] Batch 559, Loss 0.17015527188777924\n","[Training Epoch 100] Batch 560, Loss 0.18291230499744415\n","[Training Epoch 100] Batch 561, Loss 0.16090616583824158\n","[Training Epoch 100] Batch 562, Loss 0.17992055416107178\n","[Training Epoch 100] Batch 563, Loss 0.1704319715499878\n","[Training Epoch 100] Batch 564, Loss 0.17400814592838287\n","[Training Epoch 100] Batch 565, Loss 0.16838273406028748\n","[Training Epoch 100] Batch 566, Loss 0.167018324136734\n","[Training Epoch 100] Batch 567, Loss 0.1639026701450348\n","[Training Epoch 100] Batch 568, Loss 0.1524428129196167\n","[Training Epoch 100] Batch 569, Loss 0.1653398871421814\n","[Training Epoch 100] Batch 570, Loss 0.181378573179245\n","[Training Epoch 100] Batch 571, Loss 0.17408707737922668\n","[Training Epoch 100] Batch 572, Loss 0.15902048349380493\n","[Training Epoch 100] Batch 573, Loss 0.16369563341140747\n","[Training Epoch 100] Batch 574, Loss 0.15621516108512878\n","[Training Epoch 100] Batch 575, Loss 0.1668161302804947\n","[Training Epoch 100] Batch 576, Loss 0.16778452694416046\n","[Training Epoch 100] Batch 577, Loss 0.1584169566631317\n","[Training Epoch 100] Batch 578, Loss 0.16610786318778992\n","[Training Epoch 100] Batch 579, Loss 0.1825367510318756\n","[Training Epoch 100] Batch 580, Loss 0.16847357153892517\n","[Training Epoch 100] Batch 581, Loss 0.16949430108070374\n","[Training Epoch 100] Batch 582, Loss 0.17351354658603668\n","[Training Epoch 100] Batch 583, Loss 0.15721207857131958\n","[Training Epoch 100] Batch 584, Loss 0.15796884894371033\n","[Training Epoch 100] Batch 585, Loss 0.16087567806243896\n","[Training Epoch 100] Batch 586, Loss 0.15543198585510254\n","[Training Epoch 100] Batch 587, Loss 0.1748284548521042\n","[Training Epoch 100] Batch 588, Loss 0.16121497750282288\n","[Training Epoch 100] Batch 589, Loss 0.1789996325969696\n","[Training Epoch 100] Batch 590, Loss 0.17224052548408508\n","[Training Epoch 100] Batch 591, Loss 0.1493830531835556\n","[Training Epoch 100] Batch 592, Loss 0.16034239530563354\n","[Training Epoch 100] Batch 593, Loss 0.16213321685791016\n","[Training Epoch 100] Batch 594, Loss 0.1567065417766571\n","[Training Epoch 100] Batch 595, Loss 0.1639414131641388\n","[Training Epoch 100] Batch 596, Loss 0.17808163166046143\n","[Training Epoch 100] Batch 597, Loss 0.14947593212127686\n","[Training Epoch 100] Batch 598, Loss 0.18581295013427734\n","[Training Epoch 100] Batch 599, Loss 0.1691393256187439\n","[Training Epoch 100] Batch 600, Loss 0.17506879568099976\n","[Training Epoch 100] Batch 601, Loss 0.1498604714870453\n","[Training Epoch 100] Batch 602, Loss 0.16083413362503052\n","[Training Epoch 100] Batch 603, Loss 0.16135983169078827\n","[Training Epoch 100] Batch 604, Loss 0.18673422932624817\n","[Training Epoch 100] Batch 605, Loss 0.1724376678466797\n","[Training Epoch 100] Batch 606, Loss 0.17461979389190674\n","[Training Epoch 100] Batch 607, Loss 0.15408623218536377\n","[Training Epoch 100] Batch 608, Loss 0.16615398228168488\n","[Training Epoch 100] Batch 609, Loss 0.16509947180747986\n","[Training Epoch 100] Batch 610, Loss 0.16023755073547363\n","[Training Epoch 100] Batch 611, Loss 0.1692938506603241\n","[Training Epoch 100] Batch 612, Loss 0.1858242005109787\n","[Training Epoch 100] Batch 613, Loss 0.15677109360694885\n","[Training Epoch 100] Batch 614, Loss 0.18470294773578644\n","[Training Epoch 100] Batch 615, Loss 0.15124230086803436\n","[Training Epoch 100] Batch 616, Loss 0.15131621062755585\n","[Training Epoch 100] Batch 617, Loss 0.17708483338356018\n","[Training Epoch 100] Batch 618, Loss 0.17860333621501923\n","[Training Epoch 100] Batch 619, Loss 0.16168317198753357\n","[Training Epoch 100] Batch 620, Loss 0.1797775775194168\n","[Training Epoch 100] Batch 621, Loss 0.17657378315925598\n","[Training Epoch 100] Batch 622, Loss 0.16166716814041138\n","[Training Epoch 100] Batch 623, Loss 0.15801455080509186\n","[Training Epoch 100] Batch 624, Loss 0.1569601595401764\n","[Training Epoch 100] Batch 625, Loss 0.15182068943977356\n","[Training Epoch 100] Batch 626, Loss 0.16945233941078186\n","[Training Epoch 100] Batch 627, Loss 0.15798541903495789\n","[Training Epoch 100] Batch 628, Loss 0.17755703628063202\n","[Training Epoch 100] Batch 629, Loss 0.17295777797698975\n","[Training Epoch 100] Batch 630, Loss 0.17600412666797638\n","[Training Epoch 100] Batch 631, Loss 0.17089007794857025\n","[Training Epoch 100] Batch 632, Loss 0.19119611382484436\n","[Training Epoch 100] Batch 633, Loss 0.16871094703674316\n","[Training Epoch 100] Batch 634, Loss 0.1467590034008026\n","[Training Epoch 100] Batch 635, Loss 0.17680911719799042\n","[Training Epoch 100] Batch 636, Loss 0.15643766522407532\n","[Training Epoch 100] Batch 637, Loss 0.18970057368278503\n","[Training Epoch 100] Batch 638, Loss 0.16015835106372833\n","[Training Epoch 100] Batch 639, Loss 0.16238942742347717\n","[Training Epoch 100] Batch 640, Loss 0.15900608897209167\n","[Training Epoch 100] Batch 641, Loss 0.1895633488893509\n","[Training Epoch 100] Batch 642, Loss 0.16501984000205994\n","[Training Epoch 100] Batch 643, Loss 0.1599857658147812\n","[Training Epoch 100] Batch 644, Loss 0.1739569902420044\n","[Training Epoch 100] Batch 645, Loss 0.15753625333309174\n","[Training Epoch 100] Batch 646, Loss 0.1729639619588852\n","[Training Epoch 100] Batch 647, Loss 0.1710289716720581\n","[Training Epoch 100] Batch 648, Loss 0.1504756212234497\n","[Training Epoch 100] Batch 649, Loss 0.18583928048610687\n","[Training Epoch 100] Batch 650, Loss 0.14009913802146912\n","[Training Epoch 100] Batch 651, Loss 0.14922428131103516\n","[Training Epoch 100] Batch 652, Loss 0.17900842428207397\n","[Training Epoch 100] Batch 653, Loss 0.16877791285514832\n","[Training Epoch 100] Batch 654, Loss 0.17830201983451843\n","[Training Epoch 100] Batch 655, Loss 0.15988019108772278\n","[Training Epoch 100] Batch 656, Loss 0.14596018195152283\n","[Training Epoch 100] Batch 657, Loss 0.20144590735435486\n","[Training Epoch 100] Batch 658, Loss 0.16691136360168457\n","[Training Epoch 100] Batch 659, Loss 0.17390528321266174\n","[Training Epoch 100] Batch 660, Loss 0.14627480506896973\n","[Training Epoch 100] Batch 661, Loss 0.17773568630218506\n","[Training Epoch 100] Batch 662, Loss 0.16053980588912964\n","[Training Epoch 100] Batch 663, Loss 0.17891162633895874\n","[Training Epoch 100] Batch 664, Loss 0.1413654237985611\n","[Training Epoch 100] Batch 665, Loss 0.13749882578849792\n","[Training Epoch 100] Batch 666, Loss 0.16897863149642944\n","[Training Epoch 100] Batch 667, Loss 0.1824568510055542\n","[Training Epoch 100] Batch 668, Loss 0.16810712218284607\n","[Training Epoch 100] Batch 669, Loss 0.16689810156822205\n","[Training Epoch 100] Batch 670, Loss 0.15314005315303802\n","[Training Epoch 100] Batch 671, Loss 0.18675382435321808\n","[Training Epoch 100] Batch 672, Loss 0.16714362800121307\n","[Training Epoch 100] Batch 673, Loss 0.15870040655136108\n","[Training Epoch 100] Batch 674, Loss 0.16991522908210754\n","[Training Epoch 100] Batch 675, Loss 0.16601048409938812\n","[Training Epoch 100] Batch 676, Loss 0.1540074497461319\n","[Training Epoch 100] Batch 677, Loss 0.15874835848808289\n","[Training Epoch 100] Batch 678, Loss 0.17517217993736267\n","[Training Epoch 100] Batch 679, Loss 0.15661931037902832\n","[Training Epoch 100] Batch 680, Loss 0.17814721167087555\n","[Training Epoch 100] Batch 681, Loss 0.16216060519218445\n","[Training Epoch 100] Batch 682, Loss 0.15398463606834412\n","[Training Epoch 100] Batch 683, Loss 0.1470118761062622\n","[Training Epoch 100] Batch 684, Loss 0.16400957107543945\n","[Training Epoch 100] Batch 685, Loss 0.17526596784591675\n","[Training Epoch 100] Batch 686, Loss 0.15123310685157776\n","[Training Epoch 100] Batch 687, Loss 0.16131611168384552\n","[Training Epoch 100] Batch 688, Loss 0.15659867227077484\n","[Training Epoch 100] Batch 689, Loss 0.1724315881729126\n","[Training Epoch 100] Batch 690, Loss 0.16734854876995087\n","[Training Epoch 100] Batch 691, Loss 0.1521330326795578\n","[Training Epoch 100] Batch 692, Loss 0.17035850882530212\n","[Training Epoch 100] Batch 693, Loss 0.17382849752902985\n","[Training Epoch 100] Batch 694, Loss 0.15374675393104553\n","[Training Epoch 100] Batch 695, Loss 0.17209261655807495\n","[Training Epoch 100] Batch 696, Loss 0.1839311569929123\n","[Training Epoch 100] Batch 697, Loss 0.1594727635383606\n","[Training Epoch 100] Batch 698, Loss 0.17792737483978271\n","[Training Epoch 100] Batch 699, Loss 0.15651483833789825\n","[Training Epoch 100] Batch 700, Loss 0.16709066927433014\n","[Training Epoch 100] Batch 701, Loss 0.1772117018699646\n","[Training Epoch 100] Batch 702, Loss 0.1576116979122162\n","[Training Epoch 100] Batch 703, Loss 0.1677083969116211\n","[Training Epoch 100] Batch 704, Loss 0.16254732012748718\n","[Training Epoch 100] Batch 705, Loss 0.1644197404384613\n","[Training Epoch 100] Batch 706, Loss 0.16579897701740265\n","[Training Epoch 100] Batch 707, Loss 0.1809847205877304\n","[Training Epoch 100] Batch 708, Loss 0.16342154145240784\n","[Training Epoch 100] Batch 709, Loss 0.13156826794147491\n","[Training Epoch 100] Batch 710, Loss 0.1558127999305725\n","[Training Epoch 100] Batch 711, Loss 0.16968435049057007\n","[Training Epoch 100] Batch 712, Loss 0.16925737261772156\n","[Training Epoch 100] Batch 713, Loss 0.15850891172885895\n","[Training Epoch 100] Batch 714, Loss 0.19277998805046082\n","[Training Epoch 100] Batch 715, Loss 0.17267704010009766\n","[Training Epoch 100] Batch 716, Loss 0.1754370480775833\n","[Training Epoch 100] Batch 717, Loss 0.16506478190422058\n","[Training Epoch 100] Batch 718, Loss 0.16758862137794495\n","[Training Epoch 100] Batch 719, Loss 0.17886430025100708\n","[Training Epoch 100] Batch 720, Loss 0.16717629134655\n","[Training Epoch 100] Batch 721, Loss 0.1822642982006073\n","[Training Epoch 100] Batch 722, Loss 0.17169679701328278\n","[Training Epoch 100] Batch 723, Loss 0.14935728907585144\n","[Training Epoch 100] Batch 724, Loss 0.14594265818595886\n","[Training Epoch 100] Batch 725, Loss 0.15915799140930176\n","[Training Epoch 100] Batch 726, Loss 0.152049258351326\n","[Training Epoch 100] Batch 727, Loss 0.1776447296142578\n","[Training Epoch 100] Batch 728, Loss 0.19436411559581757\n","[Training Epoch 100] Batch 729, Loss 0.1728324294090271\n","[Training Epoch 100] Batch 730, Loss 0.15605910122394562\n","[Training Epoch 100] Batch 731, Loss 0.15105745196342468\n","[Training Epoch 100] Batch 732, Loss 0.1573449969291687\n","[Training Epoch 100] Batch 733, Loss 0.17283186316490173\n","[Training Epoch 100] Batch 734, Loss 0.15299637615680695\n","[Training Epoch 100] Batch 735, Loss 0.17022916674613953\n","[Training Epoch 100] Batch 736, Loss 0.1678031086921692\n","[Training Epoch 100] Batch 737, Loss 0.18222767114639282\n","[Training Epoch 100] Batch 738, Loss 0.17855852842330933\n","[Training Epoch 100] Batch 739, Loss 0.1744861602783203\n","[Training Epoch 100] Batch 740, Loss 0.15456143021583557\n","[Training Epoch 100] Batch 741, Loss 0.16969048976898193\n","[Training Epoch 100] Batch 742, Loss 0.16628992557525635\n","[Training Epoch 100] Batch 743, Loss 0.14957469701766968\n","[Training Epoch 100] Batch 744, Loss 0.1556938886642456\n","[Training Epoch 100] Batch 745, Loss 0.1610146462917328\n","[Training Epoch 100] Batch 746, Loss 0.15911413729190826\n","[Training Epoch 100] Batch 747, Loss 0.14571943879127502\n","[Training Epoch 100] Batch 748, Loss 0.1990850269794464\n","[Training Epoch 100] Batch 749, Loss 0.14224299788475037\n","[Training Epoch 100] Batch 750, Loss 0.17972958087921143\n","[Training Epoch 100] Batch 751, Loss 0.16970817744731903\n","[Training Epoch 100] Batch 752, Loss 0.15653395652770996\n","[Training Epoch 100] Batch 753, Loss 0.16906175017356873\n","[Training Epoch 100] Batch 754, Loss 0.17272689938545227\n","[Training Epoch 100] Batch 755, Loss 0.14272403717041016\n","[Training Epoch 100] Batch 756, Loss 0.19518254697322845\n","[Training Epoch 100] Batch 757, Loss 0.187086284160614\n","[Training Epoch 100] Batch 758, Loss 0.1705615520477295\n","[Training Epoch 100] Batch 759, Loss 0.1702578365802765\n","[Training Epoch 100] Batch 760, Loss 0.16437950730323792\n","[Training Epoch 100] Batch 761, Loss 0.16117331385612488\n","[Training Epoch 100] Batch 762, Loss 0.16856804490089417\n","[Training Epoch 100] Batch 763, Loss 0.18320727348327637\n","[Training Epoch 100] Batch 764, Loss 0.14640295505523682\n","[Training Epoch 100] Batch 765, Loss 0.17450278997421265\n","[Training Epoch 100] Batch 766, Loss 0.16553457081317902\n","[Training Epoch 100] Batch 767, Loss 0.1822596788406372\n","[Training Epoch 100] Batch 768, Loss 0.18081839382648468\n","[Training Epoch 100] Batch 769, Loss 0.16480578482151031\n","[Training Epoch 100] Batch 770, Loss 0.18645763397216797\n","[Training Epoch 100] Batch 771, Loss 0.17143608629703522\n","[Training Epoch 100] Batch 772, Loss 0.1550188660621643\n","[Training Epoch 100] Batch 773, Loss 0.17305761575698853\n","[Training Epoch 100] Batch 774, Loss 0.16239388287067413\n","[Training Epoch 100] Batch 775, Loss 0.1492213010787964\n","[Training Epoch 100] Batch 776, Loss 0.15855823457241058\n","[Training Epoch 100] Batch 777, Loss 0.18216495215892792\n","[Training Epoch 100] Batch 778, Loss 0.177947536110878\n","[Training Epoch 100] Batch 779, Loss 0.18378552794456482\n","[Training Epoch 100] Batch 780, Loss 0.1783694624900818\n","[Training Epoch 100] Batch 781, Loss 0.17192867398262024\n","[Training Epoch 100] Batch 782, Loss 0.14946284890174866\n","[Training Epoch 100] Batch 783, Loss 0.14873164892196655\n","[Training Epoch 100] Batch 784, Loss 0.17539314925670624\n","[Training Epoch 100] Batch 785, Loss 0.16881057620048523\n","[Training Epoch 100] Batch 786, Loss 0.17857521772384644\n","[Training Epoch 100] Batch 787, Loss 0.1764025241136551\n","[Training Epoch 100] Batch 788, Loss 0.16329649090766907\n","[Training Epoch 100] Batch 789, Loss 0.15924565494060516\n","[Training Epoch 100] Batch 790, Loss 0.15338167548179626\n","[Training Epoch 100] Batch 791, Loss 0.16472983360290527\n","[Training Epoch 100] Batch 792, Loss 0.1500805914402008\n","[Training Epoch 100] Batch 793, Loss 0.17917749285697937\n","[Training Epoch 100] Batch 794, Loss 0.1720888912677765\n","[Training Epoch 100] Batch 795, Loss 0.18063224852085114\n","[Training Epoch 100] Batch 796, Loss 0.16012009978294373\n","[Training Epoch 100] Batch 797, Loss 0.1565866470336914\n","[Training Epoch 100] Batch 798, Loss 0.16399091482162476\n","[Training Epoch 100] Batch 799, Loss 0.16017189621925354\n","[Training Epoch 100] Batch 800, Loss 0.16124865412712097\n","[Training Epoch 100] Batch 801, Loss 0.16722697019577026\n","[Training Epoch 100] Batch 802, Loss 0.15562978386878967\n","[Training Epoch 100] Batch 803, Loss 0.18557396531105042\n","[Training Epoch 100] Batch 804, Loss 0.1641862392425537\n","[Training Epoch 100] Batch 805, Loss 0.16192400455474854\n","[Training Epoch 100] Batch 806, Loss 0.16395887732505798\n","[Training Epoch 100] Batch 807, Loss 0.1699288785457611\n","[Training Epoch 100] Batch 808, Loss 0.18289053440093994\n","[Training Epoch 100] Batch 809, Loss 0.15960977971553802\n","[Training Epoch 100] Batch 810, Loss 0.13587094843387604\n","[Training Epoch 100] Batch 811, Loss 0.16842138767242432\n","[Training Epoch 100] Batch 812, Loss 0.17221717536449432\n","[Training Epoch 100] Batch 813, Loss 0.15895706415176392\n","[Training Epoch 100] Batch 814, Loss 0.1858060359954834\n","[Training Epoch 100] Batch 815, Loss 0.16791218519210815\n","[Training Epoch 100] Batch 816, Loss 0.1610184609889984\n","[Training Epoch 100] Batch 817, Loss 0.19413354992866516\n","[Training Epoch 100] Batch 818, Loss 0.1578182876110077\n","[Training Epoch 100] Batch 819, Loss 0.1495548039674759\n","[Training Epoch 100] Batch 820, Loss 0.16623863577842712\n","[Training Epoch 100] Batch 821, Loss 0.1612042784690857\n","[Training Epoch 100] Batch 822, Loss 0.16027973592281342\n","[Training Epoch 100] Batch 823, Loss 0.1672935038805008\n","[Training Epoch 100] Batch 824, Loss 0.1560884416103363\n","[Training Epoch 100] Batch 825, Loss 0.17818966507911682\n","[Training Epoch 100] Batch 826, Loss 0.16503064334392548\n","[Training Epoch 100] Batch 827, Loss 0.1927690953016281\n","[Training Epoch 100] Batch 828, Loss 0.162913516163826\n","[Training Epoch 100] Batch 829, Loss 0.1598135530948639\n","[Training Epoch 100] Batch 830, Loss 0.17547725141048431\n","[Training Epoch 100] Batch 831, Loss 0.18635571002960205\n","[Training Epoch 100] Batch 832, Loss 0.1468265950679779\n","[Training Epoch 100] Batch 833, Loss 0.16801218688488007\n","[Training Epoch 100] Batch 834, Loss 0.1769126057624817\n","[Training Epoch 100] Batch 835, Loss 0.18148574233055115\n","[Training Epoch 100] Batch 836, Loss 0.1649416834115982\n","[Training Epoch 100] Batch 837, Loss 0.18097391724586487\n","[Training Epoch 100] Batch 838, Loss 0.1680331528186798\n","[Training Epoch 100] Batch 839, Loss 0.17363297939300537\n","[Training Epoch 100] Batch 840, Loss 0.16569986939430237\n","[Training Epoch 100] Batch 841, Loss 0.16386127471923828\n","[Training Epoch 100] Batch 842, Loss 0.1795009970664978\n","[Training Epoch 100] Batch 843, Loss 0.15877050161361694\n","[Training Epoch 100] Batch 844, Loss 0.153705894947052\n","[Training Epoch 100] Batch 845, Loss 0.17102673649787903\n","[Training Epoch 100] Batch 846, Loss 0.1659916639328003\n","[Training Epoch 100] Batch 847, Loss 0.15286597609519958\n","[Training Epoch 100] Batch 848, Loss 0.1699293553829193\n","[Training Epoch 100] Batch 849, Loss 0.19593395292758942\n","[Training Epoch 100] Batch 850, Loss 0.15577156841754913\n","[Training Epoch 100] Batch 851, Loss 0.165371835231781\n","[Training Epoch 100] Batch 852, Loss 0.1670674979686737\n","[Training Epoch 100] Batch 853, Loss 0.1756914108991623\n","[Training Epoch 100] Batch 854, Loss 0.17639011144638062\n","[Training Epoch 100] Batch 855, Loss 0.1687776893377304\n","[Training Epoch 100] Batch 856, Loss 0.15434131026268005\n","[Training Epoch 100] Batch 857, Loss 0.15880495309829712\n","[Training Epoch 100] Batch 858, Loss 0.17288145422935486\n","[Training Epoch 100] Batch 859, Loss 0.15312550961971283\n","[Training Epoch 100] Batch 860, Loss 0.175790473818779\n","[Training Epoch 100] Batch 861, Loss 0.15964959561824799\n","[Training Epoch 100] Batch 862, Loss 0.15668749809265137\n","[Training Epoch 100] Batch 863, Loss 0.18039369583129883\n","[Training Epoch 100] Batch 864, Loss 0.17474690079689026\n","[Training Epoch 100] Batch 865, Loss 0.17859430611133575\n","[Training Epoch 100] Batch 866, Loss 0.17114028334617615\n","[Training Epoch 100] Batch 867, Loss 0.1678120195865631\n","[Training Epoch 100] Batch 868, Loss 0.17573969066143036\n","[Training Epoch 100] Batch 869, Loss 0.14191854000091553\n","[Training Epoch 100] Batch 870, Loss 0.16908785700798035\n","[Training Epoch 100] Batch 871, Loss 0.1762421727180481\n","[Training Epoch 100] Batch 872, Loss 0.16065959632396698\n","[Training Epoch 100] Batch 873, Loss 0.1871967315673828\n","[Training Epoch 100] Batch 874, Loss 0.16398602724075317\n","[Training Epoch 100] Batch 875, Loss 0.1768936663866043\n","[Training Epoch 100] Batch 876, Loss 0.16143763065338135\n","[Training Epoch 100] Batch 877, Loss 0.15254652500152588\n","[Training Epoch 100] Batch 878, Loss 0.17259839177131653\n","[Training Epoch 100] Batch 879, Loss 0.16055265069007874\n","[Training Epoch 100] Batch 880, Loss 0.1639072597026825\n","[Training Epoch 100] Batch 881, Loss 0.1634606570005417\n","[Training Epoch 100] Batch 882, Loss 0.16062751412391663\n","[Training Epoch 100] Batch 883, Loss 0.19285641610622406\n","[Training Epoch 100] Batch 884, Loss 0.16256655752658844\n","[Training Epoch 100] Batch 885, Loss 0.1596759408712387\n","[Training Epoch 100] Batch 886, Loss 0.1642664074897766\n","[Training Epoch 100] Batch 887, Loss 0.17421403527259827\n","[Training Epoch 100] Batch 888, Loss 0.16879115998744965\n","[Training Epoch 100] Batch 889, Loss 0.15192323923110962\n","[Training Epoch 100] Batch 890, Loss 0.17652224004268646\n","[Training Epoch 100] Batch 891, Loss 0.15107938647270203\n","[Training Epoch 100] Batch 892, Loss 0.14423368871212006\n","[Training Epoch 100] Batch 893, Loss 0.167121022939682\n","[Training Epoch 100] Batch 894, Loss 0.17614895105361938\n","[Training Epoch 100] Batch 895, Loss 0.14645661413669586\n","[Training Epoch 100] Batch 896, Loss 0.16963469982147217\n","[Training Epoch 100] Batch 897, Loss 0.17246538400650024\n","[Training Epoch 100] Batch 898, Loss 0.16482596099376678\n","[Training Epoch 100] Batch 899, Loss 0.16794560849666595\n","[Training Epoch 100] Batch 900, Loss 0.1537081003189087\n","[Training Epoch 100] Batch 901, Loss 0.16898399591445923\n","[Training Epoch 100] Batch 902, Loss 0.16090303659439087\n","[Training Epoch 100] Batch 903, Loss 0.17011719942092896\n","[Training Epoch 100] Batch 904, Loss 0.1708930879831314\n","[Training Epoch 100] Batch 905, Loss 0.18046531081199646\n","[Training Epoch 100] Batch 906, Loss 0.17822134494781494\n","[Training Epoch 100] Batch 907, Loss 0.18162767589092255\n","[Training Epoch 100] Batch 908, Loss 0.1553405225276947\n","[Training Epoch 100] Batch 909, Loss 0.17276480793952942\n","[Training Epoch 100] Batch 910, Loss 0.1798161119222641\n","[Training Epoch 100] Batch 911, Loss 0.16087034344673157\n","[Training Epoch 100] Batch 912, Loss 0.17532339692115784\n","[Training Epoch 100] Batch 913, Loss 0.1738244891166687\n","[Training Epoch 100] Batch 914, Loss 0.16398915648460388\n","[Training Epoch 100] Batch 915, Loss 0.1714765429496765\n","[Training Epoch 100] Batch 916, Loss 0.16234567761421204\n","[Training Epoch 100] Batch 917, Loss 0.1584356129169464\n","[Training Epoch 100] Batch 918, Loss 0.20481544733047485\n","[Training Epoch 100] Batch 919, Loss 0.16616582870483398\n","[Training Epoch 100] Batch 920, Loss 0.16472816467285156\n","[Training Epoch 100] Batch 921, Loss 0.18238815665245056\n","[Training Epoch 100] Batch 922, Loss 0.19444717466831207\n","[Training Epoch 100] Batch 923, Loss 0.18400397896766663\n","[Training Epoch 100] Batch 924, Loss 0.15727093815803528\n","[Training Epoch 100] Batch 925, Loss 0.17289429903030396\n","[Training Epoch 100] Batch 926, Loss 0.1895512342453003\n","[Training Epoch 100] Batch 927, Loss 0.16404247283935547\n","[Training Epoch 100] Batch 928, Loss 0.1606835126876831\n","[Training Epoch 100] Batch 929, Loss 0.179016575217247\n","[Training Epoch 100] Batch 930, Loss 0.1806904822587967\n","[Training Epoch 100] Batch 931, Loss 0.18097004294395447\n","[Training Epoch 100] Batch 932, Loss 0.16101661324501038\n","[Training Epoch 100] Batch 933, Loss 0.18099336326122284\n","[Training Epoch 100] Batch 934, Loss 0.16961057484149933\n","[Training Epoch 100] Batch 935, Loss 0.16383683681488037\n","[Training Epoch 100] Batch 936, Loss 0.15078121423721313\n","[Training Epoch 100] Batch 937, Loss 0.1726757287979126\n","[Training Epoch 100] Batch 938, Loss 0.15757474303245544\n","[Training Epoch 100] Batch 939, Loss 0.18018725514411926\n","[Training Epoch 100] Batch 940, Loss 0.17597468197345734\n","[Training Epoch 100] Batch 941, Loss 0.18210068345069885\n","[Training Epoch 100] Batch 942, Loss 0.1657327115535736\n","[Training Epoch 100] Batch 943, Loss 0.18389570713043213\n","[Training Epoch 100] Batch 944, Loss 0.15656572580337524\n","[Training Epoch 100] Batch 945, Loss 0.15826541185379028\n","[Training Epoch 100] Batch 946, Loss 0.15655067563056946\n","[Training Epoch 100] Batch 947, Loss 0.1919373720884323\n","[Training Epoch 100] Batch 948, Loss 0.18922223150730133\n","[Training Epoch 100] Batch 949, Loss 0.1629563570022583\n","[Training Epoch 100] Batch 950, Loss 0.17435850203037262\n","[Training Epoch 100] Batch 951, Loss 0.17965972423553467\n","[Training Epoch 100] Batch 952, Loss 0.1824442595243454\n","[Training Epoch 100] Batch 953, Loss 0.1836259514093399\n","[Training Epoch 100] Batch 954, Loss 0.17443925142288208\n","[Training Epoch 100] Batch 955, Loss 0.17740291357040405\n","[Training Epoch 100] Batch 956, Loss 0.148800790309906\n","[Training Epoch 100] Batch 957, Loss 0.13864701986312866\n","[Training Epoch 100] Batch 958, Loss 0.15023332834243774\n","[Training Epoch 100] Batch 959, Loss 0.1853732019662857\n","[Training Epoch 100] Batch 960, Loss 0.14805933833122253\n","[Training Epoch 100] Batch 961, Loss 0.15982237458229065\n","[Training Epoch 100] Batch 962, Loss 0.1686548888683319\n","[Training Epoch 100] Batch 963, Loss 0.16535767912864685\n","[Training Epoch 100] Batch 964, Loss 0.1442524790763855\n","[Training Epoch 100] Batch 965, Loss 0.18767309188842773\n","[Training Epoch 100] Batch 966, Loss 0.16820670664310455\n","[Training Epoch 100] Batch 967, Loss 0.1635351926088333\n","[Training Epoch 100] Batch 968, Loss 0.16511686146259308\n","[Training Epoch 100] Batch 969, Loss 0.17640408873558044\n","[Training Epoch 100] Batch 970, Loss 0.17230471968650818\n","[Training Epoch 100] Batch 971, Loss 0.16168195009231567\n","[Training Epoch 100] Batch 972, Loss 0.1579636037349701\n","[Training Epoch 100] Batch 973, Loss 0.16389498114585876\n","[Training Epoch 100] Batch 974, Loss 0.1767571121454239\n","[Training Epoch 100] Batch 975, Loss 0.16618937253952026\n","[Training Epoch 100] Batch 976, Loss 0.18687790632247925\n","[Training Epoch 100] Batch 977, Loss 0.17669712007045746\n","[Training Epoch 100] Batch 978, Loss 0.1608698070049286\n","[Training Epoch 100] Batch 979, Loss 0.17276722192764282\n","[Training Epoch 100] Batch 980, Loss 0.1582379937171936\n","[Training Epoch 100] Batch 981, Loss 0.16653019189834595\n","[Training Epoch 100] Batch 982, Loss 0.15865901112556458\n","[Training Epoch 100] Batch 983, Loss 0.1608104109764099\n","[Training Epoch 100] Batch 984, Loss 0.16588109731674194\n","[Training Epoch 100] Batch 985, Loss 0.16425475478172302\n","[Training Epoch 100] Batch 986, Loss 0.18752437829971313\n","[Training Epoch 100] Batch 987, Loss 0.19970597326755524\n","[Training Epoch 100] Batch 988, Loss 0.16849099099636078\n","[Training Epoch 100] Batch 989, Loss 0.18009458482265472\n","[Training Epoch 100] Batch 990, Loss 0.1763988435268402\n","[Training Epoch 100] Batch 991, Loss 0.1696081906557083\n","[Training Epoch 100] Batch 992, Loss 0.16073355078697205\n","[Training Epoch 100] Batch 993, Loss 0.1777765154838562\n","[Training Epoch 100] Batch 994, Loss 0.17427754402160645\n","[Training Epoch 100] Batch 995, Loss 0.18350279331207275\n","[Training Epoch 100] Batch 996, Loss 0.18361634016036987\n","[Training Epoch 100] Batch 997, Loss 0.13707154989242554\n","[Training Epoch 100] Batch 998, Loss 0.17476977407932281\n","[Training Epoch 100] Batch 999, Loss 0.15714102983474731\n","[Training Epoch 100] Batch 1000, Loss 0.17312321066856384\n","[Training Epoch 100] Batch 1001, Loss 0.1665520817041397\n","[Training Epoch 100] Batch 1002, Loss 0.1758798062801361\n","[Training Epoch 100] Batch 1003, Loss 0.17304500937461853\n","[Training Epoch 100] Batch 1004, Loss 0.17242464423179626\n","[Training Epoch 100] Batch 1005, Loss 0.16470736265182495\n","[Training Epoch 100] Batch 1006, Loss 0.173953115940094\n","[Training Epoch 100] Batch 1007, Loss 0.17755940556526184\n","[Training Epoch 100] Batch 1008, Loss 0.1600627452135086\n","[Training Epoch 100] Batch 1009, Loss 0.15570850670337677\n","[Training Epoch 100] Batch 1010, Loss 0.16213801503181458\n","[Training Epoch 100] Batch 1011, Loss 0.14739900827407837\n","[Training Epoch 100] Batch 1012, Loss 0.16827645897865295\n","[Training Epoch 100] Batch 1013, Loss 0.17250902950763702\n","[Training Epoch 100] Batch 1014, Loss 0.1738681197166443\n","[Training Epoch 100] Batch 1015, Loss 0.19901813566684723\n","[Training Epoch 100] Batch 1016, Loss 0.16444216668605804\n","[Training Epoch 100] Batch 1017, Loss 0.1508301943540573\n","[Training Epoch 100] Batch 1018, Loss 0.17981639504432678\n","[Training Epoch 100] Batch 1019, Loss 0.18349581956863403\n","[Training Epoch 100] Batch 1020, Loss 0.16915211081504822\n","[Training Epoch 100] Batch 1021, Loss 0.162191241979599\n","[Training Epoch 100] Batch 1022, Loss 0.18487583100795746\n","[Training Epoch 100] Batch 1023, Loss 0.16708536446094513\n","[Training Epoch 100] Batch 1024, Loss 0.1712433397769928\n","[Training Epoch 100] Batch 1025, Loss 0.1494484692811966\n","[Training Epoch 100] Batch 1026, Loss 0.19588032364845276\n","[Training Epoch 100] Batch 1027, Loss 0.15545304119586945\n","[Training Epoch 100] Batch 1028, Loss 0.1715075969696045\n","[Training Epoch 100] Batch 1029, Loss 0.16918691992759705\n","[Training Epoch 100] Batch 1030, Loss 0.15909573435783386\n","[Training Epoch 100] Batch 1031, Loss 0.16960427165031433\n","[Training Epoch 100] Batch 1032, Loss 0.16534166038036346\n","[Training Epoch 100] Batch 1033, Loss 0.1488112509250641\n","[Training Epoch 100] Batch 1034, Loss 0.1645071804523468\n","[Training Epoch 100] Batch 1035, Loss 0.16350078582763672\n","[Training Epoch 100] Batch 1036, Loss 0.16732986271381378\n","[Training Epoch 100] Batch 1037, Loss 0.19356533885002136\n","[Training Epoch 100] Batch 1038, Loss 0.17423716187477112\n","[Training Epoch 100] Batch 1039, Loss 0.16468319296836853\n","[Training Epoch 100] Batch 1040, Loss 0.18146468698978424\n","[Training Epoch 100] Batch 1041, Loss 0.17097395658493042\n","[Training Epoch 100] Batch 1042, Loss 0.1691949963569641\n","[Training Epoch 100] Batch 1043, Loss 0.1612074077129364\n","[Training Epoch 100] Batch 1044, Loss 0.14399951696395874\n","[Training Epoch 100] Batch 1045, Loss 0.17565372586250305\n","[Training Epoch 100] Batch 1046, Loss 0.16962289810180664\n","[Training Epoch 100] Batch 1047, Loss 0.1825794130563736\n","[Training Epoch 100] Batch 1048, Loss 0.15585699677467346\n","[Training Epoch 100] Batch 1049, Loss 0.15300768613815308\n","[Training Epoch 100] Batch 1050, Loss 0.18543893098831177\n","[Training Epoch 100] Batch 1051, Loss 0.15299631655216217\n","[Training Epoch 100] Batch 1052, Loss 0.16672009229660034\n","[Training Epoch 100] Batch 1053, Loss 0.1956465244293213\n","[Training Epoch 100] Batch 1054, Loss 0.1688544899225235\n","[Training Epoch 100] Batch 1055, Loss 0.1704515814781189\n","[Training Epoch 100] Batch 1056, Loss 0.15566331148147583\n","[Training Epoch 100] Batch 1057, Loss 0.17403945326805115\n","[Training Epoch 100] Batch 1058, Loss 0.16450384259223938\n","[Training Epoch 100] Batch 1059, Loss 0.1589823067188263\n","[Training Epoch 100] Batch 1060, Loss 0.17680776119232178\n","[Training Epoch 100] Batch 1061, Loss 0.15615102648735046\n","[Training Epoch 100] Batch 1062, Loss 0.15576837956905365\n","[Training Epoch 100] Batch 1063, Loss 0.17014530301094055\n","[Training Epoch 100] Batch 1064, Loss 0.16127806901931763\n","[Training Epoch 100] Batch 1065, Loss 0.16299715638160706\n","[Training Epoch 100] Batch 1066, Loss 0.1660832017660141\n","[Training Epoch 100] Batch 1067, Loss 0.1635768711566925\n","[Training Epoch 100] Batch 1068, Loss 0.178921639919281\n","[Training Epoch 100] Batch 1069, Loss 0.18770748376846313\n","[Training Epoch 100] Batch 1070, Loss 0.1763107180595398\n","[Training Epoch 100] Batch 1071, Loss 0.16776971518993378\n","[Training Epoch 100] Batch 1072, Loss 0.17623679339885712\n","[Training Epoch 100] Batch 1073, Loss 0.16340340673923492\n","[Training Epoch 100] Batch 1074, Loss 0.15974575281143188\n","[Training Epoch 100] Batch 1075, Loss 0.16562530398368835\n","[Training Epoch 100] Batch 1076, Loss 0.16087312996387482\n","[Training Epoch 100] Batch 1077, Loss 0.1783168613910675\n","[Training Epoch 100] Batch 1078, Loss 0.16751191020011902\n","[Training Epoch 100] Batch 1079, Loss 0.1785113513469696\n","[Training Epoch 100] Batch 1080, Loss 0.16771164536476135\n","[Training Epoch 100] Batch 1081, Loss 0.15193960070610046\n","[Training Epoch 100] Batch 1082, Loss 0.17560754716396332\n","[Training Epoch 100] Batch 1083, Loss 0.1485120803117752\n","[Training Epoch 100] Batch 1084, Loss 0.17656514048576355\n","[Training Epoch 100] Batch 1085, Loss 0.1872100532054901\n","[Training Epoch 100] Batch 1086, Loss 0.1652797907590866\n","[Training Epoch 100] Batch 1087, Loss 0.21770960092544556\n","[Training Epoch 100] Batch 1088, Loss 0.13331404328346252\n","[Training Epoch 100] Batch 1089, Loss 0.1601434350013733\n","[Training Epoch 100] Batch 1090, Loss 0.17628750205039978\n","[Training Epoch 100] Batch 1091, Loss 0.17311710119247437\n","[Training Epoch 100] Batch 1092, Loss 0.16456684470176697\n","[Training Epoch 100] Batch 1093, Loss 0.1713319718837738\n","[Training Epoch 100] Batch 1094, Loss 0.16967153549194336\n","[Training Epoch 100] Batch 1095, Loss 0.19390451908111572\n","[Training Epoch 100] Batch 1096, Loss 0.16595014929771423\n","[Training Epoch 100] Batch 1097, Loss 0.16016505658626556\n","[Training Epoch 100] Batch 1098, Loss 0.1744207739830017\n","[Training Epoch 100] Batch 1099, Loss 0.16528478264808655\n","[Training Epoch 100] Batch 1100, Loss 0.15263016521930695\n","[Training Epoch 100] Batch 1101, Loss 0.17466133832931519\n","[Training Epoch 100] Batch 1102, Loss 0.16224153339862823\n","[Training Epoch 100] Batch 1103, Loss 0.14674174785614014\n","[Training Epoch 100] Batch 1104, Loss 0.14097335934638977\n","[Training Epoch 100] Batch 1105, Loss 0.15676376223564148\n","[Training Epoch 100] Batch 1106, Loss 0.1784088909626007\n","[Training Epoch 100] Batch 1107, Loss 0.20044396817684174\n","[Training Epoch 100] Batch 1108, Loss 0.18944332003593445\n","[Training Epoch 100] Batch 1109, Loss 0.1878661811351776\n","[Training Epoch 100] Batch 1110, Loss 0.1774756908416748\n","[Training Epoch 100] Batch 1111, Loss 0.1752561330795288\n","[Training Epoch 100] Batch 1112, Loss 0.16292686760425568\n","[Training Epoch 100] Batch 1113, Loss 0.18653756380081177\n","[Training Epoch 100] Batch 1114, Loss 0.18037500977516174\n","[Training Epoch 100] Batch 1115, Loss 0.16282311081886292\n","[Training Epoch 100] Batch 1116, Loss 0.1741897612810135\n","[Training Epoch 100] Batch 1117, Loss 0.15454071760177612\n","[Training Epoch 100] Batch 1118, Loss 0.16406086087226868\n","[Training Epoch 100] Batch 1119, Loss 0.14953657984733582\n","[Training Epoch 100] Batch 1120, Loss 0.1916937232017517\n","[Training Epoch 100] Batch 1121, Loss 0.16147655248641968\n","[Training Epoch 100] Batch 1122, Loss 0.17563670873641968\n","[Training Epoch 100] Batch 1123, Loss 0.17261651158332825\n","[Training Epoch 100] Batch 1124, Loss 0.17217504978179932\n","[Training Epoch 100] Batch 1125, Loss 0.17219655215740204\n","[Training Epoch 100] Batch 1126, Loss 0.18393835425376892\n","[Training Epoch 100] Batch 1127, Loss 0.15598711371421814\n","[Training Epoch 100] Batch 1128, Loss 0.15315982699394226\n","[Training Epoch 100] Batch 1129, Loss 0.1684013456106186\n","[Training Epoch 100] Batch 1130, Loss 0.1597604751586914\n","[Training Epoch 100] Batch 1131, Loss 0.16049088537693024\n","[Training Epoch 100] Batch 1132, Loss 0.14059123396873474\n","[Training Epoch 100] Batch 1133, Loss 0.18354018032550812\n","[Training Epoch 100] Batch 1134, Loss 0.15972495079040527\n","[Training Epoch 100] Batch 1135, Loss 0.17900195717811584\n","[Training Epoch 100] Batch 1136, Loss 0.18152356147766113\n","[Training Epoch 100] Batch 1137, Loss 0.1652841866016388\n","[Training Epoch 100] Batch 1138, Loss 0.1661856770515442\n","[Training Epoch 100] Batch 1139, Loss 0.14822036027908325\n","[Training Epoch 100] Batch 1140, Loss 0.1754087209701538\n","[Training Epoch 100] Batch 1141, Loss 0.15241739153862\n","[Training Epoch 100] Batch 1142, Loss 0.18299362063407898\n","[Training Epoch 100] Batch 1143, Loss 0.17692606151103973\n","[Training Epoch 100] Batch 1144, Loss 0.1973857283592224\n","[Training Epoch 100] Batch 1145, Loss 0.15106205642223358\n","[Training Epoch 100] Batch 1146, Loss 0.16208672523498535\n","[Training Epoch 100] Batch 1147, Loss 0.15392938256263733\n","[Training Epoch 100] Batch 1148, Loss 0.16019105911254883\n","[Training Epoch 100] Batch 1149, Loss 0.21526288986206055\n","[Training Epoch 100] Batch 1150, Loss 0.16340874135494232\n","[Training Epoch 100] Batch 1151, Loss 0.16946232318878174\n","[Training Epoch 100] Batch 1152, Loss 0.15889933705329895\n","[Training Epoch 100] Batch 1153, Loss 0.17680180072784424\n","[Training Epoch 100] Batch 1154, Loss 0.16211974620819092\n","[Training Epoch 100] Batch 1155, Loss 0.15768346190452576\n","[Training Epoch 100] Batch 1156, Loss 0.16129657626152039\n","[Training Epoch 100] Batch 1157, Loss 0.1731909215450287\n","[Training Epoch 100] Batch 1158, Loss 0.14807912707328796\n","[Training Epoch 100] Batch 1159, Loss 0.16953137516975403\n","[Training Epoch 100] Batch 1160, Loss 0.14114613831043243\n","/content/drive/MyDrive/Neural-CF/Torch-NCF/metrics.py:57: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","[Evluating Epoch 100] HR = 0.7258, NDCG = 0.3565\n","Model saved at epoch 100\n"]}],"source":["!python train.py --data_dir '../Yelp-Dataset/subset_5k-user_cnt/subset_review.json' --model 'neumf'"]},{"cell_type":"markdown","metadata":{"id":"feJEitTGhtPS"},"source":["# Evaluated with Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":380,"status":"ok","timestamp":1701674287582,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"},"user_tz":300},"id":"UfqFfGIX2llM","outputId":"a07ffd6d-5d75-48aa-f13d-024ff362ae92"},"outputs":[{"name":"stdout","output_type":"stream","text":["COMMAND     PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\n","tensorboa 59547 root   14u  IPv4 1477669      0t0  TCP localhost:6006 (LISTEN)\n"]}],"source":["!lsof -i :6006"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":150,"status":"ok","timestamp":1701674292044,"user":{"displayName":"Yilong Tang","userId":"01511647095502764773"},"user_tz":300},"id":"Oxv9531l2u5i"},"outputs":[],"source":["!kill -9 59547"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLUBSQOHhwQ3"},"outputs":[],"source":["# Load Tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir './runs/pretrain_neumf_epoch100_l2-0.0000001' --port 6006"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPmzCdcV6exDw72NUMtK8vB","collapsed_sections":["GohCVvVQ1XzK","NiZIFY1rjptM"],"gpuType":"T4","machine_shape":"hm","mount_file_id":"1C8DKzNq4VefaVUhbwmCjLk0CJ8ap3FgB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
